<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://www.firasesbai.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.firasesbai.com/" rel="alternate" type="text/html" /><updated>2023-10-08T19:03:38+00:00</updated><id>https://www.firasesbai.com/feed.xml</id><title type="html">Firas Esbai</title><subtitle>Dive into Software, Big Data, and AI with Practical Insights | Explore hands-on ideas at the intersection of technology on Firas Esbai's Blog.
</subtitle><author><name>Firas Esbai</name></author><entry><title type="html">Data Processing Architectures: Lambda vs Kappa</title><link href="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html" rel="alternate" type="text/html" title="Data Processing Architectures: Lambda vs Kappa" /><published>2023-09-24T00:00:00+00:00</published><updated>2023-09-24T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html">&lt;p&gt;&lt;em&gt;In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our data-driven era, the ability to harness the power of data has become a pivotal competitive advantage for businesses and organizations across industries. The vast volumes of information generated daily present both opportunities and challenges. How do we efficiently process, analyze, and derive insights from this deluge of data in real-time, without drowning in complexity?&lt;/p&gt;

&lt;p&gt;This is where data processing architectures come into play, offering structured approaches to these challenges. In this blog post, we will explore two prominent contenders in the realm of data processing: the &lt;strong&gt;Lambda&lt;/strong&gt; and &lt;strong&gt;Kappa&lt;/strong&gt; architectures.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;lambda-architecture&quot;&gt;Lambda Architecture&lt;/h2&gt;

&lt;p&gt;Lambda architecture was introduced by &lt;em&gt;Nathan Marz&lt;/em&gt; to address the challenges of data processing in a scalable and fault-tolerant manner.
The architecture takes an event stream and forks/duplicates it into two relatively independent layers called the &lt;strong&gt;Batch Layer&lt;/strong&gt; and the &lt;strong&gt;Speed Layer&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Batch layer&lt;/strong&gt; takes incoming data, combines it with historical data, and recomputes the results by iterating over the entire dataset thus allowing the system to give the most accurate results. However, the results are achieved at the expense of high latency due to the long computation time.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Speed Layer&lt;/strong&gt; on the other hand is used to provide a low-latency, near-real-time result. It performs incremental updates on data that was not processed in the last batch of the Batch Layer.&lt;/p&gt;

&lt;p&gt;The results from both systems constitute the &lt;strong&gt;Serving Layer&lt;/strong&gt;. It is responsible for serving queryable, up-to-date results to users or applications. In this layer, the query aims at merging and analyzing data from both the Batch Layer view and the incremental flow view from the Speed Layer.&lt;/p&gt;

&lt;p&gt;There are two variations on this: a &lt;strong&gt;unified serving layer&lt;/strong&gt; with one database for both outputs or &lt;strong&gt;separate serving layers&lt;/strong&gt; with two different databases, one optimized for real time and the other optimized for batch updates.&lt;/p&gt;

&lt;p&gt;The following diagram shows the lambda architecture at a high level:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_lambda_architecture.png&quot; alt=&quot;High Level Lambda Architecture&quot; /&gt;
&lt;em&gt;Figure 1: High Level Lambda Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;One of the key challenges in streaming is the reprocessing of the data. This can be due to code changes because your application evolves and you need to update the business logic or because you found a bug and you need to fix it. In either way, you will need to recompute your output to see the effect of these changes. The batch layer in the lambda architecture addresses this challenge by having a complete history of immutable data. In addition, the usage of two separate systems for processing data makes the lambda architecture flexible, easily scalable and fault tolerant. For instance, it can be used for a variety of use cases, including real-time analytics using the stream processing system and machine learning where models can leverage the large volume of data through the batch layer to generate more accurate results. If one system fails, say the batch processing system, the other can continue to operate providing real time insights into the data. Lastly, both systems can be scaled independently by either adding more nodes to the cluster or adding more streams.&lt;/p&gt;

&lt;p&gt;However, managing two separate processing systems is very complex. We need to provision and manage the infrastructure for two distributed systems including monitoring and logging which increases the cost and operations efforts of storage, compute and networking. Also, we need to align the business logic across streaming and batch codebases resulting in writing the same logic in two places with, most likely, different languages. This leads to difficult debugging and a challenge in validating data quality and making sure that the algorithms in each layer are matching.&lt;/p&gt;

&lt;p&gt;So, what’s different in Kappa architecture?&lt;/p&gt;

&lt;h2 id=&quot;kappa-architecture&quot;&gt;Kappa Architecture&lt;/h2&gt;

&lt;p&gt;The Kappa architecture was introduced by &lt;em&gt;Jay Kreps&lt;/em&gt;, co-founder and CEO at Confluent, a company built around the open source messaging system Apache Kafka, as a response to some of the challenges and complexities associated with the Lambda Architecture. 
The Kappa Architecture primarily focuses on stream processing simplifying the complexity of maintaining two systems with a single technology stack, referred to as &lt;strong&gt;Stream Processing Layer&lt;/strong&gt; in the diagram below, that can perform both real-time and batch processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_kappa_architecture.png&quot; alt=&quot;High Level Kappa Architecture&quot; /&gt;
&lt;em&gt;Figure 2: High Level Kappa Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages-1&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;The shift to a single stream processing system to handle both real-time and batch processing makes the Kappa architecture simpler, more efficient and cost effective than the lambda architecture. Having a single source of truth to all the data reduces both the burden of maintaining two separate systems and two codebases as well as the underlying costs. People can now develop, test, debug, and operate their systems on top of a single processing framework such as Apache Kafka.&lt;/p&gt;

&lt;p&gt;Stream processing is considered a paradigm shift from the traditional batch data processing. Therefore, it goes without saying that the Kappa architecture presents some challenges and limitations. In fact, processing out of order data or intricate joins combining many streams causes difficulties when transforming data in a streaming method. On top of that, data reprocessing which is now running using a single codebase, on the same framework, and with the same input data, still comes with some tradeoffs. As Jay Kreps detailed in his 
&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;original post&lt;/a&gt;, we can leverage Apache Kafka retention period (30 days for example) and take the retained data as an input to a second instance of the streaming process that will produce a new output table with the reprocessed data. However, this approach depends heavily on the configured retention period value and is limited in cases where we need to fix the algorithm or deploy a change like adding a new field that goes beyond the span of the retention period.&lt;/p&gt;

&lt;h2 id=&quot;choosing-the-right-architecture&quot;&gt;Choosing the Right Architecture&lt;/h2&gt;

&lt;p&gt;So when should we use one architecture or the other? As is often the case, it depends on some peculiarities of the implemented application.&lt;/p&gt;

&lt;p&gt;A very simple case is when the algorithms used for real-time and historical data are identical and implementable on streaming. It is then clearly very advantageous to use the same codebase to process historical and real-time data, and hence use the Kappa Architecture.
If the algorithms used to process historical data and real-time data are not always identical. Here, the choice between Lambda and Kappa becomes a tradeoff between the performance benefits of batch processing over a simpler codebase.&lt;/p&gt;

&lt;p&gt;Some examples of use cases where Kappa architecture is a good fit include fraud detection to detect fraudulent transactions in real time, process data from IoT devices in real time or recommendation engines used to to provide personalized recommendations to users in real time.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Lambda and Kappa are two popular data processing architectures that can be used to handle huge data. The ideal architecture relies on the individual needs for a given use case. We have discussed some benefits as well as drawbacks and limitations of each design to assist you in making your decision. Real-time insights are more important for businesses that want to become data-driven, which has increased the popularity of event streaming architecture. Batch processing will not go away, therefore it is essential to view the two architectures as complementing solutions rather than one being a cure for all ills.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&quot;&gt;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&quot;&gt;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.</summary></entry><entry><title type="html">Understanding Modern Data Stack</title><link href="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html" rel="alternate" type="text/html" title="Understanding Modern Data Stack" /><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html">&lt;p&gt;&lt;em&gt;In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The modern data stack is designed to empower organizations to harness the full potential of their data assets, make data-driven decisions, and stay competitive in today’s data-centric business landscape. It represents a shift towards more agile, integrated, and scalable data management practices.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-brief-history&quot;&gt;A Brief History&lt;/h2&gt;

&lt;p&gt;The modern data stack as we currently know it is a very recent development in data. In fact the rise of the cloud data warehouse triggered by the release of Amazon Redshift in late 2012 is considered one of the key developments that led to the adoption of the modern data stack. Redshift was one of the early cloud-based data warehousing solutions that offered a highly scalable and cost-effective platform for storing and analyzing large datasets. Its introduction marked a shift away from traditional on-premises data warehousing and towards cloud-based solutions. All of the other solutions in the market today like Google BigQuery and Snowflake followed the revolution set by Amazon.&lt;/p&gt;

&lt;p&gt;Consequently this shift led to the move from &lt;em&gt;Extract Transform Load (ETL)&lt;/em&gt; to &lt;em&gt;Extract Load Transform (ELT)&lt;/em&gt; pipelines. As storage becomes cheaper and more accessible, there is no need to deal with data transformations before saving it in the traditional data warehouse. Organizations just dump their data in its raw format and only apply the transformations later when needed.&lt;/p&gt;

&lt;p&gt;The rise of the cloud data warehouse has not only contributed to the transition from ETL to ELT but also the widespread adoption of BI tools. These self serve solutions democratize data usage allowing more and more personas to access it and make data-driven business decisions.‍&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-modern-data-stack&quot;&gt;What is the Modern Data Stack?&lt;/h2&gt;

&lt;p&gt;The modern data stack is a collection of tools and technologies used together to support the data flow starting from ingestion and integration of different data sources up to analysis in order to extract insights and help create data driven decisions. The particularity resides in the plug and play nature of its components and the overall ease of use without much infrastructure and data platform management overhead so that data is accessible for everyone to turn it into knowledge.&lt;/p&gt;

&lt;h2 id=&quot;objectives-of-the-modern-data-stack&quot;&gt;Objectives of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;The objectives of the modern data stack revolve around building a data infrastructure that enables organizations to efficiently and effectively manage their data, derive valuable insights, and make data-driven decisions.&lt;/p&gt;

&lt;p&gt;Here are the primary objectives of implementing a modern data stack:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Accommodate Growing Data - The modern data stack should be able to handle increasing data volumes, whether structured or unstructured, as organizations collect more data from various sources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility and Agility&lt;/strong&gt;: Easily Adapt to Changing Needs - It should be flexible enough to adapt to changing business requirements, data sources, and processing methods without the need for a complete overhaul.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Seamlessly Connect Data Sources: The stack should provide tools and processes for integrating data from diverse sources, including databases, applications, APIs, and more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost-Efficiency&lt;/strong&gt;: Optimize Resource Usage by minimising unnecessary resource usage by efficiently managing data storage and processing to control costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: Ensure Compatibility - Ensure that the various components of the stack can interoperate smoothly with each other and with external systems or tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cloud-Ready&lt;/strong&gt;: Leverage Cloud Infrastructure - Be compatible with cloud-based infrastructure to take advantage of scalability, cost-effectiveness, and the latest data services provided by cloud providers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance and Reliability&lt;/strong&gt;: Maintain High Performance - Deliver reliable and high-performance data processing to support critical business operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adaptability to Emerging Technologies&lt;/strong&gt;: Be Open to Innovation - Keep an eye on emerging technologies and trends, allowing for easy integration with new tools or platforms that may enhance the stack’s capabilities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;components-of-the-modern-data-stack&quot;&gt;Components of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;In a previous blog post about the &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;fundamentals of data engineering&lt;/a&gt; we tried to identify a common data flow that identifies the different stages including ingestion, storage, transformation, data management/governance, visualization and exploration  which are  involved in making data easily accessible.&lt;/p&gt;

&lt;p&gt;The components of the modern data stack can be perfectly mapped to the same diagram from the mentioned article, at least at a high level first, as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/15_modern_data_stack_example.png&quot; alt=&quot;Example of Componentes of Modern Data Stack in Standard Data Flow&quot; /&gt;
&lt;em&gt;Figure 1: Example of Componentes of Modern Data Stack in Standard Data Flow&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that the specific tools are changing and evolving rapidly but they usually include some of the ones we chose as an example in the diagram. In addition, some vendor technologies fit beyond a single stage as presented in the diagram and offer more capabilities such as data governance and/or machine learning.&lt;/p&gt;

&lt;p&gt;However, this only captures the stack at a high level. In fact, there is no one-size-fits-all approach when it comes to selecting the best tools and technologies to deal with your data. Every organization has a different level of data maturity, different data teams, different structures, processes, and so on.&lt;/p&gt;

&lt;p&gt;Therefore, the stack can be enriched with a &lt;strong&gt;workflow  orchestration tool&lt;/strong&gt; such as &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Apache Airflow&lt;/a&gt; or &lt;a href=&quot;https://dagster.io/&quot;&gt;Dagster&lt;/a&gt; needed to schedule your transformation in an automated fashion depending on your required frequency.&lt;/p&gt;

&lt;p&gt;Also &lt;strong&gt;data observability&lt;/strong&gt; has become a key part of the modern data stack. It ensures data reliability by monitoring data quality and identifying potential data issues throughout the stack. Explaining this concept is beyond the scope of this article but we will have a dedicated blog post about. Some of the tools worth mentioning here include &lt;a href=&quot;https://www.montecarlodata.com/&quot;&gt;Monte Carlo&lt;/a&gt; and &lt;a href=&quot;https://www.datadoghq.com/&quot;&gt;Datadog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we explored the concept of the modern data stack and its significance in the contemporary data landscape. We provided an overview of its main components and how they correlate to our standard data flow established in a previous article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/modern-data-stack/&quot;&gt;https://preset.io/blog/modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/reshaping-data-engineering/&quot;&gt;https://preset.io/blog/reshaping-data-engineering/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&quot;&gt;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&quot;&gt;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><category term="Cloud Computing" /><summary type="html">In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.</summary></entry><entry><title type="html">Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide</title><link href="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html" rel="alternate" type="text/html" title="Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide" /><published>2023-08-21T00:00:00+00:00</published><updated>2023-08-21T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html">&lt;p&gt;&lt;em&gt;In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Progressive Web Apps (PWAs) represent a significant advancement in web application development, offering users a seamless and engaging experience that combines the best aspects of both web and native mobile applications. 
By transforming your Jekyll-based blog into a PWA, you’ll enhance user engagement, improve performance, and enable key features such as offline access.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-are-progressive-web-apps&quot;&gt;What are Progressive Web Apps?&lt;/h2&gt;

&lt;p&gt;Platform-specific apps are developed for a specific operating system (OS) and/or class of devices, like an iOS or Android device. They are usually installed on the user’s device using the vendor’s app store. Websites on the other hand, can only be accessed by the user opening the browser and navigating to the site, and is highly dependent on network connectivity.&lt;/p&gt;

&lt;p&gt;So how does this relate to PWAs?&lt;/p&gt;

&lt;p&gt;Progressive web apps combine the best features of traditional websites and platform-specific apps. At their core, PWAs are web applications that take advantage of modern web technologies to deliver a reliable, fast, and immersive experience to users. Unlike traditional websites, PWAs can be installed on users’ devices, giving them a direct pathway to your content, even without a traditional app store. PWAs can be accessed through web browsers, but they offer the responsiveness and fluidity that users typically expect from native mobile applications.&lt;/p&gt;

&lt;h2 id=&quot;key-attributes-of-pwas&quot;&gt;Key Attributes of PWAs&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Offline Access&lt;/strong&gt;: One of the most significant advantages of PWAs is their ability to work offline or in low-network conditions. Users can still access content and navigate within the app, even when they’re not connected to the internet. This offline capability ensures that your content remains accessible, enhancing user satisfaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fast Loading&lt;/strong&gt;: PWAs are designed to load quickly, providing an almost instant experience to users. This is achieved through techniques like efficient caching, optimized assets, and lazy loading of content. Fast loading times lead to lower bounce rates and higher user retention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Responsive Design&lt;/strong&gt;: PWAs are responsive by default, adapting to various screen sizes and orientations. This responsiveness ensures a consistent and visually appealing experience across devices, including smartphones, tablets, and desktops.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engagement and Retention&lt;/strong&gt;: PWAs can be “installed” on users’ home screens or app drawers, creating a sense of ownership and encouraging repeated visits. This increased engagement can lead to higher retention rates, as users have easy access to your PWA.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;successful-pwa-examples&quot;&gt;Successful PWA Examples&lt;/h2&gt;

&lt;p&gt;Numerous companies have embraced PWAs to enhance user experience and drive business growth. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pinterest&lt;/strong&gt;: Pinterest’s PWA increased user engagement, with faster load times leading to a 60% increase in user engagement and a 44% increase in user-generated ad revenue.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alibaba&lt;/strong&gt;: Alibaba.com’s PWA achieved a 76% increase in conversions across browsers, with 14% more monthly active users on iOS and a 30% increase in mobile users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, check the links in the resources section.&lt;/p&gt;

&lt;h2 id=&quot;technical-features-of-pwas&quot;&gt;Technical features of PWAs&lt;/h2&gt;

&lt;p&gt;Because PWAs are websites, they have the same basic features as any other website: at least one HTML page, which very probably loads some CSS and JavaScript.&lt;/p&gt;

&lt;p&gt;Beyond that, a PWA has some additional features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;web app manifest file&lt;/strong&gt;, which, at a minimum, provides information that the browser needs to install the PWA, such as the app name and icon.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;service worker&lt;/strong&gt;, which, at a minimum, provides a basic offline experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having this in mind, a blog is nothing more than static HTML, CSS, and Javascript files. This makes it a prime candidate for adding PWA features which is the focus of the rest of this blog post.&lt;/p&gt;

&lt;p&gt;We will be adding these features to a blog built using Jekyll, a free and open source static site generator, and hosted on Github Pages. For more in depth guides on how I started my journey building this blog, you can check my 4 Parts series starting from &lt;a href=&quot;https://www.firasesbai.com/articles/2021/10/07/how-i-started-this-blog.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-manifest-file&quot;&gt;Creating a Manifest File&lt;/h2&gt;

&lt;p&gt;The Web App Manifest is a JSON document that provides application metadata such as its name, icon, and other details, which browsers can use when adding the PWA to the home screen.&lt;/p&gt;

&lt;p&gt;If you have previously generated a favicon to your blog using &lt;a href=&quot;https://realfavicongenerator.net/&quot;&gt;https://realfavicongenerator.net/&lt;/a&gt; or similar, you should already have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;site.webmanifest&lt;/code&gt; file that might be complete or miss a few properties. Otherwise, you can just use the following link &lt;a href=&quot;https://app-manifest.firebaseapp.com/&quot;&gt;https://app-manifest.firebaseapp.com/&lt;/a&gt; for reference or to generate its content.&lt;/p&gt;

&lt;p&gt;In either cases, the next step would be to add a reference to the manifest file under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file in the head section as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;manifest&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/assets/favicon/site.webmanifest&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
   
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The location of your manifest file might differ and you should update it accordingly. In my case, the manifest file is placed under assets along with the icons and favicon.&lt;/p&gt;

&lt;h2 id=&quot;implementing-service-workers&quot;&gt;Implementing Service Workers&lt;/h2&gt;

&lt;p&gt;A service worker is a powerful web technology that acts as a scriptable network proxy between a web application (such as a website) and the browser. It runs in the background, separate from the main web page, and allows you to intercept and control network requests and responses, enabling advanced features like offline access, caching, and push notifications in web applications.&lt;/p&gt;

&lt;p&gt;We will be using &lt;a href=&quot;https://developer.chrome.com/docs/workbox/&quot;&gt;Workbox&lt;/a&gt; which is a set of Javascript modules created by Google that simplifies and addresses a specific aspect of service worker development.&lt;/p&gt;

&lt;p&gt;1- Download and install the latest version of Node.js from the &lt;a href=&quot;https://nodejs.org/&quot;&gt;official website&lt;/a&gt;, and npm will be installed automatically as part of the Node.js package.&lt;/p&gt;

&lt;p&gt;2- Install Workbox CLI by running the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;npm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cli&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3- Use the Workbox Wizard to create a service worker by executing the following command inside the directory of your blog:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wizard&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The Workbox CLI wizard will guide you through setting up the service worker by choosing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; directory as the root of your app and selecting the type of files the service worker should pre-cache.&lt;/p&gt;

&lt;p&gt;At the end of this step, you should have a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-config.js&lt;/code&gt; created at the root of the project.&lt;/p&gt;

&lt;p&gt;4- Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-confiog.js&lt;/code&gt; file to generate the service worker by executing the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generateSW&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A new file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder will be generated.&lt;/p&gt;

&lt;p&gt;5- Create a new javascript file used to register your service worker. Place this file under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/js/service-worker.js&lt;/code&gt; in your Jekyll project’s root directory.&lt;/p&gt;

&lt;p&gt;The content of this file is the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;    &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Only&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;supported&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'serviceWorker'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;navigator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Wait&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;until&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loaded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addEventListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'load'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Register&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'s scope.
        navigator.serviceWorker.register('&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', { scope: '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' })
            // Output success/failure of registration.
            .then(() =&amp;gt; console.log('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registered&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'))
            .catch(() =&amp;gt; console.error('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In order to register the service worker and enable its functionality, you’ll need to include this code snippet to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file, which is a layout file included in all your pages, as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;     
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/js/service-worker.js&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;
  
  &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point if your development server is already running, you can navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:4000&lt;/code&gt; and use developer tools to verify that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;By checking the Offline box under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Manifest -&amp;gt; Service worker&lt;/code&gt; you can still access your site.&lt;/li&gt;
  &lt;li&gt;Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Cache Storage&lt;/code&gt; new entry have been created&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The newly created cache entry indicates that the predetermined assets in the workbox-config file are added to the cache during the service worker installation. This is called &lt;strong&gt;precaching&lt;/strong&gt; and is commonly used to ensure that essential resources are available offline and to improve the initial loading performance of your application.&lt;/p&gt;

&lt;p&gt;For more details on some precaching considerations you can check this &lt;a href=&quot;https://developer.chrome.com/docs/workbox/precaching-dos-and-donts/&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another type of caching is &lt;strong&gt;runtime caching&lt;/strong&gt;. It  involves caching resources dynamically during the runtime of your web application, i.e., when users interact with the application. Unlike precaching, runtime caching allows you to define caching strategies for specific URLs or URL patterns based on different criteria such as network requests, HTTP methods, and more.&lt;/p&gt;

&lt;p&gt;There some common runtime caching strategies that are out of the scope of this blog post. For more details you can refer to the &lt;a href=&quot;https://developer.chrome.com/docs/workbox/modules/workbox-strategies/&quot;&gt;workbox documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deploying-service-workers&quot;&gt;Deploying Service Workers&lt;/h2&gt;

&lt;p&gt;Knowing that Jekyll regenerates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder with each change you make to your files, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; will be lost and we have to regenerate it each time. Since our site is hosted on Github Pages, we can leverage &lt;strong&gt;Github Actions&lt;/strong&gt; to automate the execution of these commands as part of the pipeline building and deploying your site.&lt;/p&gt;

&lt;p&gt;An example of a Github Actions workflow can be found &lt;a href=&quot;https://github.com/firasesbai/firasesbai.github.io/blob/master/.github/workflows/github-pages.yml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Congratulations! You’ve successfully transformed your Jekyll blog into a powerful Progressive Web App. By implementing service workers and a manifest file, you’ve unlocked offline access and responsive design, making your content accessible to users even when they’re offline. Remember, this is just the beginning and there’s a wealth of additional features and optimizations you can explore to further enhance your PWA.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&quot;&gt;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/what-are-pwas/&quot;&gt;https://web.dev/what-are-pwas/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&quot;&gt;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/alibaba/&quot;&gt;https://web.dev/alibaba/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&quot;&gt;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sevic.dev/caching-service-worker-workbox/&quot;&gt;https://sevic.dev/caching-service-worker-workbox/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Jekyll" /><summary type="html">In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 2</title><link href="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 2" /><published>2023-03-12T00:00:00+00:00</published><updated>2023-03-12T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html">&lt;p&gt;&lt;em&gt;This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will discover how data in motion and data at rest is handled through different techniques and methods and the importance of choosing the right storage technology.&lt;/p&gt;

&lt;p&gt;If you have missed the first part, you can find it &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So buckle up, folks! This is going to be a long ride. But don’t worry, it’s worth it. Grab a snack, get comfy, and let’s dive in!”&lt;/p&gt;

&lt;h2 id=&quot;ingestion&quot;&gt;Ingestion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ETL (Extract, Transform, Load)&lt;/strong&gt; and &lt;strong&gt;ELT (Extract, Load, Transform)&lt;/strong&gt; are both data integration techniques used to move data from multiple sources to a single destination, such as a data warehouse or a data lake (more on this later in the Storage section).&lt;/p&gt;

&lt;p&gt;In ETL, data is extracted from source systems, transformed into a format suitable for the destination system, and then loaded into the destination system. 
The transformation is actually done in what is often referred to as a staging area. 
This approach is typically used in traditional data warehousing systems. 
Any data you load into your data warehouse must be transformed into a relational format before the data warehouse can ingest it. 
As a part of this data transformation process, data mapping may also be necessary to combine multiple data sources based on correlating information. 
In addition, ETL can help with data privacy and compliance by cleaning sensitive and secure data even before loading into the data warehouse.&lt;/p&gt;

&lt;p&gt;In ELT on the other hand, data is first extracted from source systems and loaded into the destination system in its raw form, where it is then transformed into the desired format. 
This approach is typically used in big data environments, where the target system, such as a data lake, is designed to handle large amounts of unstructured and semi-structured data and can perform the transformations in parallel.&lt;/p&gt;

&lt;p&gt;Rather than obsessing over this ETL vs ELT cage fight, just try to take away the following:&lt;/p&gt;

&lt;p&gt;Sometimes you may want to optimize/reshape your data sooner (because you know that’s how everyone wants to use it). 
Other times, you want to leave the schema flexible (and just let the user’s queries/views do the work) to avoid having to maintain lots of tables/views/jobs.&lt;/p&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Batch&lt;/strong&gt; and &lt;strong&gt;Stream&lt;/strong&gt; processing are two popular methods for data processing and transformation.&lt;/p&gt;

&lt;p&gt;In batch processing, we wait for a certain amount of raw data to “pile up” before running an ETL job. 
Typically this means data is between an hour to a few days old before it is made available for analysis. 
Batch ETL jobs will typically be run on a set schedule (e.g. every 24 hours), or in some cases once the amount of data reaches a certain threshold.
You should lean towards batch processing when:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data freshness is not a mission-critical issue&lt;/li&gt;
  &lt;li&gt;You are working with large datasets and are running a complex algorithm that requires access to the entire batch – e.g., sorting the entire dataset&lt;/li&gt;
  &lt;li&gt;You get access to the data in batches rather than in streams&lt;/li&gt;
  &lt;li&gt;When you are joining tables in relational databases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In stream processing, we process data as soon as it arrives in the storage layer – which would often also be very close to the time it was generated (although this would not always be the case). 
This would typically be in sub-second timeframes, so that for the end user the processing happens in real-time. 
These operations would typically not be stateful, or would only be able to store a ‘small’ state, so would usually involve a relatively simple transformation or calculation.&lt;/p&gt;

&lt;p&gt;Another alternative is &lt;strong&gt;micro-batch&lt;/strong&gt; processing. 
In micro-batch processing, we run batch processes on much smaller accumulations of data – typically less than a minute’s worth of data. 
This means data is available in near real-time. 
In practice, there is little difference between micro-batching and stream processing, and the terms would often be used interchangeably in data architecture descriptions and software platform descriptions.
Microbatch processing is useful when we need very fresh data, but not necessarily real-time – meaning we can’t wait an hour or a day for a batch processing to run, but we also don’t need to know what happened in the last few seconds. 
Example scenarios could include web analytics (clickstream) or user behavior.&lt;/p&gt;

&lt;h2 id=&quot;storage&quot;&gt;Storage&lt;/h2&gt;

&lt;p&gt;Two different types of data are used today in an organization; &lt;strong&gt;Operational&lt;/strong&gt; and &lt;strong&gt;Analytical&lt;/strong&gt;. 
Both operational and analytical data are important for organizations, but they serve different purposes and are used in different ways.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;​​Operational data refers to the data that is used in day-to-day business operations to support critical functions such as sales, marketing, and customer service. Operational data is often used to support short-term decision making and is focused on current and immediate needs.&lt;/li&gt;
  &lt;li&gt;Analytical data, on the other hand, is used for long-term strategic decision making. Analytical data is used to support trend analysis, business intelligence, and other data-driven decision making processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;oltp-vs-olap&quot;&gt;OLTP vs OLAP&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;OLTP (Online Transaction Processing)&lt;/strong&gt; and &lt;strong&gt;OLAP (Online Analytical Processing)&lt;/strong&gt; are two different types of data processing systems that are often used to manage operational and analytical data, respectively.&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLTP systems are used to process operational data and are designed to support rapid data insertion, updates, and retrievals.&lt;/li&gt;
  &lt;li&gt;Make sure that the systems can keep up with high volumes of transactions but often very small and fast in nature (e.g. online banking, FinTech application).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLAP systems are used to process analytical data and are designed to support large-scale data analysis.&lt;/li&gt;
  &lt;li&gt;Make sure that you can crunch through millions or billions of rows of data for your complex and large theories, where they need to run some fancy aggregation or calculations for data Analytics purposes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example: Online Store&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Store user data, passwords, previous transactions, find user, change its name,… basically perform INSERT, UPDATE, DELETE operations&lt;/li&gt;
  &lt;li&gt;Store actual products, their associated prices&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Find out the “total money spent by all users”&lt;/li&gt;
  &lt;li&gt;Find out “what is the most sold product”&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oltp-databases&quot;&gt;OLTP Databases&lt;/h4&gt;

&lt;p&gt;OLTP databases can use either &lt;strong&gt;SQL (Structured Query Language)&lt;/strong&gt; or &lt;strong&gt;NoSQL (Not only SQL)&lt;/strong&gt; technologies.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SQL Database&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;NoSQL Database&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Data storage model&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tables with fixed rows and columns&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: JSON Documents &lt;br /&gt; Key-value: key-value pairs &lt;br /&gt; Wide-Columns: Tables with rows and dynamic columns &lt;br /&gt; Graph&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Development history&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the 1970s with a focus on reducing data duplication&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the late 2000s with a focus on scaling and allowing for rapid application change driven by agile and DevOps practices&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Primary purpose&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;General purpose&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: general purpose &lt;br /&gt; Key-value: large amounts of data with simple lookup queries &lt;br /&gt; Wide-column: large amounts of data with predictable query patterns &lt;br /&gt; Graph: analyzing and traversing relationships between connected data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Rigid&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Flexible (Implicit schema!, querying time)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Vertical (scale-up with a larger server)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Horizontal (scale-out across commodity servers)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Joins&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically required&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically not required&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;SQL databases are based on a relational model and use a structured data model, where data is organized into tables, rows, and columns. 
This makes it easy to enforce data constraints, such as unique keys, and ensures data consistency. 
They are well tested and proven, and they have a long history of use in OLTP systems. 
This makes them a reliable choice for OLTP systems. 
However, SQL databases can be complex to set up and maintain and can be challenging to scale, particularly for large OLTP systems that require horizontal scalability and the rigid schema of SQL databases can make it difficult to accommodate changing requirements or new data types.&lt;/p&gt;

&lt;p&gt;NoSQL databases on the other hand are designed for horizontal scalability, which makes them well suited for OLTP systems that need to scale to handle large amounts of data and users. 
In addition, they use a variety of data models which makes them more flexible than SQL databases. 
This allows NoSQL databases to better handle unstructured data and changing data requirements. 
However , NoSQL databases may not provide the same level of transactional consistency as SQL databases, which can result in data inconsistencies. 
Plus they can be complex to set up and maintain as well, even though this can be addressed since they are designed to work well in cloud environments, which makes them a good choice for OLTP systems that need to scale quickly and elastically.&lt;/p&gt;

&lt;p&gt;We kept mentioning &lt;strong&gt;transactional consistency&lt;/strong&gt; and you might be wondering what is it?&lt;/p&gt;

&lt;p&gt;A transaction is a sequence of operations performed (using one or more SQL statements) on a database as a single logical unit of work. 
Transactions have the following four standard properties, usually referred to by the acronym &lt;strong&gt;ACID&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt; − ensures that all operations within the work unit are completed successfully. Otherwise, the transaction is aborted at the point of failure and all the previous operations are rolled back to their former state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; − ensures that the database properly changes states upon a successfully committed transaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; − enables transactions to operate independently of and transparent to each other.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt; − ensures that the result or effect of a committed transaction persists in case of a system failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;olap-databases&quot;&gt;OLAP Databases&lt;/h4&gt;

&lt;p&gt;In this section we will discuss two important components that are used for advanced analysis and decision-making; &lt;strong&gt;data warehouses&lt;/strong&gt; and &lt;strong&gt;data lakes&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;data-warehouse&quot;&gt;Data warehouse&lt;/h4&gt;

&lt;p&gt;A data warehouse is a centralized repository for storing and managing large amounts of data from various sources. 
Data warehouses are designed to support business intelligence (BI) and analytics applications, by providing a single source of data that can be queried, analyzed, and used to make informed decisions. 
The following are the key characteristics of a data warehouse:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Integration&lt;/em&gt; - Data warehouses integrate data from multiple sources, such as transactional systems, log files, and external data sources, into a single, unified view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data warehouses are designed to handle large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Modeling&lt;/em&gt; - Data warehouses use a specific data model, such as the star or snowflake schema, to organize data and make it easier to query and analyze.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Historical data&lt;/em&gt; - Data warehouses store historical data, allowing users to analyze trends and changes over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Performance optimization&lt;/em&gt; - Data warehouses are optimized for fast querying and analysis, by using techniques such as indexing, materialized views, and aggregations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Cleansing&lt;/em&gt; - Data warehouses often include data cleansing and normalization to ensure that data is consistent and accurate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Security and access control&lt;/em&gt; - Data warehouses have robust security and access control features to ensure that sensitive data is protected and only authorized users have access to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The explosion of big data, including the growth of structured, semi-structured, and unstructured data, made it increasingly difficult for traditional data warehouses to store and process all the data being generated. 
In addition, Traditional data warehouses required expensive hardware and software to store and process data. 
The cost of these solutions made it difficult for organizations to store all their data, which led to the adoption of data lakes as a more cost-effective alternative.&lt;/p&gt;

&lt;h4 id=&quot;data-lake&quot;&gt;Data lake&lt;/h4&gt;

&lt;p&gt;A data lake is a centralized repository that stores large amounts of raw, structured and unstructured data. The data is stored in its native format and can be accessed, processed, and analyzed later as needed. 
The following are the key characteristics of a data lake:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Flexibility&lt;/em&gt; - Data lakes allow organizations to store a wide variety of data types and formats, including structured, semi-structured, and unstructured data, without having to worry about pre-defining schemas.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data lakes are designed to handle very large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Cost-effectiveness&lt;/em&gt; - Data lakes are often implemented on low-cost, commodity hardware&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Raw data preservation&lt;/em&gt; - Data lakes preserve raw data, allowing organizations to perform in-depth analysis and retain the original data for future use.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Decentralized processing&lt;/em&gt; - Data lakes can be used to distribute processing tasks across multiple nodes in a network, allowing for increased processing speed and scalability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Self-service analytics&lt;/em&gt; - Data lakes allow business users and data scientists to perform their own data analysis, without having to rely on IT or data engineering teams.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integration with big data tools&lt;/em&gt; - Data lakes can be integrated with big data tools, such as Apache Spark and Apache Flink, allowing organizations to perform complex data processing and analysis tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following table summarizes the differences between data warehouse and data lake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/11_data_warehouse_vs_data_lake.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Warehouse vs Data Lake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As organizations move data infrastructure to the cloud, the choice of data warehouse vs. data lake, or the need for complex integrations between the two, is less of an issue. 
It is becoming natural for organizations to have both, and move data flexibly from lakes to warehouses to enable business analysis.&lt;/p&gt;

&lt;p&gt;Here is a list of some known cloud-based solutions from different cloud providers:&lt;/p&gt;

&lt;p&gt;Cloud data warehousing solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon Redshift&lt;/em&gt; - a fully-managed, analytical data warehouse that can handle petabyte-scale data, and enable querying it in seconds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Google BigQuery&lt;/em&gt; - an enterprise-grade cloud-native data warehouse, which runs fast interactive and ad-hoc queries on datasets of petabyte-scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cloud data lake solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon S3&lt;/em&gt; - an object storage platform built to store and retrieve any amount of data from any data source, and designed for 99.999999999% durability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Azure Blob Storage&lt;/em&gt; - stores billions of objects in hot, cool, or archive tiers, depending on how often data is accessed. Data ranges from structured (converted to object form) to any unstructured format - images, videos, audio, documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data lake architecture was introduced as a solution to some challenges of data warehousing with the rise of big data offering the ability to store and process big data in a cost-effective and scalable manner. 
However, it had its own set of challenges, such as the lack of reliability and transactional consistency and the complex data quality problems making it difficult for GDPR compliance and to use for critical business decisions.&lt;/p&gt;

&lt;p&gt;Can we get the best of both worlds without the complexity of managing both a data lake and a data warehouse or perhaps multiple ones?&lt;/p&gt;

&lt;h4 id=&quot;data-lakehouse&quot;&gt;Data Lakehouse&lt;/h4&gt;

&lt;p&gt;A data Lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. 
Data Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/12_data_warehouse_vs_data_lake_vs_data_lakehouse.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Data Warehouse vs Data Lake vs Data Lakehouse&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some data management solutions such as &lt;a href=&quot;https://delta.io/&quot;&gt;Delta Lake&lt;/a&gt;, which is an implementation of Data Lakehouse from Databricks, offer the ability to store and process big data in a reliable and consistent manner, while also providing the scalability and cost savings of a data lake.&lt;/p&gt;

&lt;p&gt;The data lakehouse is a relatively new concept and is still evolving, but it has the potential to become an important technology for big data processing and analysis in the future.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we went through some popular patterns such as ETL vs ELT and stream vs batch processing that are used in data ingestion and transformation and where they can be applied. 
Then we discussed the different types of data storage and some concrete implementations of them in the cloud and how they fit in an organization based on its requirements, data maturity and purpose.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://martinfowler.com/articles/data-mesh-principles.html&quot;&gt;https://martinfowler.com/articles/data-mesh-principles.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jie-tao.com/delta-lake-step-by-step1/&quot;&gt;https://www.jie-tao.com/delta-lake-step-by-step1/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&quot;&gt;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://delta.io/&quot;&gt;https://delta.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 1</title><link href="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 1" /><published>2023-03-11T00:00:00+00:00</published><updated>2023-03-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/11/data-engineering-201</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html">&lt;p&gt;&lt;em&gt;In this two parts article we will deep dive into each of the general steps of the data flow.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a previous &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;blog post&lt;/a&gt;, we saw the importance of data engineering in democratizing data inside an organization and enabling data driven decision making. 
In addition, we outlined the different stages data goes through in order to be able to extract insights out of it.&lt;/p&gt;

&lt;p&gt;Here is a reminder of our data flow blueprint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will start by taking a bird’s eye view, focusing on data sources, data governance and analysis before zooming in on the other stages of the data flow in part 2.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;data-sources&quot;&gt;Data Sources&lt;/h2&gt;

&lt;p&gt;Data sources refer to the various systems and platforms that generate data and constitute the origin of all data in an organization and play a crucial role in the data engineering and big data landscape. 
These sources can be internal systems such as transactional databases, or external sources such as social media platforms, sensors, or log files.&lt;/p&gt;

&lt;p&gt;The variety and volume of data sources have dramatically increased with the growth of big data, making it important for organizations to have a comprehensive strategy for managing, processing, and analyzing data from multiple sources.&lt;/p&gt;

&lt;p&gt;For instance, data must be transformed and stored in a format that enables efficient processing and analysis. 
There are a variety of file formats, such as Parquet and Avro, that are common to big data use cases and which provide a means of storing and exchanging large volumes of structured and semi-structured data in a standardized way, making it easier for organizations to work with data from multiple sources.&lt;/p&gt;

&lt;p&gt;Following is a table summarizing some properties and how do they compare between some big data file formats:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Properties&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CSV&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;JSON&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parquet&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Avro&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Human readability&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Format&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Key-Value&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Columunar&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compressible&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Splittable&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Complex data structure&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Schema evolution&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Read performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Write performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;CSV should typically be the fastest to write, JSON the easiest to understand for humans, and Parquet the fastest to read a subset of columns, while Avro is the fastest to read all columns at once.&lt;/p&gt;

&lt;h2 id=&quot;data-governance&quot;&gt;Data Governance&lt;/h2&gt;

&lt;p&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It includes the actions people must take, the processes they must follow, and the technology that supports them throughout the data lifecycle to ensure data is secure, private, accurate, available, and usable.&lt;/p&gt;

&lt;p&gt;Some of the key components of the data governance include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data compliance&lt;/strong&gt;: Establishing clear policies, procedures, and standards to govern data collection, storage, and usage. In addition, regularly monitoring and auditing data to ensure compliance with relevant regulations and laws&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data security&lt;/strong&gt;: Implementing security measures to protect data from unauthorized access and breaches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data privacy&lt;/strong&gt;: Ensuring that personal and confidential data is managed in accordance with relevant regulations and laws.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;: Refers to the development and implementation of activities that apply quality management techniques to data to make sure it is suitable to be used in a specific context, thus considered to be high quality data. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data cataloging&lt;/strong&gt;: Maintaining a comprehensive and up-to-date inventory of data assets, including metadata, definitions, and relationships.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data lineage&lt;/strong&gt;: Tracing the origin and evolution of data to understand its history and potential impact on decision-making.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-governance-vs-data-management&quot;&gt;Data Governance vs Data Management&lt;/h3&gt;

&lt;p&gt;The scope of Data management is broader than data governance. 
Data management includes all aspects of the full data lifecycle from collection and storage to usage and oversight. 
This is inclusive of data governance, which can be considered as a core component of it, but it also includes other areas such as data architecture and data modeling.&lt;/p&gt;

&lt;h3 id=&quot;benefits-of-data-governance&quot;&gt;Benefits of Data Governance&lt;/h3&gt;

&lt;p&gt;Implementing a data governance framework can increase the value of data within your organization and therefore make better and more timely decisions. 
In fact, effective data governance aims to maintain high quality data that’s both secure and compliant and easily accessible for deeper business insights.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Visualization&lt;/em&gt; and &lt;em&gt;reporting&lt;/em&gt; play a critical role in helping organizations to make informed decisions. 
By presenting data in an easily understandable format, stakeholders can gain insights into trends, patterns, and relationships that would not be evident from raw data alone. 
Effective visualization and reporting also make it easier for organizations to communicate the results of data analysis to others, including stakeholders, partners, and customers.&lt;/p&gt;

&lt;p&gt;Visualization is the process of creating visual representations of data and information, such as charts, graphs, and maps. 
Data visualization can be done using a wide range of tools including plotting libraries such as matplotlib or seaborn which are used especially by data scientists to more powerful analytics dashboards such as Tableau and Microsoft Power BI.&lt;/p&gt;

&lt;p&gt;Reporting, on the other hand, is the presentation of data and information in a structured format that is designed to be easily consumable by stakeholders. 
Reports are often used to provide an overview of key metrics, trends, and insights, and can be used to support decision making and strategic planning.&lt;/p&gt;

&lt;p&gt;Another aspect of the data analysis we mentioned in &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;this article&lt;/a&gt; is the use of &lt;em&gt;machine learning algorithms&lt;/em&gt;. 
However, it is not feasible to fully cover the topic of machine learning within the scope of this article. 
Though a good starting point for anyone new to this field is the &lt;a href=&quot;https://www.deeplearning.ai/courses/machine-learning-specialization/&quot;&gt;machine learning specialization course&lt;/a&gt; from Andrew Ng.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we saw the different types of big data file formats and how they compare to each other which can help shape our choice to achieve more efficient data processing. 
In addition, we discussed some of the key components of data governance and its benefit to achieve better data quality and therefore better data analysis through the usage of visualization and reporting.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://devopedia.org/data-serialization&quot;&gt;https://devopedia.org/data-serialization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/learn/what-is-data-governance&quot;&gt;https://cloud.google.com/learn/what-is-data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/topics/data-governance&quot;&gt;https://www.ibm.com/topics/data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.heavy.ai/technical-glossary/data-quality&quot;&gt;https://www.heavy.ai/technical-glossary/data-quality&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this two parts article we will deep dive into each of the general steps of the data flow.</summary></entry><entry><title type="html">Data engineering 101: Understanding the Fundementals</title><link href="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html" rel="alternate" type="text/html" title="Data engineering 101: Understanding the Fundementals" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/01/data-engineering-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html">&lt;p&gt;&lt;em&gt;In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-democratization&quot;&gt;What is Data Democratization?&lt;/h2&gt;

&lt;p&gt;Data democratization is the process of making data accessible to all members of an organization, regardless of their role or level of technical expertise. 
It enables organizations to make better and more informed decisions by providing access to data to a wide range of users. 
Data democratization also helps to promote data-driven culture within an organization, as it enables them to explore and leverage their data to create new products and services.&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-engineering&quot;&gt;What is Data Engineering?&lt;/h2&gt;

&lt;p&gt;While data democratization is the foundation for organizations to become data-driven and make better decisions, data engineering plays a vital role in making it happen.&lt;/p&gt;

&lt;p&gt;Data engineering is the process of acquiring, storing, and preparing raw data for analysis to extract valuable insights from their data and make informed business decisions.&lt;/p&gt;

&lt;h2 id=&quot;role-of-data-engineer&quot;&gt;Role of Data Engineer&lt;/h2&gt;

&lt;p&gt;A data engineer is responsible for the “plumbing” that helps derive value from data. 
The specific responsibilities of a data engineer may vary depending on the organization and industry, but generally include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Designing and building data pipelines&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Acquire data from various sources into the appropriate storage and processing systems&lt;/li&gt;
      &lt;li&gt;Coordination of the many jobs through orchestration tools&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data storage and management&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data storage systems&lt;/li&gt;
      &lt;li&gt;Design and implement data models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data processing&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data processing methods such as batch processing and streaming processing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data governance&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Ensure data is accurate, reliable, and easily accessible to users&lt;/li&gt;
      &lt;li&gt;Establish, implement, and maintain policies and procedures for data quality, compliance, and security&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Manage data infrastructure&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Collaboration and communication&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work closely with data scientists, analysts, and other stakeholders to understand their data needs and develop solutions that meet their requirements&lt;/li&gt;
      &lt;li&gt;Communicate with other teams such as DevOps, IT and security to ensure that the data infrastructure is aligned with the organization’s overall strategy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-engineering-vs-big-data-engineering&quot;&gt;Data Engineering vs Big Data Engineering&lt;/h2&gt;

&lt;p&gt;According to a study by IDC, the global data sphere is projected to grow from 33 zettabytes (ZB) in 2018 to 175 ZB by 2025, with a compound annual growth rate of 23%. 
It’s also important to note that the vast majority of this data (about 90%) is unstructured data, such as social media posts, images, and videos.
Another study by IBM estimates that 2.5 quintillion bytes of data are created every day, and this amount is increasing exponentially.&lt;/p&gt;

&lt;p&gt;These statistics highlight the sheer volume of data that is being generated on a daily basis and the significant growth in the amount of data generated over time which leads us to the definition of &lt;strong&gt;Big Data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Big data refers to extremely large and complex data sets that are difficult to process and analyze using traditional data processing and management techniques. These data sets are characterized by the “3Vs” - &lt;em&gt;volume, velocity, and variety&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Volume&lt;/em&gt; refers to the sheer amount of data that is generated and collected, this could be in terabytes, petabytes, or even exabytes.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Velocity&lt;/em&gt; refers to the speed at which data is generated and collected. With the growing number of devices and sensors that are connected to the internet, data is being generated at an unprecedented rate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Variety&lt;/em&gt; refers to the different types of data that are generated and collected, such as structured data, unstructured data, and semi-structured data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So is data engineering always “Big”?&lt;/p&gt;

&lt;p&gt;Data engineering is the broader concept of designing, building, and maintaining systems and infrastructure to support data-driven applications and services, while big data engineering is a specific subfield that is focused on the management and analysis of extremely large and complex data sets that requires the use of advanced processing tools and analytics techniques.&lt;/p&gt;

&lt;p&gt;Overall, data engineering is needed to democratize data inside an organization for informed business decisions, not just when we are dealing with big data.&lt;/p&gt;

&lt;p&gt;After understanding the “What” and the “Why” behind data engineering, it is now time to delve into the “How” of data engineering.&lt;/p&gt;

&lt;h2 id=&quot;data-flow&quot;&gt;Data Flow&lt;/h2&gt;

&lt;p&gt;The data flow outlines the different stages involved in making data easily accessible in a reliable and accurate way for analysis and decision making.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The general steps include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Data sources are an important and challenging step due to the wide range of potential integration points.&lt;/li&gt;
      &lt;li&gt;Depending on the organization’s needs, some common data sources include:
        &lt;ul&gt;
          &lt;li&gt;Relational and non relational databases&lt;/li&gt;
          &lt;li&gt;Flat files: Data stored in CSV, Excel, or other flat file formats&lt;/li&gt;
          &lt;li&gt;Streaming data: Data generated in real-time, such as data from social media feeds or IoT devices&lt;/li&gt;
          &lt;li&gt;External data: Data from third-party sources, such as government data or data from other companies&lt;/li&gt;
          &lt;li&gt;Legacy systems: Data stored in older systems that need to be migrated to the new data infrastructure&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Governance&lt;/strong&gt;
&lt;br /&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It is not particularly a separate step in the data flow, but rather a broad concept that involves processes, people and technology that support these processes in order to maintain high quality data and make better data driven decisions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data in Motion&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Ingestion&lt;/em&gt; 
&lt;br /&gt;Ingestion is the process of acquiring and importing data from various sources into the organization’s data infrastructure.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; 
&lt;br /&gt;This is the process of cleaning, normalizing, and transforming the data acquired during the ingestion step so that it is in a format that can be used for analysis and reporting.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data at Rest: Storage&lt;/strong&gt; 
&lt;br /&gt;This is the process of storing and managing the data in a way that it can be accessed, queried, and updated easily. Therefore, choosing the right type of storage and technology is a key element not only to the following steps, but also to previous ones due to the fact that we might require multiple storages in different steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Analysis&lt;/strong&gt; 
&lt;br /&gt;This is the step where we extract insights and knowledge from the processed data. This involves:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Visualization and Reporting&lt;/em&gt;
&lt;br /&gt;Creating visual representations of the data to help communicate insights and make it easy to understand&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Machine learning&lt;/em&gt;
&lt;br /&gt;Using algorithms to learn from the data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we have seen how valuable it is to democratize data inside an organization to make the shift towards data-driven decision making and the main role played by data engineers in the different steps of the raw data goes through to reach this goal. 
In the following blog post we will dive deeper into each step to uncover best practices, tools and paradigms used and how they have evolved in an ever changing field. Stay tuned!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&quot;&gt;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&quot;&gt;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/andkret/Cookbook&quot;&gt;https://github.com/andkret/Cookbook&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.</summary></entry><entry><title type="html">Prometheus Monitoring: How to Collect and Analyze Metrics</title><link href="https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring.html" rel="alternate" type="text/html" title="Prometheus Monitoring: How to Collect and Analyze Metrics" /><published>2023-01-15T00:00:00+00:00</published><updated>2023-01-15T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring.html">&lt;p&gt;&lt;em&gt;In this article we will deep dive into prometheus, the open-source monitoring and alerting system, and see how we can use it to monitor a simple python application.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All the source code is available &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Prometheus is an open-source monitoring and alerting system. 
It is often used to monitor the performance of various systems and services, and can send alerts if certain thresholds are exceeded. 
Prometheus also has a powerful query language that can be used to analyze the data it collects.&lt;/p&gt;

&lt;p&gt;Prometheus uses a pull model to collect metrics from targets. 
This means that Prometheus actively scrapes metrics from specified targets at regular intervals, rather than waiting for the targets to push metrics to it.&lt;/p&gt;

&lt;h3 id=&quot;prometheus-architecture&quot;&gt;Prometheus Architecture&lt;/h3&gt;

&lt;p&gt;The main components of Prometheus are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prometheus Server&lt;/strong&gt;: This is the core component of Prometheus. It is responsible for scraping metrics from specified targets, storing metrics in its built-in time-series database, and processing and answering queries via PromQL.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: A target is a system or service that Prometheus scrapes metrics from. Targets typically expose metrics in a specific format over HTTP, which Prometheus can understand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time-Series Database&lt;/strong&gt;: Prometheus stores all the metrics it collects in its built-in time-series database. This allows Prometheus to quickly query and analyze metrics, and to provide historical data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PromQL&lt;/strong&gt;: Prometheus has a powerful query language called PromQL, which can be used to retrieve and analyze metrics stored in its database. PromQL allows you to filter and aggregate metrics, and to create complex queries and alerts.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alertmanager&lt;/strong&gt;: Prometheus has a built-in alerting system called Alertmanager that can be used to trigger alerts based on metrics and send notifications through various channels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exporters&lt;/strong&gt;: Prometheus exporters are third-party tools that convert metrics from various systems and technologies into a format that Prometheus can understand. This allows Prometheus to scrape metrics from a wide variety of sources, such as JMX for Java applications, StatsD, SNMP, and more. For more details, check this blog &lt;a href=&quot;https://www.firasesbai.com/articles/2021/01/01/kafka-monitoring.html&quot;&gt;post&lt;/a&gt; on how to use JMX exporter to monitor an apache kafka cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These components work together in the following way:&lt;/p&gt;

&lt;p&gt;The prometheus server scrapes metrics from specified targets, stores the metrics in its built-in time-series database, and uses PromQL to process and answer queries. 
The Alertmanager is used to trigger alerts based on metrics and send notifications. 
In addition, exporters can be used to convert metrics from various systems and technologies into a format that Prometheus can understand.&lt;/p&gt;

&lt;h2 id=&quot;collecting-and-storing-metrics&quot;&gt;Collecting and Storing Metrics&lt;/h2&gt;

&lt;p&gt;Prometheus periodically scrapes metrics from specified targets, which are typically the applications or services that you want to monitor. 
These targets expose metrics in a specific format, usually over HTTP, that Prometheus can understand.&lt;/p&gt;

&lt;p&gt;Prometheus supports several types of metrics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Counter&lt;/strong&gt;: A cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gauge&lt;/strong&gt;: A metric that represents a single numerical value that can arbitrarily go up and down.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;: A metric that samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: Similar to a histogram, a summary samples observations and provides both the sum of all observed values and the number of observations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Untyped&lt;/strong&gt;: A catch-all metric type that can be used to represent any sort of value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vector&lt;/strong&gt;: A collection of metrics with the same name and type, but with different labels. This allows users to aggregate and select time series based on their labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Prometheus also supports the concept of &lt;strong&gt;labels&lt;/strong&gt;, which are key-value pairs that can be added to metrics to provide more context. 
Labels can be used to group and filter metrics, making it easy to identify trends and patterns in the data.&lt;/p&gt;

&lt;h2 id=&quot;integrating-with-other-tools&quot;&gt;Integrating with Other Tools&lt;/h2&gt;

&lt;p&gt;There are several popular tools that can integrate with Prometheus:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Grafana&lt;/strong&gt;: A popular open-source visualization and dashboarding tool that can be used to create interactive and informative charts and graphs based on Prometheus metrics.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kapacitor&lt;/strong&gt;: A data processing engine that can be used to perform calculations on Prometheus metrics in real-time, such as anomaly detection or aggregations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thanos&lt;/strong&gt;: A set of components that can be used to scale Prometheus and make it highly available, by providing features such as long-term storage and querying across multiple Prometheus instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;In our &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;example&lt;/a&gt;, we created a simple python application using fastapi. 
Our application has two endpoints; one that returns the prometheus metrics and another one that returns a random fact about cats. 
When querying the latter, we also show how to create your own prometheus metrics by measuring the number of times our endpoint was triggered. 
This is an example of a metric of type counter.&lt;/p&gt;

&lt;h3 id=&quot;set-up&quot;&gt;Set up&lt;/h3&gt;

&lt;p&gt;1- Clone this Github &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;repository&lt;/a&gt;, change into the corresponding directory and run the following command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up -d&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will start two docker containers: one corresponding to the &lt;em&gt;prometheus service&lt;/em&gt; and the other is our python application container called &lt;em&gt;fastapi-app&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;2- Access the prometheus dashboard in your web browser through: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:9090/targets&lt;/code&gt; where you will see our python application as a target for the prometheus service to monitor as shown in the screenshot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_prometheus_interface.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Prometheus Web Interface&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;3- Go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://127.0.0.1:8000/docs&lt;/code&gt; where you will see the Swagger UI of our python application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_fastapi_swagger_ui.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: FastAPI Swagger UI&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As indicated in the screenshot above, our application has two endpoints:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt;: Makes the monitoring metrics available for prometheus to scrape&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/cat_facts&lt;/code&gt;: Returns a random fact about cats.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As mentioned when triggering the last endpoint, we calculate in the background a prometheus metric called: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random_facts_api_execution_counter&lt;/code&gt; which measures the number of times this endpoint was triggered.&lt;/p&gt;

&lt;p&gt;When we request the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt; endpoint, we see a list of metrics measuring several things such as the &lt;em&gt;request time&lt;/em&gt;, &lt;em&gt;python garbage collection objects&lt;/em&gt;, etc…&lt;/p&gt;

&lt;p&gt;These metrics were made available by adding the &lt;strong&gt;prometheus middleware&lt;/strong&gt; to our application. 
  In addition, we can also see our own created metric as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_custom_prometheus_metric.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 3: Custom Prometheus Metrics Example&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;That’s it! You have successfully created a custom metric in your python application and you were able to scrape it and make it visible to prometheus.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this blog post, we discussed Prometheus monitoring, a widely used and powerful monitoring and alerting system. 
We covered topics such as Prometheus architecture, how to collect the different types of metrics and gave an example of popular tools that can integrate with Prometheus. 
In addition, we went through an example of how to deploy a python application in which we created a custom metric and we were able to scrap it and visualize it using prometheus.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://prometheus.io/docs/introduction/overview/&quot;&gt;https://prometheus.io/docs/introduction/overview/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://grafana.com/grafana/&quot;&gt;https://grafana.com/grafana/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.influxdata.com/kapacitor/v1.6/working/scraping-and-discovery/&quot;&gt;https://docs.influxdata.com/kapacitor/v1.6/working/scraping-and-discovery/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thanos.io/&quot;&gt;https://thanos.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Observability" /><category term="Software Development" /><summary type="html">In this article we will deep dive into prometheus, the open-source monitoring and alerting system, and see how we can use it to monitor a simple python application.</summary></entry><entry><title type="html">The Lean Startup</title><link href="https://www.firasesbai.com/notes/2022/09/11/the-lean-startup.html" rel="alternate" type="text/html" title="The Lean Startup" /><published>2022-09-11T00:00:00+00:00</published><updated>2022-09-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/notes/2022/09/11/the-lean-startup</id><content type="html" xml:base="https://www.firasesbai.com/notes/2022/09/11/the-lean-startup.html">&lt;p&gt;&lt;em&gt;Notes from the Lean Startup Book By Eric Ries&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_the_lean_startup.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Lean Startup Validated Learnings to Achieve Growth&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Lean startup takes its name from the lean manufacturing revolution that originated in Japan with the Toyota Production System. 
Since not only building cars requires management but also building a startup, the lean startup movement came to existence. 
Entrepreneurs are afraid of implementing traditional management practices early in a startup thinking that this will limit innovation. 
As a consequence, entrepreneurs take a ”just do it” attitude avoiding all forms of management, process and discipline. Unfortunately this approach leads to chaos more often then it does to success.&lt;/p&gt;

&lt;p&gt;Eric Ries describes in this book the why and the how that led to the birth of this movement and the tools that will teach you how to drive a Startup.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Favorite Quotes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Think Big, Start Small.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;An Experiment is a Product.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The lean startup is a framework not a blueprint of steps to follow. It is designed to be adapted to the specific conditions of each specific company.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Main Takeaways&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Lean Startup has &lt;em&gt;12 Principles&lt;/em&gt; that are categorised into three main parts making up the lifecycle of a Startup.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part 1: Vision&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Explore the importance of learning as the measure of progress for a startup.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 1: Start&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Goal of a startup&lt;/em&gt;: Figure out the right thing to build; the thing customers want and will pay for as quickly as possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 2: Define&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;What is a Startup?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;A startup is a human institution(process) designed to create a new product or service(innovation is part of the broad term product to encompass any source of value for the people) under conditions of extreme uncertainty.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;What is an Entrepreneur?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Anyone who is creating a new product or business under conditions of extreme uncertainty is an entrepreneur regardless of the size of the company, the industry or the sector of the economy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 3: Learn&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Validated Learning:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Learning is the essential unit of progress for startups. The effort that is not necessary for learning what customers want can be eliminated.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Productivity in a Startup:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Not in terms of how much stuff we are building but in terms of how much validated learning we’re getting for our efforts.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The audacity of zero:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;The irony is that it is often easier to raise money or acquire other resources when you have zero revenu, zero customers and zero traction than when you have a small amount. 
 Zero invites imagination, but small numbers invite questions about whether large numbers will ever materialise.&lt;/p&gt;

    &lt;p&gt;We can mitigate the waste that happens because of the audacity of zero with &lt;strong&gt;validated learning&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;In the lean startup model, every product, every feature, every marketing campaign –everything a startup does – is understood to be an &lt;strong&gt;experiment&lt;/strong&gt; designed to achieve validated learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 4: Experiment&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A true experiment follows the scientific method. It begins with a clear hypothesis that makes predictions about what is supposed to happen. It then tests those predictions empirically.&lt;/p&gt;

    &lt;p&gt;The goal of every startup’s experiment is to discover how to build a sustainable business around that &lt;strong&gt;vision&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;How?&lt;/em&gt; The second part will tell us that.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Part 2: Steer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;How vision leads to steering?&lt;/p&gt;

&lt;p&gt;Build-Measure-Learn feedback loop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_learn_build_measure.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Learn Build Measure Loop&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is at the core of the lean startup model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 5: Leap&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Every business plan begins with a set of assumptions. 
 The riskiest elements of a startup’s plan are called &lt;strong&gt;leap of faith assumptions&lt;/strong&gt;.
 They are called leaps of faith because the success of the entire venture rests on them. If they are true, tremendous opportunity awaits. If they are false, the startup risks total failure.&lt;/p&gt;

    &lt;p&gt;The most important leap-of-faith questions any new startup faces are &lt;strong&gt;value hypothesis&lt;/strong&gt; and &lt;strong&gt;growth hypothesis&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;The value hypothesis tests whether a product or service really delivers value to customers once they are using it.&lt;/p&gt;

    &lt;p&gt;Growth hypothesis tests how new customers will discover a product or service.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Example: why facebook impressed the investors?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;More than half of the users came back to the site every single day -&amp;gt; validated value hypothesis: the customers find the product valuable.&lt;/p&gt;

    &lt;p&gt;By the end of the month of launch 3/4 of harvard’s undergraduates were using it without a dollar of marketing or advertising have been spent -&amp;gt; validated growth hypothesis.&lt;/p&gt;

    &lt;p&gt;We need to confirm that your leap-of-faith questions are based in reality, that the customer has a significant problem worth solving.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;How?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Early contact with customers&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;The goal of such early contact with customers is to understand our potential customers and what problems they have. With that understanding we can craft a &lt;strong&gt;customer archetype&lt;/strong&gt;, a brief document that seeks to humanise the proposed target customer. &lt;br /&gt;
 The point is not to find the average customer but to find early adopters; the customers who feel the need for the product most accurately and experiment with them.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;When?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Enter the build phase as quickly as possible with a &lt;strong&gt;Concierge Minimum Viable Product&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 6: Test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The goal of the &lt;strong&gt;MVP&lt;/strong&gt; is to begin the process of learning, not to end it. Unlike a prototype or concept test, an MVP is designed not just to answer product design or technical questions. 
 Its goal is to test fundamental business hypotheses.&lt;/p&gt;

    &lt;p&gt;Before new products can be sold successfully to the mass market, they have to be sold to early adopters.&lt;/p&gt;

    &lt;p&gt;MVP ranges in complexity from extremely simple smoke tests (little more than an advertisement) to actual early prototypes complete with problems and missing features.&lt;/p&gt;

    &lt;p&gt;The lesson of the MVP is that any additional work beyond what was required to start learning is waste, no matter how important it might have seemed at the time.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Dropbox had a video as an MVP. It validated the leap-of-faith assumption that customers wanted the product by signing up to the beta waiting list on the website after watching the video.&lt;/p&gt;

    &lt;p&gt;For startups I believe in the following &lt;strong&gt;quality principle&lt;/strong&gt;: If we do not know who the customer is we do not know what quality is. So remove any feature, process or effort that does not contribute directly to the learning you seek.&lt;/p&gt;

    &lt;p&gt;The most commun speed bumps a Startup faces with an MVP are legal issues, fears about competitors (no need, present the competition with your idea and they won’t build it),  branding (different brand) risks and the impact on morale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 7: Measure&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;At the beginning a startup is little more than a business model (projections on how many customers the company expects to attract, how much it will spend, how much revenue and profit that will lead to) on a piece of paper.&lt;/p&gt;

    &lt;p&gt;A startup’s job is to:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Rigorously measure where it is right now&lt;/li&gt;
      &lt;li&gt;Devise experiments to learn how to move the real numbers closer to the ideal reflected in the business plan&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;How to measure progress ?  &lt;strong&gt;Innovation Accounting&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Innovation accounting is a quantitative approach that allows us to see whether our engine tuning efforts are bearing fruit. 
 It also allows us to create learning milestones useful for entrepreneurs as a way of assessing their progress accurately and objectively. 
 They are also invaluable to managers and investors who must hold entrepreneurs accountable.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Use an MVP to establish real data on where the company is right now.&lt;/li&gt;
      &lt;li&gt;Attempt to tune the engine (micro changes and product optimizations) from the baseline toward the ideal.&lt;/li&gt;
      &lt;li&gt;Pivot or Persevere: When a company pivots, it starts the process all over again. The sign of a successful pivot is that these engine-tuning activities are more productive after the pivot then before.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;Innovation accounting will not work if a startup is being misled by &lt;strong&gt;Vanity Metrics&lt;/strong&gt;: gross number of customers and so on. 
 The alternative is the kind of metrics we use to judge our business and our learning milestones; &lt;strong&gt;Actionable Metrics&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;Good metrics:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Actionable
        &lt;ul&gt;
          &lt;li&gt;It must demonstrate clear cause and effect. When cause and effect is clearly understood, people are better able to learn from their actions.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Accessible
        &lt;ul&gt;
          &lt;li&gt;All too many reports are not understood. The easiest way to make reports comprehensible is to use tangible concrete units. What is a website hit? Nobody is really sure but everyone knows what a person visiting the website is.&lt;/li&gt;
          &lt;li&gt;Accessibility also refers to widespread access to the reports. One way is to automatically generate a document containing each experiment and its results explained and mail it to every employee.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Auditable
        &lt;ul&gt;
          &lt;li&gt;We must ensure that the data is credible to employees. How? We need to be able to test the data by hand by talking to customers. Metrics are people too. This is the only way to check if the reports contain true facts.&lt;/li&gt;
          &lt;li&gt;The mechanisms that generate the reports are not too complex.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Solutions?&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Instead of looking at gross metrics, use &lt;strong&gt;cohort-based&lt;/strong&gt; metrics.&lt;/li&gt;
      &lt;li&gt;Instead of looking for cause-and-effect relationships after the fact, we launch each new feature as a true &lt;strong&gt;split-test experiment&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kanban&lt;/strong&gt;; Teams working in this system begin to measure their productivity according to validated learning not in terms of the production of new features.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;ins&gt;Cohort analysis&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;This is one of the most important tools of a startup analytics. 
 Instead of looking at cumulative totals or gross numbers such as total revenue and total number of customers one looks at the performance of each group of customers that comes into contact with the product independently. 
 Each group is called a cohort.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;Split-test experiment&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;This is one in which different versions of a product are offered to customers at the same time. By observing the changes in behavior between the two groups one can make inferences about the impact of the different variations. 
 Although split testing often is thought of as a marketing-specific practice, lean startups incorporate it directly into product development.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;Kanban&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;User stories are not considered complete until they led to validated learning 
 Stories can be in one of 4 states of development:&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;BACKLOG&lt;/th&gt;
          &lt;th&gt;IN PROGRESS&lt;/th&gt;
          &lt;th&gt;BUILT&lt;/th&gt;
          &lt;th&gt;VALIDATED&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;A&lt;/td&gt;
          &lt;td&gt;D&lt;/td&gt;
          &lt;td&gt;F&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;B&lt;/td&gt;
          &lt;td&gt;E&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;C&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;No bucket can contain more than 3 projects at a time.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Validated&lt;/em&gt; is defined as knowing whether the story was a good idea to have been done in the first place. 
 This validation usually would come in the form of a split test showing a change in customer behavior but also might include customer interviews or surveys.
 If the validation fails and it turns out the story is a bad idea, the relevant feature is removed from the product.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 8: Pivot (or Persevere)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A pivot is not just an exhortation to change. It is a special kind of structured change designed to test a new fundamental hypothesis about the product business model and engine of growth.&lt;/p&gt;

    &lt;p&gt;A pivot is better understood as a new strategic hypothesis that will require a new MVP to test.&lt;/p&gt;

    &lt;p&gt;Every startup should have a regular pivot or persevere meeting. Less than a few weeks is too often and more than a few months is too infrequent. Each startup needs to find its own pace.&lt;/p&gt;

    &lt;p&gt;Normally, the &lt;strong&gt;runway&lt;/strong&gt; is defined as the remaining cash in the bank divided by the monthly burn rate or net drain on that account balance. 
 The true measure of runway in a startup is how many pivots the startup has left: the number of opportunities it has to make a fundamental change to its business strategy. 
 The startup has to find a way to achieve the same amount of validated learning at lower cost or in a shorter time.&lt;/p&gt;

    &lt;p&gt;A catalogue of pivots:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Zoom-in pivot&lt;/li&gt;
      &lt;li&gt;Zoom-out pivot&lt;/li&gt;
      &lt;li&gt;Customer segment pivot&lt;/li&gt;
      &lt;li&gt;Customer need pivot&lt;/li&gt;
      &lt;li&gt;Platform pivot&lt;/li&gt;
      &lt;li&gt;Business architecture pivot&lt;/li&gt;
      &lt;li&gt;Value capture pivot&lt;/li&gt;
      &lt;li&gt;Engine of growth pivot&lt;/li&gt;
      &lt;li&gt;Channel pivot&lt;/li&gt;
      &lt;li&gt;Technology pivot &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Part 3: Accelerate&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 9: Batch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The small batch approach produces a finished product every few seconds whereas the large batch approach must deliver all the products at once at the end.&lt;/p&gt;

    &lt;p&gt;The biggest advantage of working in small batches is that quality problems can be identified much sooner.&lt;/p&gt;

    &lt;p&gt;What if it turns out that the customer doesn’t want the product we’re building? Working in small batches ensures that a startup can minimize the expenditure of time, money and effort that ultimately turns out to have been wasted.&lt;/p&gt;

    &lt;p&gt;The essential lesson is not that everyone should be shipping fifty times per day but that by reducing batch size we can get through the Build-Measure-Learn feedback loop more quickly than our competitors can. 
 The ability to learn faster from customers is the essential competitive advantage that startups must possess.&lt;/p&gt;

    &lt;p&gt;Lean production solves the problem of stockouts with a technique called &lt;em&gt;Pull&lt;/em&gt;. 
 When companies switch to this kind of production, their warehouses immediately shrink, as the amount of just-in-case inventory (called work-in-progress (WIP) inventory) is reduced dramatically.&lt;/p&gt;

    &lt;p&gt;Some people misunderstand the lean startup model as simply applying pull to customer wants. This assumes that customers could tell us what products to build and that this would act as the pull signal to product developpement to make them. Customers often don’t know what they want.&lt;/p&gt;

    &lt;p&gt;Product development process in a lean startup is responding to pull requests in the form of experiments that need to be run. 
 Thus it is not the customer but rather our &lt;em&gt;hypothesis about the customer&lt;/em&gt; that pulls work from product development and other functions. Any other work is waste.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_the_startup_way.png&quot; alt=&quot;image&quot; /&gt;
 &lt;br /&gt;&lt;em&gt;Figure 3: The Startup Way, Image from &lt;a href=&quot;http://www.thestartupway.com/&quot;&gt;theleanstartup&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principal 10: Growth&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The engine of growth is the mechanism that startups use to achieve sustainable growth. 
 &lt;strong&gt;Sustainable growth&lt;/strong&gt; is characterized by one simple rule: new customers come from the actions of past customers.&lt;/p&gt;

    &lt;p&gt;Four primary ways past customers drive sustainable growth:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Word of mouth&lt;/li&gt;
      &lt;li&gt;As a side effect of product usage&lt;/li&gt;
      &lt;li&gt;Through funded advertising&lt;/li&gt;
      &lt;li&gt;Through repeat purchase or use &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;These sources of sustainable growth power the feedback loops called &lt;strong&gt;engines of growth&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;Three engines of growth 
 Engines of growth are designed to give startups a relatively small set of metrics on which to focus their energies.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The sticky engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Attract and retain customers for the long term.&lt;/p&gt;

    &lt;p&gt;Companies using the sticky engine of growth track their &lt;strong&gt;attrition rate&lt;/strong&gt; or churn rate very carefully. The churn rate is defined as the fraction of customers in any period who fail to remain engaged with the company’s product. 
 The rules that govern the sticky engine of growth are pretty simple: if the rate of new customer acquisition exceeds the churn rate, the product will grow. The speed of growth is determined by what is called the &lt;strong&gt;rate of compounding&lt;/strong&gt; which is simply the natural growth rate minus the churn rate. 
 The way to find growth is to focus on existing customers to make the product even more engaging to them.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The viral engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;The viral engine is powered by a feedback loop that can be quantified. It is called the &lt;strong&gt;viral loop&lt;/strong&gt; and its speed is determined by a single mathematical term called the &lt;strong&gt;viral coefficient&lt;/strong&gt;. 
 The viral coefficient measures how many new customers will use a product as a consequence of each new customer who signs up. For a product with a viral coefficient of 0.1, one in every ten customers will recruit one of his or her friends.
 Companies that rely on the viral engine of growth must focus on increasing the viral coefficient more than anything else. 
 A consequence of this is that many viral products do not charge customers directly but rely on indirect sources of revenue such as advertising. This is because viral products cannot afford to have any friction impede the process of signing customers up and recruiting their friends.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The paid engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Each customer pays a certain amount of money for the product over his or her “lifetime” as a customer. Once variable costs are deducted this usually is called the customer lifetime value (LTV). This revenue can be invested in growth by buying advertising. 
 If the company wants to increase its rate of growth it can do so in one of two ways:
 Increase the revenue from each customer or drive down the cost of acquiring a new customer. 
 Suppose an advertisement costs 100$ and causes 50 new customers to sign up for the service. This ad has a cost per acquisition (CPA) of 2$. In this example if the product has an LTV &amp;gt; 2$ the product will grow. The margin between the LTV and CPA determines how fast the paid engine of growth will turn (this is called the marginal profit)&lt;/p&gt;

    &lt;p&gt;I strongly recommend that startups focus on one engine at a time. Only after pursuing one engine thoroughly should a startup consider a pivot to one of the others.&lt;/p&gt;

    &lt;p&gt;Product/market fit = moment when a startup finally finds a widespread set of customers that resonate with its product. 
 Since each engine of growth can be defined quantitatively, each has a unique set of metrics that can be used to evaluate whether a startup is on the verge of achieving product/market fit.&lt;/p&gt;

    &lt;p&gt;Two assumptions that are wrong:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Our startup has failed to achieve product/market fit -&amp;gt; a pivot is a failure event&lt;/li&gt;
      &lt;li&gt;Once our product has achieved product/market fit we won’t have to pivot anymore &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Every engine of growth is tied to a given set of customers and their related habits, preferences, advertising channels and interconnections. 
 At some point that set of customers will be exhausted. This may take a long time or a short time depending on one’s industry and timing.&lt;/p&gt;

    &lt;p&gt;Companies of any size can suffer from the perpetual affliction. 
 They need to manage a portfolio of activities simultaneously tuning their engine of growth and developing new sources of growth for when the engine inevitably runs its course.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 11: Adapt&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The wisdom of the five whys:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Repeating why five times can help uncover the root problem and correct it. 
 The core idea of five whys is to tie investments directly to the prevention of the most problematic symptoms. 
 Here is how to use 5 whys analysis to build an adaptive organization: consistently make a proportional investment at each of the five levels of the hierarchy. In other words the investment should be smaller when the symptom is minor and larger when the symptom is more painful. 
 Startup teams should go through the five whys whenever they encounter any kind of failure, including technical faults, failures to achieve business results or unexpected changes in customer behaviour.&lt;/p&gt;

    &lt;p&gt;The curse of the 5 blames
 Instead of using the 5 whys to find and fix problems managers and employees can fall into the trap of using the 5 blames as a means for venting their frustrations and calling out colleagues for systemic failures 
 To escape the 5 blames make sure that everyone affected by the problem is in the room during the analysis of the root cause.&lt;/p&gt;

    &lt;p&gt;Once you are ready to begin, start with a narrowly targeted class of symptoms. It’s better to give the team a chance to learn how to do the process first and then expand into higher-stakes areas later.&lt;/p&gt;

    &lt;p&gt;Appoint a five whys master for each area in which the method is being used. This individual is tasked with being the moderator for each 5 whys meeting making decisions about which prevention steps to take, and assigning the follow up work from the meeting.  The master must be senior enough to have the authority to ensure that those assignments get done but should not be so senior that he or she will not be able to be present at the meetings because of conflicting responsibilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 12: Innovate&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Successful innovation teams must be structured correctly in order to succeed.&lt;/p&gt;

    &lt;p&gt;Startup teams require 3 structural attributes:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Scarce but secure resources&lt;/strong&gt;: Startups require much less capital overall, but that capital must be absolutely secure from tampering&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Independent authority to develop their business&lt;/strong&gt;: Startup teams must be completely cross functional. Have full-time representation from every functional department in the company that will be involved in the creation or launch of their early products. They have to be able to build and ship actual functioning products and services, not just prototypes. Approvals slow down the build-measure-learn feedback loop.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Personal stake in the outcome&lt;/strong&gt;: This is usually achieved through stock options or other forms of equity ownership but it doesn’t have to be financial. Make it clear who the innovator is and make sure the innovator receives credit for having brought the new product to life if it is successful. &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;ins&gt;Creating an innovation sandbox&lt;ins&gt;&lt;/ins&gt;&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Create a sandbox for innovation that will contain the impact of the new innovation but not constrain the methods of the startup team. It works as follows:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Any team can create a true split-test experiment that affects only the sandboxed parts of the product or service or only certain customer segments or territories&lt;/li&gt;
      &lt;li&gt;One team must see the whole experiment through from end to end -&amp;gt; cross-functional team&lt;/li&gt;
      &lt;li&gt;No experiment can run longer than a specified amount of time&lt;/li&gt;
      &lt;li&gt;No experiment can affect more than a specified number of customers (usually expressed as a percentage of the company’s total mainstream customer base)&lt;/li&gt;
      &lt;li&gt;Every experiment has to be evaluated on the basis of a single standard report of five to 10 no more actionable metrics&lt;/li&gt;
      &lt;li&gt;Every team that works inside the sandbox and every product that is built must use the same metrics to evaluate success&lt;/li&gt;
      &lt;li&gt;Any team that creates an experiment must monitor the metrics and customer reactions while the experiment is in progress and abort it if something catastrophic happens&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading, I hope you enjoyed it!&lt;/em&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="notes" /><category term="Startups" /><summary type="html">Notes from the Lean Startup Book By Eric Ries</summary></entry><entry><title type="html">How I Started This Blog Part 4</title><link href="https://www.firasesbai.com/articles/2022/08/25/how-i-started-this-blog-part-4.html" rel="alternate" type="text/html" title="How I Started This Blog Part 4" /><published>2022-08-25T00:00:00+00:00</published><updated>2022-08-25T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2022/08/25/how-i-started-this-blog-part-4</id><content type="html" xml:base="https://www.firasesbai.com/articles/2022/08/25/how-i-started-this-blog-part-4.html">&lt;p&gt;&lt;em&gt;In this article we will wrap up the How I Started This Blog series by adding some features that will make your site more discoverable and easier to navigate.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In case you missed the first parts, you can find them here: &lt;a href=&quot;https://firasesbai.github.io/articles/2021/10/07/how-i-started-this-blog.html&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;https://firasesbai.github.io/articles/2022/03/23/how-i-started-this-blog-part-2.html&quot;&gt;Part 2&lt;/a&gt; and &lt;a href=&quot;https://www.firasesbai.com/articles/2022/03/27/how-i-started-this-blog-part-3.html&quot;&gt;Part 3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;search&quot;&gt;Search&lt;/h2&gt;

&lt;p&gt;As the number of articles increases over time in your blog, it will be difficult to navigate and find a certain article that you are looking for. In this case, having search functionality becomes necessary.&lt;/p&gt;

&lt;p&gt;There are multiple ways to achieve this such as using Google Custom Search Engine or other paid services. Easiest solution though would be to use the &lt;a href=&quot;https://github.com/christian-fei/Simple-Jekyll-Search&quot;&gt;Simple Jekyll Instant Search&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;set-up&quot;&gt;Set up&lt;/h3&gt;

&lt;p&gt;1- Create a &lt;strong&gt;JSON&lt;/strong&gt; file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search.json&lt;/code&gt; in the root folder of your blog&lt;/p&gt;

&lt;p&gt;2- Add the following code snippet in the newly created file:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   &lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
   &lt;span class=&quot;ss&quot;&gt;layout: &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;none&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;

   &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;% for &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;posts&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;%}
    {
      &quot;title&quot;       : &quot;{{ post.title | escape }&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;,
      &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;    : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;category&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;,
      &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;        : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tags&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;join: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', '&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;,
      &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;         : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;baseurl&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;,
      &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;        : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;,
      &quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot; : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strip_html&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strip_newlines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;escape&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;
    } 
   {% unless forloop.last %},{% endunless %}
   {% endfor %}
   
   &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will go over all your posts and extract for each one necessary information such as title, category, etc… that will be used to search through your articles.&lt;/p&gt;

&lt;p&gt;3- Create a new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search-script.js&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/js/&lt;/code&gt; folder in your root directory. You can find an example of it &lt;a href=&quot;https://github.com/firasesbai/firasesbai.github.io/blob/master/js/search-script.js&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;4- Create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search.html&lt;/code&gt; page under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_includes&lt;/code&gt; folder.&lt;/p&gt;

&lt;p&gt;5- Add the following code snippet to it:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   
   &lt;span class=&quot;o&quot;&gt;&amp;lt;!--&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Html&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Elements&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Search&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;div&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;search-container&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;search-input&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;search...&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ul&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;results-container&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/ul&amp;gt;
   &amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;div&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

   &lt;span class=&quot;o&quot;&gt;&amp;lt;!--&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pointing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/js/search-script.js&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;

   &amp;lt;!-- Configuration --&amp;gt;
   &amp;lt;script&amp;gt;
   SimpleJekyllSearch({
   searchInput: document.getElementById('search-input'),
   resultsContainer: document.getElementById('results-container'),
   json: '/se&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;',
   searchResultTemplate: '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;div&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{url}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/h4&amp;gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/div&amp;gt;'
   })
   &amp;lt;/s&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cript&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;

   
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;6- Include the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search.html&lt;/code&gt; page into your home page found under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_layouts/home.html&lt;/code&gt; by adding the following line:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;div&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;align&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;right&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;%- include search.html -%} &amp;lt;/div&amp;gt;   
   
   &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should now be able to see a search bar, as shown below in the screenshot, and be able to type in keywords to find an article.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/8_search_bar.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt; &lt;em&gt;Figure 1: Search Bar&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;pagination&quot;&gt;Pagination&lt;/h2&gt;

&lt;p&gt;For the same reasons mentioned above, having a pagination system in place will make navigating your rich blog easier rather than making your readers scroll down endlessly.&lt;/p&gt;

&lt;h3 id=&quot;set-up-1&quot;&gt;Set up&lt;/h3&gt;

&lt;p&gt;1- Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem &quot;jekyll-paginate-v2&quot;&lt;/code&gt; to your site’s Gemfile and run bundle&lt;/p&gt;

&lt;p&gt;2- Add the following to your site’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; file:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt; 
      
   &lt;span class=&quot;c1&quot;&gt;# Pagination Settings&lt;/span&gt;
   &lt;span class=&quot;ss&quot;&gt;pagination:
    enabled: &lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;per_page: &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;permalink: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'/page/:num/'&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;title: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' - page :num'&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;limit: &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;sort_field: &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'date'&lt;/span&gt;
    &lt;span class=&quot;ss&quot;&gt;sort_reverse: &lt;/span&gt;&lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;
      
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will enable the pagination and sort your posts based on their release date having the latest at the top and a maximum of 5 articles per page.&lt;/p&gt;

&lt;p&gt;3-  Add the following code snippet to your home page found under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_layouts/home.html&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt; 
      
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ul&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;post-list&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;lt;!--&lt;/span&gt; 
        &lt;span class=&quot;no&quot;&gt;Here&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paginator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;called&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;% for &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paginator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;posts&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;%}
      &amp;lt;li&amp;gt;
	  
        &amp;lt;h3&amp;gt; &amp;lt;a class=&quot;post-link&quot; href=&quot;{{ post.url | relative_url }&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;gt;{{ post.title | escape }}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;
        &amp;lt;span class=&quot;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;gt;{{ post.date | date: &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;%-d, %Y&quot; }}&amp;lt;/span&amp;gt;

		{%-&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show_excerpts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;%}
          {{ post.excerpt }&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;%- endif -%}
      &amp;lt;/li&amp;gt;
    {% endfor %}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/ul&amp;gt;
  
    &amp;lt;!-- 
    Showing buttons to move to the next and to the previous list of posts (pager buttons).
    --&amp;gt;
    {% if paginator.total_pages &amp;gt; 1 %}
    &amp;lt;ul class=&quot;pager&quot;&amp;gt;
      {% if paginator.previous_page %}
      &amp;lt;li class=&quot;previous&quot;&amp;gt;
          &amp;lt;a href=&quot;{{ paginator.previous_page_path | prepend: site.baseurl | replace: '/&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' }}&quot;&amp;gt;&amp;amp;larr; Newer Posts&amp;lt;/a&amp;gt;
      &amp;lt;/li&amp;gt;
      {% endif %}
      {% if paginator.next_page %}
      &amp;lt;li class=&quot;next&quot;&amp;gt;
          &amp;lt;a href=&quot;{{ paginator.next_page_path | prepend: site.baseurl | replace: '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/', '/&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;gt;Older Posts &amp;amp;rarr;&amp;lt;/a&amp;gt;
      &amp;lt;/li&amp;gt;
      {% endif %}
    &amp;lt;/ul&amp;gt;
   {% endif %}
      
   &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This will render your posts in a list using the &lt;strong&gt;paginator&lt;/strong&gt; variable provided by the gem and using the corresponding configurations you added in step 2.&lt;/p&gt;

&lt;p&gt;That is it! If you have more then five posts, navigation buttons like shown below will appear at the bottom of your homepage in order to be able to visit your older posts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/8_navigation_buttons.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Navigation Buttons&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;managing-a-custom-domain-for-your-site&quot;&gt;Managing a custom domain for your Site&lt;/h2&gt;

&lt;p&gt;We have mentioned in the first &lt;a href=&quot;https://firasesbai.github.io/articles/2021/10/07/how-i-started-this-blog.html&quot;&gt;blog post&lt;/a&gt; of the series that we have opted for &lt;strong&gt;GitHub Pages&lt;/strong&gt; as our free static site hosting service. 
Using this will serve your site under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://&amp;lt;username&amp;gt;.github.io&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;username&lt;/code&gt; is your GitHub user account.&lt;/p&gt;

&lt;p&gt;That worked out of the box for us and was a good starting point. Now that we have a fully functional website armed with all the features that we need, it is time to customise that URL by using a custom domain. 
This has multiple benefits namely better promoting our own brand, providing trust to our readers and making the site more discoverable.&lt;/p&gt;

&lt;h3 id=&quot;set-up-2&quot;&gt;Set up&lt;/h3&gt;

&lt;p&gt;1- Head over to &lt;a href=&quot;https://domains.google/&quot;&gt;Google Domains&lt;/a&gt; and search for a new domain&lt;/p&gt;

&lt;p&gt;2- Follow the steps in order to finalise your purchase. A standard &lt;strong&gt;.com&lt;/strong&gt; should cost you around 12 Euros per year.&lt;/p&gt;

&lt;p&gt;3- Under your newly created domain, click on &lt;strong&gt;DNS&lt;/strong&gt; from the sidebar and add a new &lt;strong&gt;Custom Records&lt;/strong&gt; with the following configurations:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Custom Record 1&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt; 
      
   &lt;span class=&quot;no&quot;&gt;Host&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;www&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autocompleted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yourdomainname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
   &lt;span class=&quot;no&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;CNAME&lt;/span&gt;
   &lt;span class=&quot;no&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3600&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;no&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;io&lt;/span&gt;
      
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Custom Record 2&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt; 
      
   &lt;span class=&quot;no&quot;&gt;Host&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;leave&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yourdomainname&lt;/span&gt; 
   &lt;span class=&quot;no&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;A&lt;/span&gt;
   &lt;span class=&quot;no&quot;&gt;TTL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3600&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;no&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;mf&quot;&gt;185.199&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;110.153&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;185.199&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;111.153&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;185.199&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;109.153&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;185.199&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;108.153&lt;/span&gt;
      
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;4- On GitHub, navigate to your site’s repository and go to Settings&lt;/p&gt;

&lt;p&gt;5- Click on Pages from the sidebar, add your custom domain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.yourdomainname.com&lt;/code&gt; and click save.&lt;/p&gt;

&lt;p&gt;6- Add &lt;strong&gt;CNAME&lt;/strong&gt; file to your master branch. By default when you add a custom domain, github adds the CNAME file to your &lt;em&gt;gh-pages&lt;/em&gt; branch, which gets overwritten by the build from your master branch each time you push new changes.&lt;/p&gt;

&lt;p&gt;That’s it! You should now be able to access your blog at the new URL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;www.yourdomainname.com&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Through these blog posts we have only touched the tip of the iceberg. Working on your blog will be a continuous process involving experimentation and trial of new features all in the purpose of reaching more people to share your thoughts and learnings. 
I hope this will get you started on a good foot and if you have any suggestions or an interesting feature and you would like to share it, feel free to reach out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.webjeda.com/instant-jekyll-search/&quot;&gt;https://blog.webjeda.com/instant-jekyll-search/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jekyllrb.com/docs/pagination/&quot;&gt;https://jekyllrb.com/docs/pagination/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/sverrirs/jekyll-paginate-v2&quot;&gt;https://github.com/sverrirs/jekyll-paginate-v2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site&quot;&gt;https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Jekyll" /><summary type="html">In this article we will wrap up the How I Started This Blog series by adding some features that will make your site more discoverable and easier to navigate.</summary></entry><entry><title type="html">Reading Recommendations</title><link href="https://www.firasesbai.com/notes/2022/08/17/books-recommendations.html" rel="alternate" type="text/html" title="Reading Recommendations" /><published>2022-08-17T00:00:00+00:00</published><updated>2022-08-17T00:00:00+00:00</updated><id>https://www.firasesbai.com/notes/2022/08/17/books-recommendations</id><content type="html" xml:base="https://www.firasesbai.com/notes/2022/08/17/books-recommendations.html">&lt;p&gt;&lt;em&gt;This is a curated list of book recommendations organised in different areas and subjects that might be of interest to you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/13_reading_recommendations.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a &lt;em&gt;Work In Progress&lt;/em&gt; which means you might want to keep an eye on it as it will be updated regularly.&lt;/p&gt;

&lt;h1 id=&quot;self-help&quot;&gt;Self-Help&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/4865.How_to_Win_Friends_and_Influence_People&quot;&gt;How to Win Friends and Influence People&lt;/a&gt; - &lt;em&gt;by Dale Carnegie&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://joshuafoer.com/&quot;&gt;Moonwalking with Einstein: The Art and Science of Remembering Everything&lt;/a&gt; - &lt;em&gt;by Joshua Foer&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://davidepstein.com/the-range/&quot;&gt;Range: Why Generalists Triumph in a Specialized World&lt;/a&gt; - &lt;em&gt;by David Epstein&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://calnewport.com/writing/&quot;&gt;So Good They Can’t Ignore You: Why Skills Trump Passion in the Quest for Work You Love&lt;/a&gt; - &lt;em&gt;by Cal Newport&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://calnewport.com/writing/&quot;&gt;Deep Work: Rules for Focused Success in a Distracted World&lt;/a&gt; - &lt;em&gt;by Cal Newport&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;productivity&quot;&gt;Productivity&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tim.blog/tim-ferriss-books/#the-4-hour-workweek&quot;&gt;The 4-Hour Workweek&lt;/a&gt; - &lt;em&gt;by Timothy Ferriss&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;money&quot;&gt;Money&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.naphill.org/shop/books/paperback/think-and-grow-rich-the-1937-edition/&quot;&gt;Think and Grow Rich&lt;/a&gt; - &lt;em&gt;by Napoleon Hill&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.richdad.com/about/rich-dad&quot;&gt;Rich Dad, Poor Dad&lt;/a&gt; - &lt;em&gt;by Robert T. Kiyosaki&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.morganhousel.com/&quot;&gt;The Psychology of Money&lt;/a&gt; - &lt;em&gt;by Morgan Housel&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;investement&quot;&gt;Investement&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/1052.The_Richest_Man_in_Babylon&quot;&gt;The Richest Man in Babylon&lt;/a&gt; - &lt;em&gt;by  George S. Clason&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;startups&quot;&gt;Startups&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://theleanstartup.com/&quot;&gt;The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses&lt;/a&gt; - &lt;em&gt;by Eric Ries&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://caroli.org/en/livro/lean-inception-how-to-align-people-and-build-the-right-product/&quot;&gt;Lean Inception: How to Align People and Build the Right Product&lt;/a&gt; - &lt;em&gt;by Paulo Caroli&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;artificial-intelligence&quot;&gt;Artificial Intelligence&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/34272565&quot;&gt;Life 3.0: Being Human in the Age of Artificial Intelligence&lt;/a&gt; - &lt;em&gt;by Max Tegmark&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Firas Esbai</name></author><category term="notes" /><category term="General" /><summary type="html">This is a curated list of book recommendations organised in different areas and subjects that might be of interest to you.</summary></entry></feed>