<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://www.firasesbai.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.firasesbai.com/" rel="alternate" type="text/html" /><updated>2024-08-25T11:25:50+00:00</updated><id>https://www.firasesbai.com/feed.xml</id><title type="html">Firas Esbai</title><subtitle>Firas Esbai's Lifelong Learning Journey.
</subtitle><author><name>Firas Esbai</name></author><entry><title type="html">Cloudflare Analytics vs Google Analytics</title><link href="https://www.firasesbai.com/articles/2024/08/11/cloudflare-vs-google-analytics.html" rel="alternate" type="text/html" title="Cloudflare Analytics vs Google Analytics" /><published>2024-08-11T00:00:00+00:00</published><updated>2024-08-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2024/08/11/cloudflare-vs-google-analytics</id><content type="html" xml:base="https://www.firasesbai.com/articles/2024/08/11/cloudflare-vs-google-analytics.html">&lt;p&gt;&lt;em&gt;In this article we will look into the reasons for discrepancies between the data reported by Cloudflare Analytics and Google Analytics and how to leverage both of them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Tracking and analyzing web data is essential for keeping a holistic view on your site’s performance. This is especially useful for bloggers to track key metrics such as which articles and pages are most visited, where visitors are located and the percentage of visitors who leave after viewing only one page just to name a few.&lt;/p&gt;

&lt;p&gt;For monitoring this site’s performance, I have opted for integrating Google Analytics web service. In addition, I’m using cloudflare for managing a custom domain name. Cloudflare has implemented Analytics where users can also gain insights to their managed sites. These however show big discrepancies from the data present in Google Analytics. Understanding the reasons behind it is what we will try to accomplish in this blog post.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data-collection-method&quot; id=&quot;markdown-toc-data-collection-method&quot;&gt;Data Collection Method&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#server-side-tracking&quot; id=&quot;markdown-toc-server-side-tracking&quot;&gt;Server-Side Tracking&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#client-side-tracking&quot; id=&quot;markdown-toc-client-side-tracking&quot;&gt;Client-Side Tracking&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#types-of-data-collected&quot; id=&quot;markdown-toc-types-of-data-collected&quot;&gt;Types of Data Collected&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cloudflare&quot; id=&quot;markdown-toc-cloudflare&quot;&gt;Cloudflare&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#google-analytics&quot; id=&quot;markdown-toc-google-analytics&quot;&gt;Google Analytics&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recommendation&quot; id=&quot;markdown-toc-recommendation&quot;&gt;Recommendation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;data-collection-method&quot;&gt;Data Collection Method&lt;/h2&gt;

&lt;p&gt;Understanding the different methods used by each tool to collect data will reveal the main reason for discrepancies in their reports.&lt;/p&gt;

&lt;h3 id=&quot;server-side-tracking&quot;&gt;Server-Side Tracking&lt;/h3&gt;

&lt;p&gt;Cloudflare uses DNS and server-side data to track visits. This means it collects data on every request made to your website’s server. This includes all HTTP/HTTPS requests, regardless of whether they are made by human users, bots, or automated scripts. Therefore, every unique IP address for a request is identified as a visit.&lt;/p&gt;

&lt;p&gt;For this reason, Cloudflare Analytics probably will show higher unique visitors than Google Analytics unique pageviews because when a bot or API is consuming partial content from your site without loading the full page it counts as a unique visitor in Cloudflare but not as a pageview.&lt;/p&gt;

&lt;h3 id=&quot;client-side-tracking&quot;&gt;Client-Side Tracking&lt;/h3&gt;

&lt;p&gt;Google Analytics uses a JavaScript tag embedded in web pages to collect data. When a user loads a page, the script sends data back to Google Analytics. Even though Google Analytics filters out known bot traffic, it can miss traffic from users who block Javascript, use privacy-focused browsers or have ad blockers that prevent Google Analytics from loading.&lt;/p&gt;

&lt;p&gt;For this reason, Cloudflare Analytics probably will show higher number of visitors than Google Analytics.&lt;/p&gt;

&lt;h2 id=&quot;types-of-data-collected&quot;&gt;Types of Data Collected&lt;/h2&gt;

&lt;p&gt;Knowing the reason behind the data of both Cloudflare and Google Analytics was only the beginning. In this section, we will look into the type of data collected by them and its purpose.&lt;/p&gt;

&lt;h3 id=&quot;cloudflare&quot;&gt;Cloudflare&lt;/h3&gt;

&lt;p&gt;Cloudflare often provides high-level technical data. It tracks every request made to your server which gives a comprehensive view of all incoming traffic and how many requests each resource on your website receives. In addition, Cloudflare provides data to help maintain the security and efficiency of your website such as blocked threats or DDoS attacks and cache hits/misses and load times.&lt;/p&gt;

&lt;h3 id=&quot;google-analytics&quot;&gt;Google Analytics&lt;/h3&gt;

&lt;p&gt;Google Analytics excels in tracking how users interact with your website. It provides detailed metrics on page views, sessions, bounce rates, and conversion rates. Also, it tracks where your visitors come from, whether through organic search, paid ads, social media, or referrals. This will help you understand your users’ behavior and engagement and their journey through your website.&lt;/p&gt;

&lt;h2 id=&quot;recommendation&quot;&gt;Recommendation&lt;/h2&gt;

&lt;p&gt;Leveraging data from both Cloudflare and Google Analytics offers a powerful combination of insights that will allow you to maximise your understanding of your website traffic along with detailed insights into user behavior and improve the performance and security.&lt;/p&gt;

&lt;p&gt;One way to achieve this would be using &lt;a href=&quot;https://lookerstudio.google.com/overview&quot;&gt;Looker Studio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Looker Studio is an online free tool by Google that will allow you to easily access data from multiple sources through its built-in connectors and visualise it through interactive reports and dashboards.&lt;/p&gt;

&lt;p&gt;If you are using Google Analytics then getting your data into Looker Studio is just a few clicks away. However, for Cloudflare data it is a bit more than that.&lt;/p&gt;

&lt;p&gt;Cloudflare web analytics is free and does not require an Enterprise plan but getting data into Looker Studio in an automated fashion would require a Cloudflare Enterprise account with Cloudflare Logs enabled. For more information, check this &lt;a href=&quot;https://developers.cloudflare.com/analytics/analytics-integrations/looker/&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An alternative workaround to visualize your Cloudflare data in Looker Studio can be achieved by exporting the collected data into Google Sheets first and then use the latter as a new data source in Looker Studio.&lt;/p&gt;

&lt;p&gt;The data export step can be done either manually or using a script that sends requests to Cloudflare’s API. Choosing one option over the other is up to you depending on your requirements and needs for up-to-date dashboards and how often you will review them.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we covered the reasons behind the difference of data gathered by Cloudflare Analytics and data coming from Google Analytics. We also saw how to use Looker Studio as a central reporting solution for combining both data sources for a comprehensive website analytics.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://developers.cloudflare.com/analytics/faq/about-analytics/#4lt2VoRUorCudxN1xzxpOt&quot;&gt;https://developers.cloudflare.com/analytics/faq/about-analytics/#4lt2VoRUorCudxN1xzxpOt&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Blogging" /><summary type="html">In this article we will look into the reasons for discrepancies between the data reported by Cloudflare Analytics and Google Analytics and how to leverage both of them.</summary></entry><entry><title type="html">Data Warehouse Modeling Techniques</title><link href="https://www.firasesbai.com/articles/2024/08/11/data-warehouse-modeling-techniques.html" rel="alternate" type="text/html" title="Data Warehouse Modeling Techniques" /><published>2024-08-11T00:00:00+00:00</published><updated>2024-08-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2024/08/11/data-warehouse-modeling-techniques</id><content type="html" xml:base="https://www.firasesbai.com/articles/2024/08/11/data-warehouse-modeling-techniques.html">&lt;p&gt;&lt;em&gt;In this article we will go over the big three approaches for modeling analytical data.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will go through some definitions around data modeling and database design approaches as well as the differences between a data warehouse and a data mart, how they are used in business intelligence and data analytics and the modeling techniques applied with them.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-a-data-warehouse&quot; id=&quot;markdown-toc-what-is-a-data-warehouse&quot;&gt;What is a Data Warehouse?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-warehouse-vs-data-mart&quot; id=&quot;markdown-toc-data-warehouse-vs-data-mart&quot;&gt;Data Warehouse vs Data Mart&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-data-modeling&quot; id=&quot;markdown-toc-what-is-data-modeling&quot;&gt;What is Data Modeling?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#normalization-vs-denormalization&quot; id=&quot;markdown-toc-normalization-vs-denormalization&quot;&gt;Normalization vs Denormalization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#normalization&quot; id=&quot;markdown-toc-normalization&quot;&gt;Normalization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#denormalization&quot; id=&quot;markdown-toc-denormalization&quot;&gt;Denormalization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-warehouse-modeling-techniques&quot; id=&quot;markdown-toc-data-warehouse-modeling-techniques&quot;&gt;Data Warehouse Modeling Techniques&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#modeling-techniques-implementation-on-databricks-lakehouse-platform&quot; id=&quot;markdown-toc-modeling-techniques-implementation-on-databricks-lakehouse-platform&quot;&gt;Modeling Techniques Implementation on Databricks Lakehouse Platform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;what-is-a-data-warehouse&quot;&gt;What is a Data Warehouse?&lt;/h2&gt;

&lt;p&gt;Data Warehouse is not the same as a database but rather typically built on top of other databases. It aggregates data from dozens of data sources such as operational systems or external systems and the data is actually copied rather than moved.&lt;/p&gt;

&lt;p&gt;Building a Data Warehouse serves the purpose of having a one stop shop: all your data in a single location which will enable you to make data driven decisions through trend analysis and business intelligence.&lt;/p&gt;

&lt;h2 id=&quot;data-warehouse-vs-data-mart&quot;&gt;Data Warehouse vs Data Mart&lt;/h2&gt;

&lt;p&gt;Unlike the data warehouse which is a centralized data repository that provides a holistic view on the organization’s data, data mart is a specialized and focused subset of a data warehouse. It supports specific analytical needs and is generally easier and quicker to design and implement compared to a data warehouse.&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-modeling&quot;&gt;What is Data Modeling?&lt;/h2&gt;

&lt;p&gt;The aim of the data model design is to represent the data in an easy way for reusability, flexibility, and scalability. The process of crafting data models involves 3 steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Conceptual&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Analysis and design phase to understand the key business entities and attributes and capture their interactions as per the business processes and rules within the organization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Logical&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Technology agnostic model that adds more details and relations between entities for how the conceptual model will be implemented.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Physical&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Defines how the logical model will be implemented using the system and technology in question to ensure that the writes and reads can be performed efficiently.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;normalization-vs-denormalization&quot;&gt;Normalization vs Denormalization&lt;/h2&gt;

&lt;p&gt;Normalization and Denormalization are two opposing approaches to database design.&lt;/p&gt;

&lt;h3 id=&quot;normalization&quot;&gt;Normalization&lt;/h3&gt;

&lt;p&gt;In normalization, the focus is on reducing data redundancy and non-consistency and improving data integrity. This is achieved by organizing data into separate related tables in accordance with normal forms.&lt;/p&gt;

&lt;p&gt;There are several normal forms each building upon the previous one and the main normalization norms are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;First normal form (1NF)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Each column is unique and has a single value. The table has a unique primary key.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Second normal form (2NF)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The requirements of 1NF, plus &lt;em&gt;partial dependencies&lt;/em&gt; are removed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Third normal form (3NF)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The requirements of 2NF, plus each table contains only relevant Fields related to its primary key and has no &lt;em&gt;transitive dependencies&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A &lt;em&gt;partial dependency&lt;/em&gt; occurs when a subset of fields in a composite key can be used to determine a non-key column of the table.&lt;/p&gt;

&lt;p&gt;For example, considering the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;StudentID&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;CourseID&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;StudentName&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;CourseName&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;InstructorName&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;101&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CS101&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;John&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Database&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Dr. Smith&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;102&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CS102&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Emma&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Networking&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Dr. Johnson&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;103&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;CS102&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;John&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Networking&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Dr. Johnson&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;The primary key is the combination of (StudentId, CourseID)&lt;/li&gt;
  &lt;li&gt;StudentName depends only on StudentID&lt;/li&gt;
  &lt;li&gt;CourseName depends only on CourseID&lt;/li&gt;
  &lt;li&gt;Here, we have two partial dependencies:
    &lt;ul&gt;
      &lt;li&gt;StudentID → StudentName&lt;/li&gt;
      &lt;li&gt;CourseID → CourseName&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A &lt;em&gt;transitive dependency&lt;/em&gt; occurs when a non-key field depends on another non-key field.&lt;/p&gt;

&lt;p&gt;For example, considering the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;EmployeeID&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;EmployeeName&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;DepartmentID&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;DepartmentName&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;E01&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Alice&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;D1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Sales&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;E02&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;D2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Marketing&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;E03&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;D1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Sales&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;The primary key is EmployeeID&lt;/li&gt;
  &lt;li&gt;EmployeeName depends on EmployeeID&lt;/li&gt;
  &lt;li&gt;DepartmentID depends on EmployeeID&lt;/li&gt;
  &lt;li&gt;DepartmentName depends on DepartmentID&lt;/li&gt;
  &lt;li&gt;Here, we have a transitive dependency:
    &lt;ul&gt;
      &lt;li&gt;EmployeeID → DepartmentID → DepartmentName&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most practical database designs typically aim for 3NF as it generally provides a good balance between data integrity and performance.&lt;/p&gt;

&lt;p&gt;In general, normalization is best suited for systems with frequent insert, update, and delete operations, like transactional databases. What about denormalization?&lt;/p&gt;

&lt;h3 id=&quot;denormalization&quot;&gt;Denormalization&lt;/h3&gt;

&lt;p&gt;Denormalization on the other hand focuses on improving query performance, especially for read-heavy workloads, by combining related data from multiple tables increasing data redundancy and storage requirements and decreasing the number of tables in the database.&lt;/p&gt;

&lt;p&gt;Denormalization is ideal for reporting systems and data warehouses where complex joins are expensive.&lt;/p&gt;

&lt;p&gt;The choice between normalization and denormalization depends on specific application needs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use normalization when data integrity and consistency are crucial, and when the database undergoes frequent updates.&lt;/li&gt;
  &lt;li&gt;Opt for denormalization when query performance is a priority, especially for read-intensive applications or when complex joins significantly slow down data retrieval.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-warehouse-modeling-techniques&quot;&gt;Data Warehouse Modeling Techniques&lt;/h2&gt;

&lt;p&gt;The three big approaches for modeling analytical data are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inmon&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Top-down approach&lt;/li&gt;
      &lt;li&gt;Advocates for creating a centralized, integrated &lt;em&gt;Enterprise Data Warehouse&lt;/em&gt; (EDW)&lt;/li&gt;
      &lt;li&gt;Uses a normalized structure (3NF) for the core warehouse where data is organized by subject reflecting the overall business structure&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Data Marts&lt;/em&gt; are created from the EDW as needed for analytical needs&lt;/li&gt;
      &lt;li&gt;The popular option for modeling the data mart is a &lt;strong&gt;Star Schema&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Rules how to build data warehouse, store and organise data (Bill Inmon, 1990):
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Integrated&lt;/strong&gt; = other data sources&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Subject oriented&lt;/strong&gt; = reorganise the data per subject&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Time variant&lt;/strong&gt; = data warehouse contains historical data&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Non volatile&lt;/strong&gt; = data warehouse remains stable between refreshes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kimball&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Bottom up approach&lt;/li&gt;
      &lt;li&gt;Focuses on building data marts that address specific business processes making the data mart the data warehouse itself&lt;/li&gt;
      &lt;li&gt;Uses a &lt;strong&gt;Star or Snowflake Schema&lt;/strong&gt; for the design of the data marts&lt;/li&gt;
      &lt;li&gt;Data is modulated with two general types of tables: &lt;strong&gt;Facts&lt;/strong&gt; and &lt;strong&gt;Dimensions&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;slowly changing dimension (SCD)&lt;/strong&gt; is necessary to track changes in dimensions&lt;/li&gt;
      &lt;li&gt;SCD three most common ones:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Type 1&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;Overwrite existing dimension records. This is super simple and means you have no access to the deleted historical dimension records&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Type 2&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;Keep a full history of dimension records. When a record changes, that specific record is flagged as changed, and a new dimension record is created that reflects the current status of the attributes&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Type 3&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;A type 3 SCD is similar to a type 2 SCD, but instead of creating a new Row, a change in a type 3 SCD creates a new field&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Vault&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Data Vaults organize data into three different types: &lt;strong&gt;hubs&lt;/strong&gt;, &lt;strong&gt;links&lt;/strong&gt;, and &lt;strong&gt;satellites&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Hubs represent core business entities. The primary key of Hub tables is usually derived by a combination of business concept ID, load date, and other metadata information&lt;/li&gt;
          &lt;li&gt;Links represent relationships between hubs. It has only the join keys. It is like a Factless Fact table in the dimensional model. No attributes - just join keys&lt;/li&gt;
          &lt;li&gt;Satellites store attributes about hubs or links. They have descriptive information on core business entities. They are similar to a normalized version of a Dimension table&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Data Vault is a “write-optimized” modeling style&lt;/li&gt;
      &lt;li&gt;Great fit for data lakes and lakehouse approach&lt;/li&gt;
      &lt;li&gt;Supports agile development approaches:
        &lt;ul&gt;
          &lt;li&gt;It can be easily extended without massive refactoring like the dimensional models&lt;/li&gt;
          &lt;li&gt;Additional hubs can be easily added to links and additional satellites can be added to a Hub with minimal changes&lt;/li&gt;
          &lt;li&gt;Existing ETL jobs need significantly less refactoring when the data model changes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;modeling-techniques-implementation-on-databricks-lakehouse-platform&quot;&gt;Modeling Techniques Implementation on Databricks Lakehouse Platform&lt;/h2&gt;

&lt;p&gt;A &lt;strong&gt;data Lakehouse&lt;/strong&gt; is a new, open architecture that combines the best elements of data lakes and data warehouses. The Databricks Lakehouse is a large-scale enterprise-level platform that can host many use cases and data products. Therefore, it can support many different data modeling styles for different purposes and both normalized Data Vault (write-optimized) and denormalized dimensional models (read-optimized) data modeling styles have their place.&lt;/p&gt;

&lt;p&gt;The application of both techniques can be especially recognized through the lenses of the &lt;strong&gt;Medallion Architecture&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The medallion architecture is a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables).&lt;/p&gt;

&lt;p&gt;The following diagram summarizes the mapping of different layers, their purpose, and applied modeling techniques in each:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/19_data_lakehouse_architecture.png&quot; alt=&quot;Data lakehouse architecture and modeling techniques&quot; /&gt;
&lt;em&gt;Figure 1: Data lakehouse architecture and modeling techniques - &lt;a href=&quot;https://www.databricks.com/blog/2022/06/24/data-warehousing-modeling-techniques-and-their-implementation-on-the-databricks-lakehouse-platform.html&quot;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First data is landed in the Bronze layer in raw format using the same models of source systems in order to get converted to delta lake format. Next it flows into the Silver layer where it gets aggregated using more normalized model. Finally, the gold layer is meant for read-optimized presentation layer using denormalized models.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we covered the three main data warehouse modeling techniques and relevant design concepts and how they are applied in some form of combination taking the example of Databricks lakehouse platform.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.snowflake.com/guides/difference-between-data-warehouse-and-data-mart/&quot;&gt;https://www.snowflake.com/guides/difference-between-data-warehouse-and-data-mart/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/glossary/medallion-architecture&quot;&gt;https://www.databricks.com/glossary/medallion-architecture&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html&quot;&gt;https://www.databricks.com/blog/2022/06/24/prescriptive-guidance-for-implementing-a-data-vault-model-on-the-databricks-lakehouse-platform.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this article we will go over the big three approaches for modeling analytical data.</summary></entry><entry><title type="html">Preparation Guide for Google Cloud Professional Data Engineer Certification</title><link href="https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification.html" rel="alternate" type="text/html" title="Preparation Guide for Google Cloud Professional Data Engineer Certification" /><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification.html">&lt;p&gt;&lt;em&gt;This article contains a collection of notes, mind maps and resources to support you while preparing for the google cloud professional data engineer certification.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/strong&gt;: The new Professional Data Engineer exam will be live starting November 13. The new version reflects updates to Google Cloud’s data storing, data sharing, and data governance and has less emphasis on operationalizing machine learning models. 
That being said, I believe most of the content is still relevant and can serve as a guide to assist you as you begin your preparation.&lt;/p&gt;

&lt;p&gt;So brace yourselves, this is gonna be a rather long post filled with too many images extracted from different parts of the mind maps I used for my preparation in order to make them easy to read and follow.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#google-cloud&quot; id=&quot;markdown-toc-google-cloud&quot;&gt;Google Cloud&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#infrastructure&quot; id=&quot;markdown-toc-infrastructure&quot;&gt;Infrastructure&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vpc-networks&quot; id=&quot;markdown-toc-vpc-networks&quot;&gt;VPC Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-transfer-services&quot; id=&quot;markdown-toc-data-transfer-services&quot;&gt;Data Transfer Services&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#resource-manager&quot; id=&quot;markdown-toc-resource-manager&quot;&gt;Resource Manager&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#security&quot; id=&quot;markdown-toc-security&quot;&gt;Security&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#compute&quot; id=&quot;markdown-toc-compute&quot;&gt;Compute&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#storage&quot; id=&quot;markdown-toc-storage&quot;&gt;Storage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ingestion-and-processing&quot; id=&quot;markdown-toc-ingestion-and-processing&quot;&gt;Ingestion and Processing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-pipelines-management&quot; id=&quot;markdown-toc-data-pipelines-management&quot;&gt;Data Pipelines Management&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-governance&quot; id=&quot;markdown-toc-data-governance&quot;&gt;Data Governance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#analytics&quot; id=&quot;markdown-toc-analytics&quot;&gt;Analytics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#machine-learning&quot; id=&quot;markdown-toc-machine-learning&quot;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ingestion-and-pocessing&quot; id=&quot;markdown-toc-ingestion-and-pocessing&quot;&gt;Ingestion and Pocessing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pubsub&quot; id=&quot;markdown-toc-pubsub&quot;&gt;Pub/Sub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataproc&quot; id=&quot;markdown-toc-dataproc&quot;&gt;Dataproc&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataflow&quot; id=&quot;markdown-toc-dataflow&quot;&gt;Dataflow&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storage-1&quot; id=&quot;markdown-toc-storage-1&quot;&gt;Storage&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-storage&quot; id=&quot;markdown-toc-cloud-storage&quot;&gt;Cloud Storage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-sql&quot; id=&quot;markdown-toc-cloud-sql&quot;&gt;Cloud SQL&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#query-insights&quot; id=&quot;markdown-toc-query-insights&quot;&gt;Query Insights&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-spanner&quot; id=&quot;markdown-toc-cloud-spanner&quot;&gt;Cloud Spanner&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#firestore&quot; id=&quot;markdown-toc-firestore&quot;&gt;Firestore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#datastore&quot; id=&quot;markdown-toc-datastore&quot;&gt;Datastore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#memorystore&quot; id=&quot;markdown-toc-memorystore&quot;&gt;Memorystore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bigtable&quot; id=&quot;markdown-toc-bigtable&quot;&gt;Bigtable&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bigquery&quot; id=&quot;markdown-toc-bigquery&quot;&gt;BigQuery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;google-cloud&quot;&gt;Google Cloud&lt;/h2&gt;

&lt;p&gt;Before we dive into the characteristics of Google Cloud services that will enable professional data engineers to design, build and operationalize data processing systems, let’s start with a 10,000-foot view on different topics that may be included in the exam.&lt;/p&gt;

&lt;h3 id=&quot;infrastructure&quot;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;Google Cloud services are available in different locations divided into &lt;strong&gt;Regions&lt;/strong&gt;.
Regions contain multiple &lt;strong&gt;Zones&lt;/strong&gt; where the resources are deployed and are isolated from one another so that failures in one zone do not affect other zones in a region. 
Most regions have at least three zones and can have more. All regions have at least two zones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_regions.png&quot; alt=&quot;Google Cloud Regions&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 1: Google Cloud Regions, Source: &lt;a href=&quot;https://cloud.google.com/about/locations#lightbox-regions-map&quot;&gt;https://cloud.google.com/about/locations#lightbox-regions-map&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Google data centers are connected with Google’s own high-speed network. Google is the only cloud provider that owns all the fiber connecting its data center together. A huge amount of the world’s internet traffic goes through Google’s network.&lt;/p&gt;

&lt;p&gt;In addition to the data centers, there are points of presence all over the world. They allow access to Google’s network where all messages are encrypted, secure and very fast.&lt;/p&gt;

&lt;p&gt;In addition to the POPs, Google runs a global caching system or CDN that consists of hundreds of more nodes. You can easily take advantage of this CDN to cache your content, thus increasing your application performance and decreasing your networking cost.&lt;/p&gt;

&lt;h3 id=&quot;vpc-networks&quot;&gt;VPC Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_vpc_networks.png&quot; alt=&quot;Google Cloud Platform VPC Networks&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 2: Google Cloud Platform VPC Networks&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-transfer-services&quot;&gt;Data Transfer Services&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_transfer_services.png&quot; alt=&quot;Google Cloud Platform Data Transfer Services&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 3: Google Cloud Platform Data Transfer Services&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resource-manager&quot;&gt;Resource Manager&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_resource_manager.png&quot; alt=&quot;Google Cloud Platform Resource Manager&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 4: Google Cloud Platform Resource Manager&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_security.png&quot; alt=&quot;Security in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 5: Security in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;compute&quot;&gt;Compute&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_compute.png&quot; alt=&quot;Google Cloud Platform Compute&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 6: Google Cloud Platform Compute&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;storage&quot;&gt;Storage&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_storage.png&quot; alt=&quot;Google Cloud Platform Storage&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 7: Google Cloud Platform Storage&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;ingestion-and-processing&quot;&gt;Ingestion and Processing&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_ingestion_and_processing.png&quot; alt=&quot;Ingestion and Processing in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 8: Ingestion and Processing in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-pipelines-management&quot;&gt;Data Pipelines Management&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_pipelines_management.png&quot; alt=&quot;Data Pipelines Management in Google Cloud Platform Data&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 9: Data Pipelines Management in Google Cloud Platform Data&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-governance&quot;&gt;Data Governance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_governance.png&quot; alt=&quot;Data Governance in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 10: Data Governance in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;analytics&quot;&gt;Analytics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_analytics.png&quot; alt=&quot;Analytics in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 11: Analytics in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_machine_learning.png&quot; alt=&quot;Machine Learning in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 12: Machine Learning in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;ingestion-and-pocessing&quot;&gt;Ingestion and Pocessing&lt;/h2&gt;

&lt;p&gt;As a professional data engineer, designing data processing systems requires building and operationalizing data pipelines by choosing the appropriate services to integrate new data sources and processing the data in batch or streaming fashion. In this section, we deep dive into services that will allow you to ingest data in real time and build data processing systems whether you are migrating on premises workloads or starting from scratch.&lt;/p&gt;

&lt;h3 id=&quot;pubsub&quot;&gt;Pub/Sub&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_pub_sub.png&quot; alt=&quot;Pub/Sub&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 13: Pub/Sub&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataproc&quot;&gt;Dataproc&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataproc.png&quot; alt=&quot;Dataproc 1/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 14: Dataproc 1/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataproc_2.png&quot; alt=&quot;Dataproc 2/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 15: Dataproc 2/2&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataflow&quot;&gt;Dataflow&lt;/h3&gt;

&lt;p&gt;It allows you to execute your Apache Beans pipelines on Google Cloud.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A managed service that provides the resources necessary to create pipelines
    &lt;ul&gt;
      &lt;li&gt;Defines &lt;em&gt;HOW&lt;/em&gt; to run the pipeline:
        &lt;ul&gt;
          &lt;li&gt;Optimizes the graph by fusing transforms for example for best execution path&lt;/li&gt;
          &lt;li&gt;Breaks jobs into units of work&lt;/li&gt;
          &lt;li&gt;Schedules them to various workers&lt;/li&gt;
          &lt;li&gt;Optimization is always ongoing
            &lt;ul&gt;
              &lt;li&gt;Units of work are continually rebalanced mid job which provides fault tolerance&lt;/li&gt;
              &lt;li&gt;autoscaling mid job&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Resources –both compute and storage– are deployed on demand and on a per job basis&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Apache Beam SDK, which provides the programming environment to make the creation of streaming and batch pipelines easier
    &lt;ul&gt;
      &lt;li&gt;Defines &lt;em&gt;WHAT&lt;/em&gt; has to be done&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow.png&quot; alt=&quot;Dataflow 1/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 16: Dataflow 1/3&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow_2.png&quot; alt=&quot;Dataflow 2/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 17: Dataflow 2/3&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow_3.png&quot; alt=&quot;Dataflow 3/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 18: Dataflow 3/3&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;storage-1&quot;&gt;Storage&lt;/h2&gt;

&lt;p&gt;One of a data engineer’s most important skills is choosing the right storage technology, which involves knowing how to use managed services and having a solid grasp of storage performance and pricing. To further optimize your data processing and cut expenses, consider data modeling, schema design, and data life cycle management. In this section we will delve into the many storage options provided by Google Cloud.&lt;/p&gt;

&lt;h3 id=&quot;cloud-storage&quot;&gt;Cloud Storage&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_storage.png&quot; alt=&quot;Cloud Storage 1/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 19: Cloud Storage 1/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_storage_2.png&quot; alt=&quot;Cloud Storage 2/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 20: Cloud Storage 2/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Google Cloud provides 3 ways to manage the KEK encryption key:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Google Managed Encryption Keys - GMEK: automatic encryption using Cloud KMS (Key Management Service)&lt;/li&gt;
  &lt;li&gt;Customer Managed Encryption Keys - CMEK: you control the creation and existance of the KEK key in KMS&lt;/li&gt;
  &lt;li&gt;Customer Supplied Encryption Keys - CSEK: you provide the KEK key&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cloud-sql&quot;&gt;Cloud SQL&lt;/h3&gt;

&lt;p&gt;Cloud SQL is a fully managed relational database service for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MySQL&lt;/li&gt;
  &lt;li&gt;PostgreSQL&lt;/li&gt;
  &lt;li&gt;Microsoft SQL&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_sql.png&quot; alt=&quot;Cloud SQL&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 21: Cloud SQL&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;query-insights&quot;&gt;Query Insights&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_insights.png&quot; alt=&quot;Query Insights&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 22: Query Insights&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;cloud-spanner&quot;&gt;Cloud Spanner&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_spanner.png&quot; alt=&quot;Cloud Spanner&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 23: Cloud Spanner&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;firestore&quot;&gt;Firestore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_firestore.png&quot; alt=&quot;Firestore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 24: Firestore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;datastore&quot;&gt;Datastore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_datastore.png&quot; alt=&quot;Datastore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 25: Datastore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;memorystore&quot;&gt;Memorystore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_memorystore.png&quot; alt=&quot;Memorystore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 26: Memorystore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;bigtable&quot;&gt;Bigtable&lt;/h3&gt;

&lt;p&gt;Bigtable is a fully managed NoSQL database service. It is suitable for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Storing &amp;gt; 1TB&lt;/li&gt;
  &lt;li&gt;High Throughput&lt;/li&gt;
  &lt;li&gt;Low latency random data access&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigtable.png&quot; alt=&quot;Bigtable&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 27: Bigtable&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;bigquery&quot;&gt;BigQuery&lt;/h2&gt;

&lt;p&gt;The last section is solely dedicated to BigQuery. BigQuery is a serverless and cost-effective data warehouse. It is deeply integrated with the GCP’s analytical and data processing offering, allowing customers to build an enterprise ready cloud native data warehouse. BigQuery is part of Google Cloud’s comprehensive data analytics platform that covers the analytics value chain from Ingest, process and store to advanced analytics and collaboration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery.png&quot; alt=&quot;BigQuery 1/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 28: BigQuery 1/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_2.png&quot; alt=&quot;BigQuery 2/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 29: BigQuery 2/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_3.png&quot; alt=&quot;BigQuery 3/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 30: BigQuery 3/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_4.png&quot; alt=&quot;BigQuery 4/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 31: BigQuery 4/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_5.png&quot; alt=&quot;BigQuery 5/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 32: BigQuery 5/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_6.png&quot; alt=&quot;BigQuery 6/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 33: BigQuery 6/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_7.png&quot; alt=&quot;BigQuery 7/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 34: BigQuery 7/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_8.png&quot; alt=&quot;BigQuery 8/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 35: BigQuery 8/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_9.png&quot; alt=&quot;BigQuery 9/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 36: BigQuery 9/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_10.png&quot; alt=&quot;BigQuery 10/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 37: BigQuery 10/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_11.png&quot; alt=&quot;BigQuery 11/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 38: BigQuery 11/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_12.png&quot; alt=&quot;BigQuery 12/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 39: BigQuery 12/12&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;Developer Cheat Sheet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://googlecloudcheatsheet.withgoogle.com/&quot;&gt;https://googlecloudcheatsheet.withgoogle.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Cloud Girl:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/priyankavergadia/GCPSketchnote&quot;&gt;https://github.com/priyankavergadia/GCPSketchnote&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Cloud Product list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/terms/services&quot;&gt;https://cloud.google.com/terms/services&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;21 products explained under 2 minutes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/inside-google-cloud/21-google-cloud-tools-each-explained-under-2-minutes&quot;&gt;https://cloud.google.com/blog/topics/inside-google-cloud/21-google-cloud-tools-each-explained-under-2-minutes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GCP Data Engineer Study Guide:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf&quot;&gt;https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Data Engineering Cheat Sheet on GCP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf&quot;&gt;https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Schema design best practices for Bigtable:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/bigtable/docs/schema-design&quot;&gt;https://cloud.google.com/bigtable/docs/schema-design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Optimize query computation for BigQuery:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/bigquery/docs/best-practices-performance-compute&quot;&gt;https://cloud.google.com/bigquery/docs/best-practices-performance-compute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Cloud Computing" /><summary type="html">This article contains a collection of notes, mind maps and resources to support you while preparing for the google cloud professional data engineer certification.</summary></entry><entry><title type="html">Learning How to Learn</title><link href="https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn.html" rel="alternate" type="text/html" title="Learning How to Learn" /><published>2023-10-21T00:00:00+00:00</published><updated>2023-10-21T00:00:00+00:00</updated><id>https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn</id><content type="html" xml:base="https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn.html">&lt;p&gt;&lt;em&gt;Notes from the online course learning how to learn: powerful mental tools to help you master tough subjects by Barbara Oakley.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/17_learning_how_to_learn.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Learning How to Learn Mindmap&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#main-takeaways&quot; id=&quot;markdown-toc-main-takeaways&quot;&gt;Main Takeaways&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-what-is-learning&quot; id=&quot;markdown-toc-1-what-is-learning&quot;&gt;1/ What is learning?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-memory&quot; id=&quot;markdown-toc-2-memory&quot;&gt;2/ Memory&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#working-memory&quot; id=&quot;markdown-toc-working-memory&quot;&gt;Working Memory&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#long-term-memory&quot; id=&quot;markdown-toc-long-term-memory&quot;&gt;Long Term Memory&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-how-to-improve-your-memory&quot; id=&quot;markdown-toc-3-how-to-improve-your-memory&quot;&gt;3/ How to improve your memory&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-chunking&quot; id=&quot;markdown-toc-4-chunking&quot;&gt;4/ Chunking&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#what-is-a-chunk&quot; id=&quot;markdown-toc-what-is-a-chunk&quot;&gt;What is a Chunk?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#how-to-form-a-chunk&quot; id=&quot;markdown-toc-how-to-form-a-chunk&quot;&gt;How to form a chunk?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-value-of-a-library-of-chunks&quot; id=&quot;markdown-toc-the-value-of-a-library-of-chunks&quot;&gt;The value of a Library of Chunks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#interleaving&quot; id=&quot;markdown-toc-interleaving&quot;&gt;Interleaving&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-procrastination&quot; id=&quot;markdown-toc-5-procrastination&quot;&gt;5/ Procrastination&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#what-is-a-habit&quot; id=&quot;markdown-toc-what-is-a-habit&quot;&gt;What is a habit?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#harnessing-your-zombies&quot; id=&quot;markdown-toc-harnessing-your-zombies&quot;&gt;Harnessing your zombies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#6-juggling-life-and-learning&quot; id=&quot;markdown-toc-6-juggling-life-and-learning&quot;&gt;6/ Juggling Life and Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In a world that’s constantly evolving, the ability to learn efficiently and effectively is a skill that can unlock doors to personal and professional growth. Whether you’re a student striving for academic excellence, a professional seeking to stay competitive in your career, or simply someone who wants to enhance their ability to acquire new knowledge, the &lt;strong&gt;&lt;a href=&quot;https://www.coursera.org/learn/learning-how-to-learn&quot;&gt;Learn How to Learn course&lt;/a&gt;&lt;/strong&gt; is a valuable resource that can empower you to do just that.&lt;/p&gt;

&lt;p&gt;This course has garnered attention from learners of all backgrounds. It delves into the science behind learning and equips you with proven strategies to enhance your learning abilities. As we explore the key takeaways from this course, you’ll discover how to overcome common obstacles, develop effective study habits, and harness the power of your brain to grasp new concepts with confidence.&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-what-is-learning&quot;&gt;1/ What is learning?&lt;/h3&gt;

&lt;p&gt;There are two different modes of thinking:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Focused&lt;/strong&gt;: concentrate intently on something you’re trying to learn or to understand&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Diffuse&lt;/strong&gt;: relaxed thinking style related to a set of neural resting states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you’re learning something new, especially something that’s a bit challenging, your mind needs to be able to go back and forth between the two different learning modes. Study something hard by focusing intently. Then take a break or at least change your focus to something different for a while. During this time of seeming relaxation, your brain’s diffuse mode has a chance to work away in the background and help you out with your conceptual understanding.&lt;/p&gt;

&lt;p&gt;Salvador Dali and Thomas Edison are two well-known innovators who leveraged interleaving between these phases. They would hold onto objects while falling asleep in their chairs. They would think of a problem and once they lost consciousness the object they held would fall to the floor and wake them up. Many of their imaginative ideas came from this state.&lt;/p&gt;

&lt;h3 id=&quot;2-memory&quot;&gt;2/ Memory&lt;/h3&gt;

&lt;h4 id=&quot;working-memory&quot;&gt;Working Memory&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;The part of memory that has to do with what you’re immediately and consciously processing in your mind. It is centred out of the prefrontal cortex. There are also connections to other parts of your brain so you can access long-term memories.&lt;/li&gt;
  &lt;li&gt;Working memory holds only about &lt;strong&gt;four items&lt;/strong&gt; at a time.&lt;/li&gt;
  &lt;li&gt;You often need to keep repeating what you’re trying to work with so it stays in your working memory; for example repeat a phone number to yourself until you have a chance to write it down. You may find yourself shutting your eyes to keep any other items from intruding into the limited slots of your working memory as you concentrate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;long-term-memory&quot;&gt;Long Term Memory&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Wide storage warehouse distributed over the brain.&lt;/li&gt;
  &lt;li&gt;It is immense and has so many items that they can bury each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To move information into long-term memory, it often takes time and practice. To help with this process use a technique called &lt;strong&gt;Spaced Repetition&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Research has shown that when you first try to put an item of information in long-term memory, you need to revisit it at least a few times to increase the chances that you’ll be able to find it later when you might need it.&lt;/p&gt;

&lt;p&gt;You might be surprised to learn that just plain being awake creates toxic products in your brain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sleep&lt;/strong&gt; is your brain’s way of keeping itself healthy. When you sleep, your brain cells shrink. This causes an increase in the space between your brain cells. Fluid can flow past these cells and wash the toxins out.&lt;/p&gt;

&lt;p&gt;Sleep is also an important part of the memory and learning process. It seems that during sleep, your brain tidies up ideas and concepts you’re thinking about and learning. It erases the less important parts of memory and simultaneously strengthens areas that you need or want to remember. During sleep, your brain also rehearses some of the tougher parts of whatever you’re trying to learn, going over and over neural patterns to deepen and strengthen them.&lt;br /&gt;
Sleep has also been shown to make a remarkable difference in your ability to figure out difficult problems and to understand what you’re trying to learn.&lt;/p&gt;

&lt;h3 id=&quot;3-how-to-improve-your-memory&quot;&gt;3/ How to improve your memory&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Simply looking away and seeing what you can recall from the material you’ve just read is a more productive approach than simply rereading. Using recall, mental retrieval of the key ideas, rather than passive rereading, will make your study time more focused and effective. The only time rereading text seems to be effective, is if you let time pass between the rereading, so that it becomes more of an exercise in spaced repetition.&lt;/li&gt;
      &lt;li&gt;Another tip is recalling material when you are outside your usual place of study can also help you strengthen your grasp of the material. You don’t realize it, but when you are learning something new you can often take in subliminal cues for the room and the space around you at the time you were originally learning the material.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mnemonics&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Let’s say you want to remember four plants that help ward off vampires; garlic, rose, hawthorn, and mustard. The first letters abbreviate to GRHM. so all you need to do to remember is to use the image of Graham cracker.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Highlighting and Underlining&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;It must be done very carefully. Otherwise it can not only be ineffective, but also misleading. If you do mark up the text, try to look for main ideas before making any marks. And try to keep your underlining or highlighting to a minimum. On the other hand, words or notes in a margin that synthesizes key concepts are a very good idea.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual/Spatial Memory&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Our mind is built for visual/spatial memory. For example, if you were asked to look around a house you never visited before, you’d soon have a sense of the general furniture layout. We can tap into this memory by associating strange and funny images to memorize a concept. We can use spaced repetition to commit this relationship to long term memory. The more senses you use the easier it is to commit to memory.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory Palace Technique&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Is a particularly powerful way of grouping things you want to remember. It involves calling to mind a familiar place like the layout of your house and using it as a visual notepad where you can deposit the concept images that you want to remember. You would imagine yourself walking through a place you know well, coupled with shockingly memorable images of what you want to remember.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Memory tricks allow people to expand their working memory with easy access to long term memory. What’s more, the memory process itself becomes an exercise in creativity. The more you memorize using these innovative techniques, the more creative you become.&lt;/p&gt;

&lt;h3 id=&quot;4-chunking&quot;&gt;4/ Chunking&lt;/h3&gt;

&lt;h4 id=&quot;what-is-a-chunk&quot;&gt;What is a Chunk?&lt;/h4&gt;

&lt;p&gt;Chunking is the mental leap that helps you unite bits of information together through meaning. The new logical whole makes the chunk easier to remember, and also makes it easier to fit the chunk into the larger picture of what you’re learning.&lt;/p&gt;

&lt;p&gt;Once you chunk an idea, a concept, or an action, you don’t need to remember all the little underlying details. You’ve got the main idea, the chunk, and that’s enough.&lt;/p&gt;

&lt;h4 id=&quot;how-to-form-a-chunk&quot;&gt;How to form a chunk?&lt;/h4&gt;

&lt;p&gt;The best chunks are the ones that are so well ingrained that you don’t even have to consciously think about connecting the neural patterns together. That actually is the point of making complex ideas, movements or reactions into a single chunk.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Focus your undivided attention on the information you want to chunk (limited short term memory)&lt;/li&gt;
  &lt;li&gt;Understand the basic idea of what you want to chunk.
    &lt;ul&gt;
      &lt;li&gt;It helps hold the underlying memory traces together&lt;/li&gt;
      &lt;li&gt;It creates broad encompassing traces that can link to other memory traces&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Practice and repetition with context
    &lt;ul&gt;
      &lt;li&gt;Context means going beyond the initial problem and seeing more broadly&lt;/li&gt;
      &lt;li&gt;Helps you see how your new formed chunks fit in the bigger picture&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is as if you have an attention octopus that slips its tentacles through those four slots of working memory when necessary to help you make connections to information that you might have in various parts of your brain.&lt;/p&gt;

&lt;h4 id=&quot;the-value-of-a-library-of-chunks&quot;&gt;The value of a Library of Chunks&lt;/h4&gt;

&lt;p&gt;Basically what people do to enhance their knowledge and gain expertise, is to gradually build the number of chunks in their mind, valuable bits of information they can piece together in new and creative ways.&lt;/p&gt;

&lt;p&gt;Chunks can also help you understand new concepts. This is because when you grasp one chunk, you’ll find that that chunk can be related in surprising ways to similar chunks, not only in that field but also in very different fields. This idea is called transfer.&lt;/p&gt;

&lt;p&gt;If you have a library of concepts and solutions internalized as chunked patterns, you can think of it as a collection or a library of neural patterns. When you’re trying to figure something out, if you have a good library of these chunks, you can more easily skip to the right solution by, metaphorically speaking, listening to whispers from your diffuse mode. Your diffuse mode can help you connect two or more chunks together in new ways to solve novel problems. Another way to think of it is this, as you build each chunk it is filling in a part of your larger knowledge picture, but if you don’t practice with your growing chunks, they can remain faint and it’s harder to put together the big picture of what you’re trying to learn.&lt;/p&gt;

&lt;p&gt;There are two ways to figure something out or to solve problems. First, through sequential step-by-step reasoning and second, through a more holistic intuition. Sequential thinking where each small step leads deliberately towards a solution, involves the focused mode. Intuition on the other hand often seems to require this creative diffuse mode linking of several seemingly different focused mode thoughts. Most difficult problems and concepts are grasped through intuition, because these new ideas make a leap away from what you’re familiar with. Keep in mind that the diffuse modes, semi-random way of making connections means that the solutions it provides should be very carefully verified using the focused mode. Intuitive insights aren’t always correct.&lt;/p&gt;

&lt;h4 id=&quot;interleaving&quot;&gt;Interleaving&lt;/h4&gt;

&lt;p&gt;Mastering a new subject means learning not only the basic chunks, but also learning how to select and use different chunks. The best way to learn is by practicing jumping back and forth between problems or situations that require different techniques or strategies. 
Interleaving is extraordinarily important. Although practice and repetition is important in helping build solid neural patterns to draw on, it’s interleaving that starts building flexibility and creativity.&lt;/p&gt;

&lt;h3 id=&quot;5-procrastination&quot;&gt;5/ Procrastination&lt;/h3&gt;

&lt;p&gt;Procrastination is the act of unnecessarily and voluntarily delaying or postponing something despite knowing that there will be negative consequences for doing so.&lt;/p&gt;

&lt;p&gt;Procrastination is an easy habit to develop because of the reward. It shares features with addiction. It offers temporary excitement and relief from sometimes boring reality.&lt;/p&gt;

&lt;h4 id=&quot;what-is-a-habit&quot;&gt;What is a habit?&lt;/h4&gt;

&lt;p&gt;Habit is an energy saver for us. It allows us to free our mind for other types of activities. You go into this habitual zombie far more often than you might think, that’s the point of habit. You don’t have to think in a focused manner about what you are doing while you are performing the habit, it saves energy. 
Habits can be good and bad, they can be brief like absently brushing back your hair or they can be long for example when you take a walk or watch television for a few hours after you get home from work.&lt;/p&gt;

&lt;p&gt;You can think of habits as having 4 parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The cue&lt;/strong&gt;: this is the trigger that launches you into zombie mode&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The routine&lt;/strong&gt;: what we do in reaction to that cue. This is your zombie mode. Zombie responses can be useful, harmless or sometimes harmful.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The reward&lt;/strong&gt;: every habit develops and continues because it rewards us. It gives us an immediate little feeling of pleasure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The belief&lt;/strong&gt;: habits have power because your belief in them. To change a habit you need to change your underlying belief.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you find yourself avoiding certain tasks because they make you feel uncomfortable, you should know there’s another helpful way to re-frame things and that’s to learn to focus on process not product. Process means the flow of time and the habits and actions associated with that flow of time. As in, I’m going to spend 20 minutes working. Product is an outcome, for example, a homework assignment that you need to finish. To prevent procrastination you want to avoid concentrating on the product. Instead your attention should be on building processes. 
The essential idea here is that the zombie habitual part of your brain likes processes because it can march mindlessly along.&lt;/p&gt;

&lt;h4 id=&quot;harnessing-your-zombies&quot;&gt;Harnessing your zombies&lt;/h4&gt;

&lt;p&gt;The trick to overriding a habit is to look to change your reaction to a cue. The only place you need to apply will power is to change your reaction to the cue. To understand that, it helps to go back through the four components of habit and we analyze them from the perspective of procrastination:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cue
    &lt;ul&gt;
      &lt;li&gt;It usually falls into one of the following categories: location, time, how you feel and reactions, either to other people or to something that just happened. Do you look something up on the web and then find yourself web surfing? The issue with procrastination is that, because it’s an automatic habit, you’re often unaware that you’ve begun to procrastinate. You can prevent the most damaging cues by shutting off your cell phone or keeping away from the internet and other distractions for brief periods of time, as when you’re doing a pomodoro.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Routine
    &lt;ul&gt;
      &lt;li&gt;The key to rewiring is to have a plan. Developing a new ritual can be helpful. Some students make it a habit to leave their phone in their car when they head for class which removes a potent distraction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The reward
    &lt;ul&gt;
      &lt;li&gt;It helps to add a new reward if you want to overcome your previous cravings. Only once your brain starts expecting that reward, will the important rewiring take place that will allow you to create new habits.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The belief
    &lt;ul&gt;
      &lt;li&gt;The most important part of changing your procrastination habit is the belief that you can do it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;6-juggling-life-and-learning&quot;&gt;6/ Juggling Life and Learning&lt;/h3&gt;

&lt;p&gt;A good way for you to keep perspective about what you’re trying to learn and accomplish is to once a week write a brief weekly list of key tasks in a planner journal. Then each day on another page of your planner, write a list of the tasks that you can reasonably work on or accomplish (The list should be short, 6 items for example where some are process oriented and others product oriented). Try to write this daily task list the evening before. Why? Research has shown that this helps your subconscious to grapple with the tasks on the list, so you can figure out how to accomplish them. Writing the list before you go to sleep enlists your zombies to help you accomplish the items on the list the next day. If you don’t write your tasks down on a list, they lurk at the edge of the four or so slots in your working memory, taking up valuable mental real estate. But once you make a task list, it frees working memory for problem-solving.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading, I hope you enjoyed it!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://barbaraoakley.com/books/learning-how-to-learn/&quot;&gt;https://barbaraoakley.com/books/learning-how-to-learn/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.brainfacts.org/&quot;&gt;https://www.brainfacts.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Procrastination&quot;&gt;https://en.wikipedia.org/wiki/Procrastination&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="notes" /><category term="General" /><summary type="html">Notes from the online course learning how to learn: powerful mental tools to help you master tough subjects by Barbara Oakley.</summary></entry><entry><title type="html">Data Processing Architectures: Lambda vs Kappa</title><link href="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html" rel="alternate" type="text/html" title="Data Processing Architectures: Lambda vs Kappa" /><published>2023-09-24T00:00:00+00:00</published><updated>2023-09-24T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html">&lt;p&gt;&lt;em&gt;In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our data-driven era, the ability to harness the power of data has become a pivotal competitive advantage for businesses and organizations across industries. The vast volumes of information generated daily present both opportunities and challenges. How do we efficiently process, analyze, and derive insights from this deluge of data in real-time, without drowning in complexity?&lt;/p&gt;

&lt;p&gt;This is where data processing architectures come into play, offering structured approaches to these challenges. In this blog post, we will explore two prominent contenders in the realm of data processing: the &lt;strong&gt;Lambda&lt;/strong&gt; and &lt;strong&gt;Kappa&lt;/strong&gt; architectures.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-architecture&quot; id=&quot;markdown-toc-lambda-architecture&quot;&gt;Lambda Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#advantages-and-disadvantages&quot; id=&quot;markdown-toc-advantages-and-disadvantages&quot;&gt;Advantages and Disadvantages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kappa-architecture&quot; id=&quot;markdown-toc-kappa-architecture&quot;&gt;Kappa Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#advantages-and-disadvantages-1&quot; id=&quot;markdown-toc-advantages-and-disadvantages-1&quot;&gt;Advantages and Disadvantages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#choosing-the-right-architecture&quot; id=&quot;markdown-toc-choosing-the-right-architecture&quot;&gt;Choosing the Right Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;lambda-architecture&quot;&gt;Lambda Architecture&lt;/h2&gt;

&lt;p&gt;Lambda architecture was introduced by &lt;em&gt;Nathan Marz&lt;/em&gt; to address the challenges of data processing in a scalable and fault-tolerant manner.
The architecture takes an event stream and forks/duplicates it into two relatively independent layers called the &lt;strong&gt;Batch Layer&lt;/strong&gt; and the &lt;strong&gt;Speed Layer&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Batch layer&lt;/strong&gt; takes incoming data, combines it with historical data, and recomputes the results by iterating over the entire dataset thus allowing the system to give the most accurate results. However, the results are achieved at the expense of high latency due to the long computation time.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Speed Layer&lt;/strong&gt; on the other hand is used to provide a low-latency, near-real-time result. It performs incremental updates on data that was not processed in the last batch of the Batch Layer.&lt;/p&gt;

&lt;p&gt;The results from both systems constitute the &lt;strong&gt;Serving Layer&lt;/strong&gt;. It is responsible for serving queryable, up-to-date results to users or applications. In this layer, the query aims at merging and analyzing data from both the Batch Layer view and the incremental flow view from the Speed Layer.&lt;/p&gt;

&lt;p&gt;There are two variations on this: a &lt;strong&gt;unified serving layer&lt;/strong&gt; with one database for both outputs or &lt;strong&gt;separate serving layers&lt;/strong&gt; with two different databases, one optimized for real time and the other optimized for batch updates.&lt;/p&gt;

&lt;p&gt;The following diagram shows the lambda architecture at a high level:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_lambda_architecture.png&quot; alt=&quot;High Level Lambda Architecture&quot; /&gt;
&lt;em&gt;Figure 1: High Level Lambda Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;One of the key challenges in streaming is the reprocessing of the data. This can be due to code changes because your application evolves and you need to update the business logic or because you found a bug and you need to fix it. In either way, you will need to recompute your output to see the effect of these changes. The batch layer in the lambda architecture addresses this challenge by having a complete history of immutable data. In addition, the usage of two separate systems for processing data makes the lambda architecture flexible, easily scalable and fault tolerant. For instance, it can be used for a variety of use cases, including real-time analytics using the stream processing system and machine learning where models can leverage the large volume of data through the batch layer to generate more accurate results. If one system fails, say the batch processing system, the other can continue to operate providing real time insights into the data. Lastly, both systems can be scaled independently by either adding more nodes to the cluster or adding more streams.&lt;/p&gt;

&lt;p&gt;However, managing two separate processing systems is very complex. We need to provision and manage the infrastructure for two distributed systems including monitoring and logging which increases the cost and operations efforts of storage, compute and networking. Also, we need to align the business logic across streaming and batch codebases resulting in writing the same logic in two places with, most likely, different languages. This leads to difficult debugging and a challenge in validating data quality and making sure that the algorithms in each layer are matching.&lt;/p&gt;

&lt;p&gt;So, what’s different in Kappa architecture?&lt;/p&gt;

&lt;h2 id=&quot;kappa-architecture&quot;&gt;Kappa Architecture&lt;/h2&gt;

&lt;p&gt;The Kappa architecture was introduced by &lt;em&gt;Jay Kreps&lt;/em&gt;, co-founder and CEO at Confluent, a company built around the open source messaging system Apache Kafka, as a response to some of the challenges and complexities associated with the Lambda Architecture. 
The Kappa Architecture primarily focuses on stream processing simplifying the complexity of maintaining two systems with a single technology stack, referred to as &lt;strong&gt;Stream Processing Layer&lt;/strong&gt; in the diagram below, that can perform both real-time and batch processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_kappa_architecture.png&quot; alt=&quot;High Level Kappa Architecture&quot; /&gt;
&lt;em&gt;Figure 2: High Level Kappa Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages-1&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;The shift to a single stream processing system to handle both real-time and batch processing makes the Kappa architecture simpler, more efficient and cost effective than the lambda architecture. Having a single source of truth to all the data reduces both the burden of maintaining two separate systems and two codebases as well as the underlying costs. People can now develop, test, debug, and operate their systems on top of a single processing framework such as Apache Kafka.&lt;/p&gt;

&lt;p&gt;Stream processing is considered a paradigm shift from the traditional batch data processing. Therefore, it goes without saying that the Kappa architecture presents some challenges and limitations. In fact, processing out of order data or intricate joins combining many streams causes difficulties when transforming data in a streaming method. On top of that, data reprocessing which is now running using a single codebase, on the same framework, and with the same input data, still comes with some tradeoffs. As Jay Kreps detailed in his 
&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;original post&lt;/a&gt;, we can leverage Apache Kafka retention period (30 days for example) and take the retained data as an input to a second instance of the streaming process that will produce a new output table with the reprocessed data. However, this approach depends heavily on the configured retention period value and is limited in cases where we need to fix the algorithm or deploy a change like adding a new field that goes beyond the span of the retention period.&lt;/p&gt;

&lt;h2 id=&quot;choosing-the-right-architecture&quot;&gt;Choosing the Right Architecture&lt;/h2&gt;

&lt;p&gt;So when should we use one architecture or the other? As is often the case, it depends on some peculiarities of the implemented application.&lt;/p&gt;

&lt;p&gt;A very simple case is when the algorithms used for real-time and historical data are identical and implementable on streaming. It is then clearly very advantageous to use the same codebase to process historical and real-time data, and hence use the Kappa Architecture.
If the algorithms used to process historical data and real-time data are not always identical. Here, the choice between Lambda and Kappa becomes a tradeoff between the performance benefits of batch processing over a simpler codebase.&lt;/p&gt;

&lt;p&gt;Some examples of use cases where Kappa architecture is a good fit include fraud detection to detect fraudulent transactions in real time, process data from IoT devices in real time or recommendation engines used to to provide personalized recommendations to users in real time.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Lambda and Kappa are two popular data processing architectures that can be used to handle huge data. The ideal architecture relies on the individual needs for a given use case. We have discussed some benefits as well as drawbacks and limitations of each design to assist you in making your decision. Real-time insights are more important for businesses that want to become data-driven, which has increased the popularity of event streaming architecture. Batch processing will not go away, therefore it is essential to view the two architectures as complementing solutions rather than one being a cure for all ills.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&quot;&gt;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&quot;&gt;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.</summary></entry><entry><title type="html">Understanding Modern Data Stack</title><link href="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html" rel="alternate" type="text/html" title="Understanding Modern Data Stack" /><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html">&lt;p&gt;&lt;em&gt;In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The modern data stack is designed to empower organizations to harness the full potential of their data assets, make data-driven decisions, and stay competitive in today’s data-centric business landscape. It represents a shift towards more agile, integrated, and scalable data management practices.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#a-brief-history&quot; id=&quot;markdown-toc-a-brief-history&quot;&gt;A Brief History&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-the-modern-data-stack&quot; id=&quot;markdown-toc-what-is-the-modern-data-stack&quot;&gt;What is the Modern Data Stack?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#objectives-of-the-modern-data-stack&quot; id=&quot;markdown-toc-objectives-of-the-modern-data-stack&quot;&gt;Objectives of the Modern Data Stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#components-of-the-modern-data-stack&quot; id=&quot;markdown-toc-components-of-the-modern-data-stack&quot;&gt;Components of the Modern Data Stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;a-brief-history&quot;&gt;A Brief History&lt;/h2&gt;

&lt;p&gt;The modern data stack as we currently know it is a very recent development in data. In fact the rise of the cloud data warehouse triggered by the release of Amazon Redshift in late 2012 is considered one of the key developments that led to the adoption of the modern data stack. Redshift was one of the early cloud-based data warehousing solutions that offered a highly scalable and cost-effective platform for storing and analyzing large datasets. Its introduction marked a shift away from traditional on-premises data warehousing and towards cloud-based solutions. All of the other solutions in the market today like Google BigQuery and Snowflake followed the revolution set by Amazon.&lt;/p&gt;

&lt;p&gt;Consequently this shift led to the move from &lt;em&gt;Extract Transform Load (ETL)&lt;/em&gt; to &lt;em&gt;Extract Load Transform (ELT)&lt;/em&gt; pipelines. As storage becomes cheaper and more accessible, there is no need to deal with data transformations before saving it in the traditional data warehouse. Organizations just dump their data in its raw format and only apply the transformations later when needed.&lt;/p&gt;

&lt;p&gt;The rise of the cloud data warehouse has not only contributed to the transition from ETL to ELT but also the widespread adoption of BI tools. These self serve solutions democratize data usage allowing more and more personas to access it and make data-driven business decisions.‍&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-modern-data-stack&quot;&gt;What is the Modern Data Stack?&lt;/h2&gt;

&lt;p&gt;The modern data stack is a collection of tools and technologies used together to support the data flow starting from ingestion and integration of different data sources up to analysis in order to extract insights and help create data driven decisions. The particularity resides in the plug and play nature of its components and the overall ease of use without much infrastructure and data platform management overhead so that data is accessible for everyone to turn it into knowledge.&lt;/p&gt;

&lt;h2 id=&quot;objectives-of-the-modern-data-stack&quot;&gt;Objectives of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;The objectives of the modern data stack revolve around building a data infrastructure that enables organizations to efficiently and effectively manage their data, derive valuable insights, and make data-driven decisions.&lt;/p&gt;

&lt;p&gt;Here are the primary objectives of implementing a modern data stack:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Accommodate Growing Data - The modern data stack should be able to handle increasing data volumes, whether structured or unstructured, as organizations collect more data from various sources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility and Agility&lt;/strong&gt;: Easily Adapt to Changing Needs - It should be flexible enough to adapt to changing business requirements, data sources, and processing methods without the need for a complete overhaul.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Seamlessly Connect Data Sources: The stack should provide tools and processes for integrating data from diverse sources, including databases, applications, APIs, and more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost-Efficiency&lt;/strong&gt;: Optimize Resource Usage by minimising unnecessary resource usage by efficiently managing data storage and processing to control costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: Ensure Compatibility - Ensure that the various components of the stack can interoperate smoothly with each other and with external systems or tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cloud-Ready&lt;/strong&gt;: Leverage Cloud Infrastructure - Be compatible with cloud-based infrastructure to take advantage of scalability, cost-effectiveness, and the latest data services provided by cloud providers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance and Reliability&lt;/strong&gt;: Maintain High Performance - Deliver reliable and high-performance data processing to support critical business operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adaptability to Emerging Technologies&lt;/strong&gt;: Be Open to Innovation - Keep an eye on emerging technologies and trends, allowing for easy integration with new tools or platforms that may enhance the stack’s capabilities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;components-of-the-modern-data-stack&quot;&gt;Components of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;In a previous blog post about the &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;fundamentals of data engineering&lt;/a&gt; we tried to identify a common data flow that identifies the different stages including ingestion, storage, transformation, data management/governance, visualization and exploration  which are  involved in making data easily accessible.&lt;/p&gt;

&lt;p&gt;The components of the modern data stack can be perfectly mapped to the same diagram from the mentioned article, at least at a high level first, as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/15_modern_data_stack_example.png&quot; alt=&quot;Example of Componentes of Modern Data Stack in Standard Data Flow&quot; /&gt;
&lt;em&gt;Figure 1: Example of Componentes of Modern Data Stack in Standard Data Flow&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that the specific tools are changing and evolving rapidly but they usually include some of the ones we chose as an example in the diagram. In addition, some vendor technologies fit beyond a single stage as presented in the diagram and offer more capabilities such as data governance and/or machine learning.&lt;/p&gt;

&lt;p&gt;However, this only captures the stack at a high level. In fact, there is no one-size-fits-all approach when it comes to selecting the best tools and technologies to deal with your data. Every organization has a different level of data maturity, different data teams, different structures, processes, and so on.&lt;/p&gt;

&lt;p&gt;Therefore, the stack can be enriched with a &lt;strong&gt;workflow  orchestration tool&lt;/strong&gt; such as &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Apache Airflow&lt;/a&gt; or &lt;a href=&quot;https://dagster.io/&quot;&gt;Dagster&lt;/a&gt; needed to schedule your transformation in an automated fashion depending on your required frequency.&lt;/p&gt;

&lt;p&gt;Also &lt;strong&gt;data observability&lt;/strong&gt; has become a key part of the modern data stack. It ensures data reliability by monitoring data quality and identifying potential data issues throughout the stack. Explaining this concept is beyond the scope of this article but we will have a dedicated blog post about. Some of the tools worth mentioning here include &lt;a href=&quot;https://www.montecarlodata.com/&quot;&gt;Monte Carlo&lt;/a&gt; and &lt;a href=&quot;https://www.datadoghq.com/&quot;&gt;Datadog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we explored the concept of the modern data stack and its significance in the contemporary data landscape. We provided an overview of its main components and how they correlate to our standard data flow established in a previous article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/modern-data-stack/&quot;&gt;https://preset.io/blog/modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/reshaping-data-engineering/&quot;&gt;https://preset.io/blog/reshaping-data-engineering/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&quot;&gt;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&quot;&gt;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><category term="Cloud Computing" /><summary type="html">In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.</summary></entry><entry><title type="html">Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide</title><link href="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html" rel="alternate" type="text/html" title="Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide" /><published>2023-08-21T00:00:00+00:00</published><updated>2023-08-21T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html">&lt;p&gt;&lt;em&gt;In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Progressive Web Apps (PWAs) represent a significant advancement in web application development, offering users a seamless and engaging experience that combines the best aspects of both web and native mobile applications. 
By transforming your Jekyll-based blog into a PWA, you’ll enhance user engagement, improve performance, and enable key features such as offline access.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-are-progressive-web-apps&quot; id=&quot;markdown-toc-what-are-progressive-web-apps&quot;&gt;What are Progressive Web Apps?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#key-attributes-of-pwas&quot; id=&quot;markdown-toc-key-attributes-of-pwas&quot;&gt;Key Attributes of PWAs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#successful-pwa-examples&quot; id=&quot;markdown-toc-successful-pwa-examples&quot;&gt;Successful PWA Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#technical-features-of-pwas&quot; id=&quot;markdown-toc-technical-features-of-pwas&quot;&gt;Technical features of PWAs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#creating-a-manifest-file&quot; id=&quot;markdown-toc-creating-a-manifest-file&quot;&gt;Creating a Manifest File&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#implementing-service-workers&quot; id=&quot;markdown-toc-implementing-service-workers&quot;&gt;Implementing Service Workers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#deploying-service-workers&quot; id=&quot;markdown-toc-deploying-service-workers&quot;&gt;Deploying Service Workers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;what-are-progressive-web-apps&quot;&gt;What are Progressive Web Apps?&lt;/h2&gt;

&lt;p&gt;Platform-specific apps are developed for a specific operating system (OS) and/or class of devices, like an iOS or Android device. They are usually installed on the user’s device using the vendor’s app store. Websites on the other hand, can only be accessed by the user opening the browser and navigating to the site, and is highly dependent on network connectivity.&lt;/p&gt;

&lt;p&gt;So how does this relate to PWAs?&lt;/p&gt;

&lt;p&gt;Progressive web apps combine the best features of traditional websites and platform-specific apps. At their core, PWAs are web applications that take advantage of modern web technologies to deliver a reliable, fast, and immersive experience to users. Unlike traditional websites, PWAs can be installed on users’ devices, giving them a direct pathway to your content, even without a traditional app store. PWAs can be accessed through web browsers, but they offer the responsiveness and fluidity that users typically expect from native mobile applications.&lt;/p&gt;

&lt;h2 id=&quot;key-attributes-of-pwas&quot;&gt;Key Attributes of PWAs&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Offline Access&lt;/strong&gt;: One of the most significant advantages of PWAs is their ability to work offline or in low-network conditions. Users can still access content and navigate within the app, even when they’re not connected to the internet. This offline capability ensures that your content remains accessible, enhancing user satisfaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fast Loading&lt;/strong&gt;: PWAs are designed to load quickly, providing an almost instant experience to users. This is achieved through techniques like efficient caching, optimized assets, and lazy loading of content. Fast loading times lead to lower bounce rates and higher user retention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Responsive Design&lt;/strong&gt;: PWAs are responsive by default, adapting to various screen sizes and orientations. This responsiveness ensures a consistent and visually appealing experience across devices, including smartphones, tablets, and desktops.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engagement and Retention&lt;/strong&gt;: PWAs can be “installed” on users’ home screens or app drawers, creating a sense of ownership and encouraging repeated visits. This increased engagement can lead to higher retention rates, as users have easy access to your PWA.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;successful-pwa-examples&quot;&gt;Successful PWA Examples&lt;/h2&gt;

&lt;p&gt;Numerous companies have embraced PWAs to enhance user experience and drive business growth. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pinterest&lt;/strong&gt;: Pinterest’s PWA increased user engagement, with faster load times leading to a 60% increase in user engagement and a 44% increase in user-generated ad revenue.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alibaba&lt;/strong&gt;: Alibaba.com’s PWA achieved a 76% increase in conversions across browsers, with 14% more monthly active users on iOS and a 30% increase in mobile users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, check the links in the resources section.&lt;/p&gt;

&lt;h2 id=&quot;technical-features-of-pwas&quot;&gt;Technical features of PWAs&lt;/h2&gt;

&lt;p&gt;Because PWAs are websites, they have the same basic features as any other website: at least one HTML page, which very probably loads some CSS and JavaScript.&lt;/p&gt;

&lt;p&gt;Beyond that, a PWA has some additional features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;web app manifest file&lt;/strong&gt;, which, at a minimum, provides information that the browser needs to install the PWA, such as the app name and icon.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;service worker&lt;/strong&gt;, which, at a minimum, provides a basic offline experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having this in mind, a blog is nothing more than static HTML, CSS, and Javascript files. This makes it a prime candidate for adding PWA features which is the focus of the rest of this blog post.&lt;/p&gt;

&lt;p&gt;We will be adding these features to a blog built using Jekyll, a free and open source static site generator, and hosted on Github Pages. For more in depth guides on how I started my journey building this blog, you can check my 4 Parts series starting from &lt;a href=&quot;https://www.firasesbai.com/articles/2021/10/07/how-i-started-this-blog.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-manifest-file&quot;&gt;Creating a Manifest File&lt;/h2&gt;

&lt;p&gt;The Web App Manifest is a JSON document that provides application metadata such as its name, icon, and other details, which browsers can use when adding the PWA to the home screen.&lt;/p&gt;

&lt;p&gt;If you have previously generated a favicon to your blog using &lt;a href=&quot;https://realfavicongenerator.net/&quot;&gt;https://realfavicongenerator.net/&lt;/a&gt; or similar, you should already have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;site.webmanifest&lt;/code&gt; file that might be complete or miss a few properties. Otherwise, you can just use the following link &lt;a href=&quot;https://app-manifest.firebaseapp.com/&quot;&gt;https://app-manifest.firebaseapp.com/&lt;/a&gt; for reference or to generate its content.&lt;/p&gt;

&lt;p&gt;In either cases, the next step would be to add a reference to the manifest file under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file in the head section as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;manifest&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/assets/favicon/site.webmanifest&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
   
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The location of your manifest file might differ and you should update it accordingly. In my case, the manifest file is placed under assets along with the icons and favicon.&lt;/p&gt;

&lt;h2 id=&quot;implementing-service-workers&quot;&gt;Implementing Service Workers&lt;/h2&gt;

&lt;p&gt;A service worker is a powerful web technology that acts as a scriptable network proxy between a web application (such as a website) and the browser. It runs in the background, separate from the main web page, and allows you to intercept and control network requests and responses, enabling advanced features like offline access, caching, and push notifications in web applications.&lt;/p&gt;

&lt;p&gt;We will be using &lt;a href=&quot;https://developer.chrome.com/docs/workbox/&quot;&gt;Workbox&lt;/a&gt; which is a set of Javascript modules created by Google that simplifies and addresses a specific aspect of service worker development.&lt;/p&gt;

&lt;p&gt;1- Download and install the latest version of Node.js from the &lt;a href=&quot;https://nodejs.org/&quot;&gt;official website&lt;/a&gt;, and npm will be installed automatically as part of the Node.js package.&lt;/p&gt;

&lt;p&gt;2- Install Workbox CLI by running the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;npm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cli&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3- Use the Workbox Wizard to create a service worker by executing the following command inside the directory of your blog:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wizard&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The Workbox CLI wizard will guide you through setting up the service worker by choosing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; directory as the root of your app and selecting the type of files the service worker should pre-cache.&lt;/p&gt;

&lt;p&gt;At the end of this step, you should have a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-config.js&lt;/code&gt; created at the root of the project.&lt;/p&gt;

&lt;p&gt;4- Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-confiog.js&lt;/code&gt; file to generate the service worker by executing the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generateSW&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A new file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder will be generated.&lt;/p&gt;

&lt;p&gt;5- Create a new javascript file used to register your service worker. Place this file under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/js/service-worker.js&lt;/code&gt; in your Jekyll project’s root directory.&lt;/p&gt;

&lt;p&gt;The content of this file is the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;    &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Only&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;supported&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'serviceWorker'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;navigator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Wait&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;until&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loaded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addEventListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'load'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Register&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'s scope.
        navigator.serviceWorker.register('&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', { scope: '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' })
            // Output success/failure of registration.
            .then(() =&amp;gt; console.log('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registered&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'))
            .catch(() =&amp;gt; console.error('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In order to register the service worker and enable its functionality, you’ll need to include this code snippet to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file, which is a layout file included in all your pages, as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;     
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/js/service-worker.js&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;
  
  &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point if your development server is already running, you can navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:4000&lt;/code&gt; and use developer tools to verify that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;By checking the Offline box under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Manifest -&amp;gt; Service worker&lt;/code&gt; you can still access your site.&lt;/li&gt;
  &lt;li&gt;Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Cache Storage&lt;/code&gt; new entry have been created&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The newly created cache entry indicates that the predetermined assets in the workbox-config file are added to the cache during the service worker installation. This is called &lt;strong&gt;precaching&lt;/strong&gt; and is commonly used to ensure that essential resources are available offline and to improve the initial loading performance of your application.&lt;/p&gt;

&lt;p&gt;For more details on some precaching considerations you can check this &lt;a href=&quot;https://developer.chrome.com/docs/workbox/precaching-dos-and-donts/&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another type of caching is &lt;strong&gt;runtime caching&lt;/strong&gt;. It  involves caching resources dynamically during the runtime of your web application, i.e., when users interact with the application. Unlike precaching, runtime caching allows you to define caching strategies for specific URLs or URL patterns based on different criteria such as network requests, HTTP methods, and more.&lt;/p&gt;

&lt;p&gt;There some common runtime caching strategies that are out of the scope of this blog post. For more details you can refer to the &lt;a href=&quot;https://developer.chrome.com/docs/workbox/modules/workbox-strategies/&quot;&gt;workbox documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deploying-service-workers&quot;&gt;Deploying Service Workers&lt;/h2&gt;

&lt;p&gt;Knowing that Jekyll regenerates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder with each change you make to your files, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; will be lost and we have to regenerate it each time. Since our site is hosted on Github Pages, we can leverage &lt;strong&gt;Github Actions&lt;/strong&gt; to automate the execution of these commands as part of the pipeline building and deploying your site.&lt;/p&gt;

&lt;p&gt;An example of a Github Actions workflow can be found &lt;a href=&quot;https://github.com/firasesbai/firasesbai.github.io/blob/master/.github/workflows/github-pages.yml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Congratulations! You’ve successfully transformed your Jekyll blog into a powerful Progressive Web App. By implementing service workers and a manifest file, you’ve unlocked offline access and responsive design, making your content accessible to users even when they’re offline. Remember, this is just the beginning and there’s a wealth of additional features and optimizations you can explore to further enhance your PWA.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&quot;&gt;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/what-are-pwas/&quot;&gt;https://web.dev/what-are-pwas/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&quot;&gt;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/alibaba/&quot;&gt;https://web.dev/alibaba/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&quot;&gt;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sevic.dev/caching-service-worker-workbox/&quot;&gt;https://sevic.dev/caching-service-worker-workbox/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Blogging" /><summary type="html">In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 2</title><link href="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 2" /><published>2023-03-12T00:00:00+00:00</published><updated>2023-03-12T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html">&lt;p&gt;&lt;em&gt;This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will discover how data in motion and data at rest is handled through different techniques and methods and the importance of choosing the right storage technology.&lt;/p&gt;

&lt;p&gt;If you have missed the first part, you can find it &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So buckle up, folks! This is going to be a long ride. But don’t worry, it’s worth it. Grab a snack, get comfy, and let’s dive in!”&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ingestion&quot; id=&quot;markdown-toc-ingestion&quot;&gt;Ingestion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformation&quot; id=&quot;markdown-toc-transformation&quot;&gt;Transformation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storage&quot; id=&quot;markdown-toc-storage&quot;&gt;Storage&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#oltp-vs-olap&quot; id=&quot;markdown-toc-oltp-vs-olap&quot;&gt;OLTP vs OLAP&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#oltp-databases&quot; id=&quot;markdown-toc-oltp-databases&quot;&gt;OLTP Databases&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#olap-databases&quot; id=&quot;markdown-toc-olap-databases&quot;&gt;OLAP Databases&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-warehouse&quot; id=&quot;markdown-toc-data-warehouse&quot;&gt;Data warehouse&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-lake&quot; id=&quot;markdown-toc-data-lake&quot;&gt;Data lake&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-lakehouse&quot; id=&quot;markdown-toc-data-lakehouse&quot;&gt;Data Lakehouse&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;ingestion&quot;&gt;Ingestion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ETL (Extract, Transform, Load)&lt;/strong&gt; and &lt;strong&gt;ELT (Extract, Load, Transform)&lt;/strong&gt; are both data integration techniques used to move data from multiple sources to a single destination, such as a data warehouse or a data lake (more on this later in the Storage section).&lt;/p&gt;

&lt;p&gt;In ETL, data is extracted from source systems, transformed into a format suitable for the destination system, and then loaded into the destination system. 
The transformation is actually done in what is often referred to as a staging area. 
This approach is typically used in traditional data warehousing systems. 
Any data you load into your data warehouse must be transformed into a relational format before the data warehouse can ingest it. 
As a part of this data transformation process, data mapping may also be necessary to combine multiple data sources based on correlating information. 
In addition, ETL can help with data privacy and compliance by cleaning sensitive and secure data even before loading into the data warehouse.&lt;/p&gt;

&lt;p&gt;In ELT on the other hand, data is first extracted from source systems and loaded into the destination system in its raw form, where it is then transformed into the desired format. 
This approach is typically used in big data environments, where the target system, such as a data lake, is designed to handle large amounts of unstructured and semi-structured data and can perform the transformations in parallel.&lt;/p&gt;

&lt;p&gt;Rather than obsessing over this ETL vs ELT cage fight, just try to take away the following:&lt;/p&gt;

&lt;p&gt;Sometimes you may want to optimize/reshape your data sooner (because you know that’s how everyone wants to use it). 
Other times, you want to leave the schema flexible (and just let the user’s queries/views do the work) to avoid having to maintain lots of tables/views/jobs.&lt;/p&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Batch&lt;/strong&gt; and &lt;strong&gt;Stream&lt;/strong&gt; processing are two popular methods for data processing and transformation.&lt;/p&gt;

&lt;p&gt;In batch processing, we wait for a certain amount of raw data to “pile up” before running an ETL job. 
Typically this means data is between an hour to a few days old before it is made available for analysis. 
Batch ETL jobs will typically be run on a set schedule (e.g. every 24 hours), or in some cases once the amount of data reaches a certain threshold.
You should lean towards batch processing when:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data freshness is not a mission-critical issue&lt;/li&gt;
  &lt;li&gt;You are working with large datasets and are running a complex algorithm that requires access to the entire batch – e.g., sorting the entire dataset&lt;/li&gt;
  &lt;li&gt;You get access to the data in batches rather than in streams&lt;/li&gt;
  &lt;li&gt;When you are joining tables in relational databases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In stream processing, we process data as soon as it arrives in the storage layer – which would often also be very close to the time it was generated (although this would not always be the case). 
This would typically be in sub-second timeframes, so that for the end user the processing happens in real-time. 
These operations would typically not be stateful, or would only be able to store a ‘small’ state, so would usually involve a relatively simple transformation or calculation.&lt;/p&gt;

&lt;p&gt;Another alternative is &lt;strong&gt;micro-batch&lt;/strong&gt; processing. 
In micro-batch processing, we run batch processes on much smaller accumulations of data – typically less than a minute’s worth of data. 
This means data is available in near real-time. 
In practice, there is little difference between micro-batching and stream processing, and the terms would often be used interchangeably in data architecture descriptions and software platform descriptions.
Microbatch processing is useful when we need very fresh data, but not necessarily real-time – meaning we can’t wait an hour or a day for a batch processing to run, but we also don’t need to know what happened in the last few seconds. 
Example scenarios could include web analytics (clickstream) or user behavior.&lt;/p&gt;

&lt;h2 id=&quot;storage&quot;&gt;Storage&lt;/h2&gt;

&lt;p&gt;Two different types of data are used today in an organization; &lt;strong&gt;Operational&lt;/strong&gt; and &lt;strong&gt;Analytical&lt;/strong&gt;. 
Both operational and analytical data are important for organizations, but they serve different purposes and are used in different ways.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;​​Operational data refers to the data that is used in day-to-day business operations to support critical functions such as sales, marketing, and customer service. Operational data is often used to support short-term decision making and is focused on current and immediate needs.&lt;/li&gt;
  &lt;li&gt;Analytical data, on the other hand, is used for long-term strategic decision making. Analytical data is used to support trend analysis, business intelligence, and other data-driven decision making processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;oltp-vs-olap&quot;&gt;OLTP vs OLAP&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;OLTP (Online Transaction Processing)&lt;/strong&gt; and &lt;strong&gt;OLAP (Online Analytical Processing)&lt;/strong&gt; are two different types of data processing systems that are often used to manage operational and analytical data, respectively.&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLTP systems are used to process operational data and are designed to support rapid data insertion, updates, and retrievals.&lt;/li&gt;
  &lt;li&gt;Make sure that the systems can keep up with high volumes of transactions but often very small and fast in nature (e.g. online banking, FinTech application).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLAP systems are used to process analytical data and are designed to support large-scale data analysis.&lt;/li&gt;
  &lt;li&gt;Make sure that you can crunch through millions or billions of rows of data for your complex and large theories, where they need to run some fancy aggregation or calculations for data Analytics purposes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example: Online Store&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Store user data, passwords, previous transactions, find user, change its name,… basically perform INSERT, UPDATE, DELETE operations&lt;/li&gt;
  &lt;li&gt;Store actual products, their associated prices&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Find out the “total money spent by all users”&lt;/li&gt;
  &lt;li&gt;Find out “what is the most sold product”&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oltp-databases&quot;&gt;OLTP Databases&lt;/h4&gt;

&lt;p&gt;OLTP databases can use either &lt;strong&gt;SQL (Structured Query Language)&lt;/strong&gt; or &lt;strong&gt;NoSQL (Not only SQL)&lt;/strong&gt; technologies.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SQL Database&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;NoSQL Database&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Data storage model&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tables with fixed rows and columns&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: JSON Documents &lt;br /&gt; Key-value: key-value pairs &lt;br /&gt; Wide-Columns: Tables with rows and dynamic columns &lt;br /&gt; Graph&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Development history&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the 1970s with a focus on reducing data duplication&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the late 2000s with a focus on scaling and allowing for rapid application change driven by agile and DevOps practices&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Primary purpose&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;General purpose&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: general purpose &lt;br /&gt; Key-value: large amounts of data with simple lookup queries &lt;br /&gt; Wide-column: large amounts of data with predictable query patterns &lt;br /&gt; Graph: analyzing and traversing relationships between connected data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Rigid&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Flexible (Implicit schema!, querying time)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Vertical (scale-up with a larger server)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Horizontal (scale-out across commodity servers)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Joins&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically required&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically not required&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;SQL databases are based on a relational model and use a structured data model, where data is organized into tables, rows, and columns. 
This makes it easy to enforce data constraints, such as unique keys, and ensures data consistency. 
They are well tested and proven, and they have a long history of use in OLTP systems. 
This makes them a reliable choice for OLTP systems. 
However, SQL databases can be complex to set up and maintain and can be challenging to scale, particularly for large OLTP systems that require horizontal scalability and the rigid schema of SQL databases can make it difficult to accommodate changing requirements or new data types.&lt;/p&gt;

&lt;p&gt;NoSQL databases on the other hand are designed for horizontal scalability, which makes them well suited for OLTP systems that need to scale to handle large amounts of data and users. 
In addition, they use a variety of data models which makes them more flexible than SQL databases. 
This allows NoSQL databases to better handle unstructured data and changing data requirements. 
However , NoSQL databases may not provide the same level of transactional consistency as SQL databases, which can result in data inconsistencies. 
Plus they can be complex to set up and maintain as well, even though this can be addressed since they are designed to work well in cloud environments, which makes them a good choice for OLTP systems that need to scale quickly and elastically.&lt;/p&gt;

&lt;p&gt;We kept mentioning &lt;strong&gt;transactional consistency&lt;/strong&gt; and you might be wondering what is it?&lt;/p&gt;

&lt;p&gt;A transaction is a sequence of operations performed (using one or more SQL statements) on a database as a single logical unit of work. 
Transactions have the following four standard properties, usually referred to by the acronym &lt;strong&gt;ACID&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt; − ensures that all operations within the work unit are completed successfully. Otherwise, the transaction is aborted at the point of failure and all the previous operations are rolled back to their former state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; − ensures that the database properly changes states upon a successfully committed transaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; − enables transactions to operate independently of and transparent to each other.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt; − ensures that the result or effect of a committed transaction persists in case of a system failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;olap-databases&quot;&gt;OLAP Databases&lt;/h4&gt;

&lt;p&gt;In this section we will discuss two important components that are used for advanced analysis and decision-making; &lt;strong&gt;data warehouses&lt;/strong&gt; and &lt;strong&gt;data lakes&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;data-warehouse&quot;&gt;Data warehouse&lt;/h4&gt;

&lt;p&gt;A data warehouse is a centralized repository for storing and managing large amounts of data from various sources. 
Data warehouses are designed to support business intelligence (BI) and analytics applications, by providing a single source of data that can be queried, analyzed, and used to make informed decisions. 
The following are the key characteristics of a data warehouse:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Integration&lt;/em&gt; - Data warehouses integrate data from multiple sources, such as transactional systems, log files, and external data sources, into a single, unified view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data warehouses are designed to handle large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Modeling&lt;/em&gt; - Data warehouses use a specific data model, such as the star or snowflake schema, to organize data and make it easier to query and analyze.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Historical data&lt;/em&gt; - Data warehouses store historical data, allowing users to analyze trends and changes over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Performance optimization&lt;/em&gt; - Data warehouses are optimized for fast querying and analysis, by using techniques such as indexing, materialized views, and aggregations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Cleansing&lt;/em&gt; - Data warehouses often include data cleansing and normalization to ensure that data is consistent and accurate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Security and access control&lt;/em&gt; - Data warehouses have robust security and access control features to ensure that sensitive data is protected and only authorized users have access to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The explosion of big data, including the growth of structured, semi-structured, and unstructured data, made it increasingly difficult for traditional data warehouses to store and process all the data being generated. 
In addition, Traditional data warehouses required expensive hardware and software to store and process data. 
The cost of these solutions made it difficult for organizations to store all their data, which led to the adoption of data lakes as a more cost-effective alternative.&lt;/p&gt;

&lt;h4 id=&quot;data-lake&quot;&gt;Data lake&lt;/h4&gt;

&lt;p&gt;A data lake is a centralized repository that stores large amounts of raw, structured and unstructured data. The data is stored in its native format and can be accessed, processed, and analyzed later as needed. 
The following are the key characteristics of a data lake:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Flexibility&lt;/em&gt; - Data lakes allow organizations to store a wide variety of data types and formats, including structured, semi-structured, and unstructured data, without having to worry about pre-defining schemas.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data lakes are designed to handle very large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Cost-effectiveness&lt;/em&gt; - Data lakes are often implemented on low-cost, commodity hardware&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Raw data preservation&lt;/em&gt; - Data lakes preserve raw data, allowing organizations to perform in-depth analysis and retain the original data for future use.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Decentralized processing&lt;/em&gt; - Data lakes can be used to distribute processing tasks across multiple nodes in a network, allowing for increased processing speed and scalability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Self-service analytics&lt;/em&gt; - Data lakes allow business users and data scientists to perform their own data analysis, without having to rely on IT or data engineering teams.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integration with big data tools&lt;/em&gt; - Data lakes can be integrated with big data tools, such as Apache Spark and Apache Flink, allowing organizations to perform complex data processing and analysis tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following table summarizes the differences between data warehouse and data lake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/11_data_warehouse_vs_data_lake.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Warehouse vs Data Lake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As organizations move data infrastructure to the cloud, the choice of data warehouse vs. data lake, or the need for complex integrations between the two, is less of an issue. 
It is becoming natural for organizations to have both, and move data flexibly from lakes to warehouses to enable business analysis.&lt;/p&gt;

&lt;p&gt;Here is a list of some known cloud-based solutions from different cloud providers:&lt;/p&gt;

&lt;p&gt;Cloud data warehousing solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon Redshift&lt;/em&gt; - a fully-managed, analytical data warehouse that can handle petabyte-scale data, and enable querying it in seconds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Google BigQuery&lt;/em&gt; - an enterprise-grade cloud-native data warehouse, which runs fast interactive and ad-hoc queries on datasets of petabyte-scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cloud data lake solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon S3&lt;/em&gt; - an object storage platform built to store and retrieve any amount of data from any data source, and designed for 99.999999999% durability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Azure Blob Storage&lt;/em&gt; - stores billions of objects in hot, cool, or archive tiers, depending on how often data is accessed. Data ranges from structured (converted to object form) to any unstructured format - images, videos, audio, documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data lake architecture was introduced as a solution to some challenges of data warehousing with the rise of big data offering the ability to store and process big data in a cost-effective and scalable manner. 
However, it had its own set of challenges, such as the lack of reliability and transactional consistency and the complex data quality problems making it difficult for GDPR compliance and to use for critical business decisions.&lt;/p&gt;

&lt;p&gt;Can we get the best of both worlds without the complexity of managing both a data lake and a data warehouse or perhaps multiple ones?&lt;/p&gt;

&lt;h4 id=&quot;data-lakehouse&quot;&gt;Data Lakehouse&lt;/h4&gt;

&lt;p&gt;A data Lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. 
Data Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/12_data_warehouse_vs_data_lake_vs_data_lakehouse.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Data Warehouse vs Data Lake vs Data Lakehouse&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some data management solutions such as &lt;a href=&quot;https://delta.io/&quot;&gt;Delta Lake&lt;/a&gt;, which is an implementation of Data Lakehouse from Databricks, offer the ability to store and process big data in a reliable and consistent manner, while also providing the scalability and cost savings of a data lake.&lt;/p&gt;

&lt;p&gt;The data lakehouse is a relatively new concept and is still evolving, but it has the potential to become an important technology for big data processing and analysis in the future.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we went through some popular patterns such as ETL vs ELT and stream vs batch processing that are used in data ingestion and transformation and where they can be applied. 
Then we discussed the different types of data storage and some concrete implementations of them in the cloud and how they fit in an organization based on its requirements, data maturity and purpose.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://martinfowler.com/articles/data-mesh-principles.html&quot;&gt;https://martinfowler.com/articles/data-mesh-principles.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jie-tao.com/delta-lake-step-by-step1/&quot;&gt;https://www.jie-tao.com/delta-lake-step-by-step1/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&quot;&gt;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://delta.io/&quot;&gt;https://delta.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 1</title><link href="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 1" /><published>2023-03-11T00:00:00+00:00</published><updated>2023-03-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/11/data-engineering-201</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html">&lt;p&gt;&lt;em&gt;In this two parts article we will deep dive into each of the general steps of the data flow.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a previous &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;blog post&lt;/a&gt;, we saw the importance of data engineering in democratizing data inside an organization and enabling data driven decision making. 
In addition, we outlined the different stages data goes through in order to be able to extract insights out of it.&lt;/p&gt;

&lt;p&gt;Here is a reminder of our data flow blueprint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will start by taking a bird’s eye view, focusing on data sources, data governance and analysis before zooming in on the other stages of the data flow in part 2.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data-sources&quot; id=&quot;markdown-toc-data-sources&quot;&gt;Data Sources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-governance&quot; id=&quot;markdown-toc-data-governance&quot;&gt;Data Governance&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-governance-vs-data-management&quot; id=&quot;markdown-toc-data-governance-vs-data-management&quot;&gt;Data Governance vs Data Management&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#benefits-of-data-governance&quot; id=&quot;markdown-toc-benefits-of-data-governance&quot;&gt;Benefits of Data Governance&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#analysis&quot; id=&quot;markdown-toc-analysis&quot;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;data-sources&quot;&gt;Data Sources&lt;/h2&gt;

&lt;p&gt;Data sources refer to the various systems and platforms that generate data and constitute the origin of all data in an organization and play a crucial role in the data engineering and big data landscape. 
These sources can be internal systems such as transactional databases, or external sources such as social media platforms, sensors, or log files.&lt;/p&gt;

&lt;p&gt;The variety and volume of data sources have dramatically increased with the growth of big data, making it important for organizations to have a comprehensive strategy for managing, processing, and analyzing data from multiple sources.&lt;/p&gt;

&lt;p&gt;For instance, data must be transformed and stored in a format that enables efficient processing and analysis. 
There are a variety of file formats, such as Parquet and Avro, that are common to big data use cases and which provide a means of storing and exchanging large volumes of structured and semi-structured data in a standardized way, making it easier for organizations to work with data from multiple sources.&lt;/p&gt;

&lt;p&gt;Following is a table summarizing some properties and how do they compare between some big data file formats:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Properties&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CSV&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;JSON&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parquet&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Avro&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Human readability&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Format&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Key-Value&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Columunar&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compressible&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Splittable&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Complex data structure&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Schema evolution&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Read performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Write performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;CSV should typically be the fastest to write, JSON the easiest to understand for humans, and Parquet the fastest to read a subset of columns, while Avro is the fastest to read all columns at once.&lt;/p&gt;

&lt;h2 id=&quot;data-governance&quot;&gt;Data Governance&lt;/h2&gt;

&lt;p&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It includes the actions people must take, the processes they must follow, and the technology that supports them throughout the data lifecycle to ensure data is secure, private, accurate, available, and usable.&lt;/p&gt;

&lt;p&gt;Some of the key components of the data governance include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data compliance&lt;/strong&gt;: Establishing clear policies, procedures, and standards to govern data collection, storage, and usage. In addition, regularly monitoring and auditing data to ensure compliance with relevant regulations and laws&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data security&lt;/strong&gt;: Implementing security measures to protect data from unauthorized access and breaches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data privacy&lt;/strong&gt;: Ensuring that personal and confidential data is managed in accordance with relevant regulations and laws.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;: Refers to the development and implementation of activities that apply quality management techniques to data to make sure it is suitable to be used in a specific context, thus considered to be high quality data. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data cataloging&lt;/strong&gt;: Maintaining a comprehensive and up-to-date inventory of data assets, including metadata, definitions, and relationships.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data lineage&lt;/strong&gt;: Tracing the origin and evolution of data to understand its history and potential impact on decision-making.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-governance-vs-data-management&quot;&gt;Data Governance vs Data Management&lt;/h3&gt;

&lt;p&gt;The scope of Data management is broader than data governance. 
Data management includes all aspects of the full data lifecycle from collection and storage to usage and oversight. 
This is inclusive of data governance, which can be considered as a core component of it, but it also includes other areas such as data architecture and data modeling.&lt;/p&gt;

&lt;h3 id=&quot;benefits-of-data-governance&quot;&gt;Benefits of Data Governance&lt;/h3&gt;

&lt;p&gt;Implementing a data governance framework can increase the value of data within your organization and therefore make better and more timely decisions. 
In fact, effective data governance aims to maintain high quality data that’s both secure and compliant and easily accessible for deeper business insights.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Visualization&lt;/em&gt; and &lt;em&gt;reporting&lt;/em&gt; play a critical role in helping organizations to make informed decisions. 
By presenting data in an easily understandable format, stakeholders can gain insights into trends, patterns, and relationships that would not be evident from raw data alone. 
Effective visualization and reporting also make it easier for organizations to communicate the results of data analysis to others, including stakeholders, partners, and customers.&lt;/p&gt;

&lt;p&gt;Visualization is the process of creating visual representations of data and information, such as charts, graphs, and maps. 
Data visualization can be done using a wide range of tools including plotting libraries such as matplotlib or seaborn which are used especially by data scientists to more powerful analytics dashboards such as Tableau and Microsoft Power BI.&lt;/p&gt;

&lt;p&gt;Reporting, on the other hand, is the presentation of data and information in a structured format that is designed to be easily consumable by stakeholders. 
Reports are often used to provide an overview of key metrics, trends, and insights, and can be used to support decision making and strategic planning.&lt;/p&gt;

&lt;p&gt;Another aspect of the data analysis we mentioned in &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;this article&lt;/a&gt; is the use of &lt;em&gt;machine learning algorithms&lt;/em&gt;. 
However, it is not feasible to fully cover the topic of machine learning within the scope of this article. 
Though a good starting point for anyone new to this field is the &lt;a href=&quot;https://www.deeplearning.ai/courses/machine-learning-specialization/&quot;&gt;machine learning specialization course&lt;/a&gt; from Andrew Ng.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we saw the different types of big data file formats and how they compare to each other which can help shape our choice to achieve more efficient data processing. 
In addition, we discussed some of the key components of data governance and its benefit to achieve better data quality and therefore better data analysis through the usage of visualization and reporting.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://devopedia.org/data-serialization&quot;&gt;https://devopedia.org/data-serialization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/learn/what-is-data-governance&quot;&gt;https://cloud.google.com/learn/what-is-data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/topics/data-governance&quot;&gt;https://www.ibm.com/topics/data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.heavy.ai/technical-glossary/data-quality&quot;&gt;https://www.heavy.ai/technical-glossary/data-quality&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">In this two parts article we will deep dive into each of the general steps of the data flow.</summary></entry><entry><title type="html">Data engineering 101: Understanding the Fundementals</title><link href="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html" rel="alternate" type="text/html" title="Data engineering 101: Understanding the Fundementals" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/01/data-engineering-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html">&lt;p&gt;&lt;em&gt;In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-data-democratization&quot; id=&quot;markdown-toc-what-is-data-democratization&quot;&gt;What is Data Democratization?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-data-engineering&quot; id=&quot;markdown-toc-what-is-data-engineering&quot;&gt;What is Data Engineering?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#role-of-data-engineer&quot; id=&quot;markdown-toc-role-of-data-engineer&quot;&gt;Role of Data Engineer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-engineering-vs-big-data-engineering&quot; id=&quot;markdown-toc-data-engineering-vs-big-data-engineering&quot;&gt;Data Engineering vs Big Data Engineering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-flow&quot; id=&quot;markdown-toc-data-flow&quot;&gt;Data Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;what-is-data-democratization&quot;&gt;What is Data Democratization?&lt;/h2&gt;

&lt;p&gt;Data democratization is the process of making data accessible to all members of an organization, regardless of their role or level of technical expertise. 
It enables organizations to make better and more informed decisions by providing access to data to a wide range of users. 
Data democratization also helps to promote data-driven culture within an organization, as it enables them to explore and leverage their data to create new products and services.&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-engineering&quot;&gt;What is Data Engineering?&lt;/h2&gt;

&lt;p&gt;While data democratization is the foundation for organizations to become data-driven and make better decisions, data engineering plays a vital role in making it happen.&lt;/p&gt;

&lt;p&gt;Data engineering is the process of acquiring, storing, and preparing raw data for analysis to extract valuable insights from their data and make informed business decisions.&lt;/p&gt;

&lt;h2 id=&quot;role-of-data-engineer&quot;&gt;Role of Data Engineer&lt;/h2&gt;

&lt;p&gt;A data engineer is responsible for the “plumbing” that helps derive value from data. 
The specific responsibilities of a data engineer may vary depending on the organization and industry, but generally include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Designing and building data pipelines&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Acquire data from various sources into the appropriate storage and processing systems&lt;/li&gt;
      &lt;li&gt;Coordination of the many jobs through orchestration tools&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data storage and management&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data storage systems&lt;/li&gt;
      &lt;li&gt;Design and implement data models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data processing&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data processing methods such as batch processing and streaming processing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data governance&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Ensure data is accurate, reliable, and easily accessible to users&lt;/li&gt;
      &lt;li&gt;Establish, implement, and maintain policies and procedures for data quality, compliance, and security&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Manage data infrastructure&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Collaboration and communication&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work closely with data scientists, analysts, and other stakeholders to understand their data needs and develop solutions that meet their requirements&lt;/li&gt;
      &lt;li&gt;Communicate with other teams such as DevOps, IT and security to ensure that the data infrastructure is aligned with the organization’s overall strategy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-engineering-vs-big-data-engineering&quot;&gt;Data Engineering vs Big Data Engineering&lt;/h2&gt;

&lt;p&gt;According to a study by IDC, the global data sphere is projected to grow from 33 zettabytes (ZB) in 2018 to 175 ZB by 2025, with a compound annual growth rate of 23%. 
It’s also important to note that the vast majority of this data (about 90%) is unstructured data, such as social media posts, images, and videos.
Another study by IBM estimates that 2.5 quintillion bytes of data are created every day, and this amount is increasing exponentially.&lt;/p&gt;

&lt;p&gt;These statistics highlight the sheer volume of data that is being generated on a daily basis and the significant growth in the amount of data generated over time which leads us to the definition of &lt;strong&gt;Big Data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Big data refers to extremely large and complex data sets that are difficult to process and analyze using traditional data processing and management techniques. These data sets are characterized by the “3Vs” - &lt;em&gt;volume, velocity, and variety&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Volume&lt;/em&gt; refers to the sheer amount of data that is generated and collected, this could be in terabytes, petabytes, or even exabytes.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Velocity&lt;/em&gt; refers to the speed at which data is generated and collected. With the growing number of devices and sensors that are connected to the internet, data is being generated at an unprecedented rate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Variety&lt;/em&gt; refers to the different types of data that are generated and collected, such as structured data, unstructured data, and semi-structured data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So is data engineering always “Big”?&lt;/p&gt;

&lt;p&gt;Data engineering is the broader concept of designing, building, and maintaining systems and infrastructure to support data-driven applications and services, while big data engineering is a specific subfield that is focused on the management and analysis of extremely large and complex data sets that requires the use of advanced processing tools and analytics techniques.&lt;/p&gt;

&lt;p&gt;Overall, data engineering is needed to democratize data inside an organization for informed business decisions, not just when we are dealing with big data.&lt;/p&gt;

&lt;p&gt;After understanding the “What” and the “Why” behind data engineering, it is now time to delve into the “How” of data engineering.&lt;/p&gt;

&lt;h2 id=&quot;data-flow&quot;&gt;Data Flow&lt;/h2&gt;

&lt;p&gt;The data flow outlines the different stages involved in making data easily accessible in a reliable and accurate way for analysis and decision making.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The general steps include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Data sources are an important and challenging step due to the wide range of potential integration points.&lt;/li&gt;
      &lt;li&gt;Depending on the organization’s needs, some common data sources include:
        &lt;ul&gt;
          &lt;li&gt;Relational and non relational databases&lt;/li&gt;
          &lt;li&gt;Flat files: Data stored in CSV, Excel, or other flat file formats&lt;/li&gt;
          &lt;li&gt;Streaming data: Data generated in real-time, such as data from social media feeds or IoT devices&lt;/li&gt;
          &lt;li&gt;External data: Data from third-party sources, such as government data or data from other companies&lt;/li&gt;
          &lt;li&gt;Legacy systems: Data stored in older systems that need to be migrated to the new data infrastructure&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Governance&lt;/strong&gt;
&lt;br /&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It is not particularly a separate step in the data flow, but rather a broad concept that involves processes, people and technology that support these processes in order to maintain high quality data and make better data driven decisions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data in Motion&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Ingestion&lt;/em&gt; 
&lt;br /&gt;Ingestion is the process of acquiring and importing data from various sources into the organization’s data infrastructure.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; 
&lt;br /&gt;This is the process of cleaning, normalizing, and transforming the data acquired during the ingestion step so that it is in a format that can be used for analysis and reporting.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data at Rest: Storage&lt;/strong&gt; 
&lt;br /&gt;This is the process of storing and managing the data in a way that it can be accessed, queried, and updated easily. Therefore, choosing the right type of storage and technology is a key element not only to the following steps, but also to previous ones due to the fact that we might require multiple storages in different steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Analysis&lt;/strong&gt; 
&lt;br /&gt;This is the step where we extract insights and knowledge from the processed data. This involves:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Visualization and Reporting&lt;/em&gt;
&lt;br /&gt;Creating visual representations of the data to help communicate insights and make it easy to understand&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Machine learning&lt;/em&gt;
&lt;br /&gt;Using algorithms to learn from the data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we have seen how valuable it is to democratize data inside an organization to make the shift towards data-driven decision making and the main role played by data engineers in the different steps of the raw data goes through to reach this goal. 
In the following blog post we will dive deeper into each step to uncover best practices, tools and paradigms used and how they have evolved in an ever changing field. Stay tuned!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&quot;&gt;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&quot;&gt;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/andkret/Cookbook&quot;&gt;https://github.com/andkret/Cookbook&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.</summary></entry></feed>