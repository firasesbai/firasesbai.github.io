<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://www.firasesbai.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.firasesbai.com/" rel="alternate" type="text/html" /><updated>2024-02-15T19:54:25+00:00</updated><id>https://www.firasesbai.com/feed.xml</id><title type="html">Firas Esbai</title><subtitle>Firas Esbai's Lifelong Learning Journey.
</subtitle><author><name>Firas Esbai</name></author><entry><title type="html">Preparation Guide for Google Cloud Professional Data Engineer Certification</title><link href="https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification.html" rel="alternate" type="text/html" title="Preparation Guide for Google Cloud Professional Data Engineer Certification" /><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/11/19/gcp-data-engineer-certification.html">&lt;p&gt;&lt;em&gt;This article contains a collection of notes, mind maps and resources to support you while preparing for the google cloud professional data engineer certification.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Disclaimer&lt;/em&gt;&lt;/strong&gt;: The new Professional Data Engineer exam will be live starting November 13. The new version reflects updates to Google Cloud’s data storing, data sharing, and data governance and has less emphasis on operationalizing machine learning models. 
That being said, I believe most of the content is still relevant and can serve as a guide to assist you as you begin your preparation.&lt;/p&gt;

&lt;p&gt;So brace yourselves, this is gonna be a rather long post filled with too many images extracted from different parts of the mind maps I used for my preparation in order to make them easy to read and follow.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#google-cloud&quot; id=&quot;markdown-toc-google-cloud&quot;&gt;Google Cloud&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#infrastructure&quot; id=&quot;markdown-toc-infrastructure&quot;&gt;Infrastructure&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vpc-networks&quot; id=&quot;markdown-toc-vpc-networks&quot;&gt;VPC Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-transfer-services&quot; id=&quot;markdown-toc-data-transfer-services&quot;&gt;Data Transfer Services&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#resource-manager&quot; id=&quot;markdown-toc-resource-manager&quot;&gt;Resource Manager&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#security&quot; id=&quot;markdown-toc-security&quot;&gt;Security&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#compute&quot; id=&quot;markdown-toc-compute&quot;&gt;Compute&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#storage&quot; id=&quot;markdown-toc-storage&quot;&gt;Storage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ingestion-and-processing&quot; id=&quot;markdown-toc-ingestion-and-processing&quot;&gt;Ingestion and Processing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-pipelines-management&quot; id=&quot;markdown-toc-data-pipelines-management&quot;&gt;Data Pipelines Management&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#data-governance&quot; id=&quot;markdown-toc-data-governance&quot;&gt;Data Governance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#analytics&quot; id=&quot;markdown-toc-analytics&quot;&gt;Analytics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#machine-learning&quot; id=&quot;markdown-toc-machine-learning&quot;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ingestion-and-pocessing&quot; id=&quot;markdown-toc-ingestion-and-pocessing&quot;&gt;Ingestion and Pocessing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pubsub&quot; id=&quot;markdown-toc-pubsub&quot;&gt;Pub/Sub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataproc&quot; id=&quot;markdown-toc-dataproc&quot;&gt;Dataproc&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dataflow&quot; id=&quot;markdown-toc-dataflow&quot;&gt;Dataflow&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storage-1&quot; id=&quot;markdown-toc-storage-1&quot;&gt;Storage&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-storage&quot; id=&quot;markdown-toc-cloud-storage&quot;&gt;Cloud Storage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-sql&quot; id=&quot;markdown-toc-cloud-sql&quot;&gt;Cloud SQL&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#query-insights&quot; id=&quot;markdown-toc-query-insights&quot;&gt;Query Insights&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cloud-spanner&quot; id=&quot;markdown-toc-cloud-spanner&quot;&gt;Cloud Spanner&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#firestore&quot; id=&quot;markdown-toc-firestore&quot;&gt;Firestore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#datastore&quot; id=&quot;markdown-toc-datastore&quot;&gt;Datastore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#memorystore&quot; id=&quot;markdown-toc-memorystore&quot;&gt;Memorystore&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bigtable&quot; id=&quot;markdown-toc-bigtable&quot;&gt;Bigtable&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bigquery&quot; id=&quot;markdown-toc-bigquery&quot;&gt;BigQuery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;google-cloud&quot;&gt;Google Cloud&lt;/h2&gt;

&lt;p&gt;Before we dive into the characteristics of Google Cloud services that will enable professional data engineers to design, build and operationalize data processing systems, let’s start with a 10,000-foot view on different topics that may be included in the exam.&lt;/p&gt;

&lt;h3 id=&quot;infrastructure&quot;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;Google Cloud services are available in different locations divided into &lt;strong&gt;Regions&lt;/strong&gt;.
Regions contain multiple &lt;strong&gt;Zones&lt;/strong&gt; where the resources are deployed and are isolated from one another so that failures in one zone do not affect other zones in a region. 
Most regions have at least three zones and can have more. All regions have at least two zones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_regions.png&quot; alt=&quot;Google Cloud Regions&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 1: Google Cloud Regions, Source: &lt;a href=&quot;https://cloud.google.com/about/locations#lightbox-regions-map&quot;&gt;https://cloud.google.com/about/locations#lightbox-regions-map&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Google data centers are connected with Google’s own high-speed network. Google is the only cloud provider that owns all the fiber connecting its data center together. A huge amount of the world’s internet traffic goes through Google’s network.&lt;/p&gt;

&lt;p&gt;In addition to the data centers, there are points of presence all over the world. They allow access to Google’s network where all messages are encrypted, secure and very fast.&lt;/p&gt;

&lt;p&gt;In addition to the POPs, Google runs a global caching system or CDN that consists of hundreds of more nodes. You can easily take advantage of this CDN to cache your content, thus increasing your application performance and decreasing your networking cost.&lt;/p&gt;

&lt;h3 id=&quot;vpc-networks&quot;&gt;VPC Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_vpc_networks.png&quot; alt=&quot;Google Cloud Platform VPC Networks&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 2: Google Cloud Platform VPC Networks&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-transfer-services&quot;&gt;Data Transfer Services&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_transfer_services.png&quot; alt=&quot;Google Cloud Platform Data Transfer Services&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 3: Google Cloud Platform Data Transfer Services&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;resource-manager&quot;&gt;Resource Manager&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_resource_manager.png&quot; alt=&quot;Google Cloud Platform Resource Manager&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 4: Google Cloud Platform Resource Manager&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;security&quot;&gt;Security&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_security.png&quot; alt=&quot;Security in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 5: Security in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;compute&quot;&gt;Compute&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_compute.png&quot; alt=&quot;Google Cloud Platform Compute&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 6: Google Cloud Platform Compute&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;storage&quot;&gt;Storage&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_storage.png&quot; alt=&quot;Google Cloud Platform Storage&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 7: Google Cloud Platform Storage&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;ingestion-and-processing&quot;&gt;Ingestion and Processing&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_ingestion_and_processing.png&quot; alt=&quot;Ingestion and Processing in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 8: Ingestion and Processing in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-pipelines-management&quot;&gt;Data Pipelines Management&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_pipelines_management.png&quot; alt=&quot;Data Pipelines Management in Google Cloud Platform Data&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 9: Data Pipelines Management in Google Cloud Platform Data&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-governance&quot;&gt;Data Governance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_data_governance.png&quot; alt=&quot;Data Governance in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 10: Data Governance in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;analytics&quot;&gt;Analytics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_analytics.png&quot; alt=&quot;Analytics in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 11: Analytics in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_machine_learning.png&quot; alt=&quot;Machine Learning in Google Cloud Platform&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 12: Machine Learning in Google Cloud Platform&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;ingestion-and-pocessing&quot;&gt;Ingestion and Pocessing&lt;/h2&gt;

&lt;p&gt;As a professional data engineer, designing data processing systems requires building and operationalizing data pipelines by choosing the appropriate services to integrate new data sources and processing the data in batch or streaming fashion. In this section, we deep dive into services that will allow you to ingest data in real time and build data processing systems whether you are migrating on premises workloads or starting from scratch.&lt;/p&gt;

&lt;h3 id=&quot;pubsub&quot;&gt;Pub/Sub&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_pub_sub.png&quot; alt=&quot;Pub/Sub&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 13: Pub/Sub&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataproc&quot;&gt;Dataproc&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataproc.png&quot; alt=&quot;Dataproc 1/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 14: Dataproc 1/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataproc_2.png&quot; alt=&quot;Dataproc 2/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 15: Dataproc 2/2&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataflow&quot;&gt;Dataflow&lt;/h3&gt;

&lt;p&gt;It allows you to execute your Apache Beans pipelines on Google Cloud.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A managed service that provides the resources necessary to create pipelines
    &lt;ul&gt;
      &lt;li&gt;Defines &lt;em&gt;HOW&lt;/em&gt; to run the pipeline:
        &lt;ul&gt;
          &lt;li&gt;Optimizes the graph by fusing transforms for example for best execution path&lt;/li&gt;
          &lt;li&gt;Breaks jobs into units of work&lt;/li&gt;
          &lt;li&gt;Schedules them to various workers&lt;/li&gt;
          &lt;li&gt;Optimization is always ongoing
            &lt;ul&gt;
              &lt;li&gt;Units of work are continually rebalanced mid job which provides fault tolerance&lt;/li&gt;
              &lt;li&gt;autoscaling mid job&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Resources –both compute and storage– are deployed on demand and on a per job basis&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Apache Beam SDK, which provides the programming environment to make the creation of streaming and batch pipelines easier
    &lt;ul&gt;
      &lt;li&gt;Defines &lt;em&gt;WHAT&lt;/em&gt; has to be done&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow.png&quot; alt=&quot;Dataflow 1/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 16: Dataflow 1/3&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow_2.png&quot; alt=&quot;Dataflow 2/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 17: Dataflow 2/3&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_dataflow_3.png&quot; alt=&quot;Dataflow 3/3&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 18: Dataflow 3/3&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;storage-1&quot;&gt;Storage&lt;/h2&gt;

&lt;p&gt;One of a data engineer’s most important skills is choosing the right storage technology, which involves knowing how to use managed services and having a solid grasp of storage performance and pricing. To further optimize your data processing and cut expenses, consider data modeling, schema design, and data life cycle management. In this section we will delve into the many storage options provided by Google Cloud.&lt;/p&gt;

&lt;h3 id=&quot;cloud-storage&quot;&gt;Cloud Storage&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_storage.png&quot; alt=&quot;Cloud Storage 1/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 19: Cloud Storage 1/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_storage_2.png&quot; alt=&quot;Cloud Storage 2/2&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 20: Cloud Storage 2/2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Google Cloud provides 3 ways to manage the KEK encryption key:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Google Managed Encryption Keys - GMEK: automatic encryption using Cloud KMS (Key Management Service)&lt;/li&gt;
  &lt;li&gt;Customer Managed Encryption Keys - CMEK: you control the creation and existance of the KEK key in KMS&lt;/li&gt;
  &lt;li&gt;Customer Supplied Encryption Keys - CSEK: you provide the KEK key&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cloud-sql&quot;&gt;Cloud SQL&lt;/h3&gt;

&lt;p&gt;Cloud SQL is a fully managed relational database service for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MySQL&lt;/li&gt;
  &lt;li&gt;PostgreSQL&lt;/li&gt;
  &lt;li&gt;Microsoft SQL&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_sql.png&quot; alt=&quot;Cloud SQL&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 21: Cloud SQL&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;query-insights&quot;&gt;Query Insights&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_insights.png&quot; alt=&quot;Query Insights&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 22: Query Insights&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;cloud-spanner&quot;&gt;Cloud Spanner&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_cloud_spanner.png&quot; alt=&quot;Cloud Spanner&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 23: Cloud Spanner&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;firestore&quot;&gt;Firestore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_firestore.png&quot; alt=&quot;Firestore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 24: Firestore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;datastore&quot;&gt;Datastore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_datastore.png&quot; alt=&quot;Datastore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 25: Datastore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;memorystore&quot;&gt;Memorystore&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_memorystore.png&quot; alt=&quot;Memorystore&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 26: Memorystore&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;bigtable&quot;&gt;Bigtable&lt;/h3&gt;

&lt;p&gt;Bigtable is a fully managed NoSQL database service. It is suitable for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Storing &amp;gt; 1TB&lt;/li&gt;
  &lt;li&gt;High Throughput&lt;/li&gt;
  &lt;li&gt;Low latency random data access&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigtable.png&quot; alt=&quot;Bigtable&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 27: Bigtable&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;bigquery&quot;&gt;BigQuery&lt;/h2&gt;

&lt;p&gt;The last section is solely dedicated to BigQuery. BigQuery is a serverless and cost-effective data warehouse. It is deeply integrated with the GCP’s analytical and data processing offering, allowing customers to build an enterprise ready cloud native data warehouse. BigQuery is part of Google Cloud’s comprehensive data analytics platform that covers the analytics value chain from Ingest, process and store to advanced analytics and collaboration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery.png&quot; alt=&quot;BigQuery 1/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 28: BigQuery 1/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_2.png&quot; alt=&quot;BigQuery 2/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 29: BigQuery 2/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_3.png&quot; alt=&quot;BigQuery 3/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 30: BigQuery 3/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_4.png&quot; alt=&quot;BigQuery 4/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 31: BigQuery 4/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_5.png&quot; alt=&quot;BigQuery 5/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 32: BigQuery 5/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_6.png&quot; alt=&quot;BigQuery 6/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 33: BigQuery 6/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_7.png&quot; alt=&quot;BigQuery 7/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 34: BigQuery 7/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_8.png&quot; alt=&quot;BigQuery 8/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 35: BigQuery 8/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_9.png&quot; alt=&quot;BigQuery 9/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 36: BigQuery 9/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_10.png&quot; alt=&quot;BigQuery 10/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 37: BigQuery 10/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_11.png&quot; alt=&quot;BigQuery 11/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 38: BigQuery 11/12&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/18_gcp_bigquery_12.png&quot; alt=&quot;BigQuery 12/12&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Figure 39: BigQuery 12/12&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;Developer Cheat Sheet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://googlecloudcheatsheet.withgoogle.com/&quot;&gt;https://googlecloudcheatsheet.withgoogle.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Cloud Girl:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/priyankavergadia/GCPSketchnote&quot;&gt;https://github.com/priyankavergadia/GCPSketchnote&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Google Cloud Product list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/terms/services&quot;&gt;https://cloud.google.com/terms/services&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;21 products explained under 2 minutes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/blog/topics/inside-google-cloud/21-google-cloud-tools-each-explained-under-2-minutes&quot;&gt;https://cloud.google.com/blog/topics/inside-google-cloud/21-google-cloud-tools-each-explained-under-2-minutes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GCP Data Engineer Study Guide:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf&quot;&gt;https://github.com/xg1990/GCP-Data-Engineer-Study-Guide/blob/master/GCP%20Data%20Engineer.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Data Engineering Cheat Sheet on GCP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf&quot;&gt;https://github.com/ml874/Data-Engineering-on-GCP-Cheatsheet/blob/master/data_engineering_on_GCP.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Schema design best practices for Bigtable:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/bigtable/docs/schema-design&quot;&gt;https://cloud.google.com/bigtable/docs/schema-design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Optimize query computation for BigQuery:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/bigquery/docs/best-practices-performance-compute&quot;&gt;https://cloud.google.com/bigquery/docs/best-practices-performance-compute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Cloud Computing" /><summary type="html">This article contains a collection of notes, mind maps and resources to support you while preparing for the google cloud professional data engineer certification.</summary></entry><entry><title type="html">Learning How to Learn</title><link href="https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn.html" rel="alternate" type="text/html" title="Learning How to Learn" /><published>2023-10-21T00:00:00+00:00</published><updated>2023-10-21T00:00:00+00:00</updated><id>https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn</id><content type="html" xml:base="https://www.firasesbai.com/notes/2023/10/21/learning-how-to-learn.html">&lt;p&gt;&lt;em&gt;Notes from the online course learning how to learn: powerful mental tools to help you master tough subjects by Barbara Oakley.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/17_learning_how_to_learn.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Learning How to Learn Mindmap&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#main-takeaways&quot; id=&quot;markdown-toc-main-takeaways&quot;&gt;Main Takeaways&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-what-is-learning&quot; id=&quot;markdown-toc-1-what-is-learning&quot;&gt;1/ What is learning?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-memory&quot; id=&quot;markdown-toc-2-memory&quot;&gt;2/ Memory&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#working-memory&quot; id=&quot;markdown-toc-working-memory&quot;&gt;Working Memory&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#long-term-memory&quot; id=&quot;markdown-toc-long-term-memory&quot;&gt;Long Term Memory&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-how-to-improve-your-memory&quot; id=&quot;markdown-toc-3-how-to-improve-your-memory&quot;&gt;3/ How to improve your memory&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-chunking&quot; id=&quot;markdown-toc-4-chunking&quot;&gt;4/ Chunking&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#what-is-a-chunk&quot; id=&quot;markdown-toc-what-is-a-chunk&quot;&gt;What is a Chunk?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#how-to-form-a-chunk&quot; id=&quot;markdown-toc-how-to-form-a-chunk&quot;&gt;How to form a chunk?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-value-of-a-library-of-chunks&quot; id=&quot;markdown-toc-the-value-of-a-library-of-chunks&quot;&gt;The value of a Library of Chunks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#interleaving&quot; id=&quot;markdown-toc-interleaving&quot;&gt;Interleaving&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-procrastination&quot; id=&quot;markdown-toc-5-procrastination&quot;&gt;5/ Procrastination&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#what-is-a-habit&quot; id=&quot;markdown-toc-what-is-a-habit&quot;&gt;What is a habit?&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#harnessing-your-zombies&quot; id=&quot;markdown-toc-harnessing-your-zombies&quot;&gt;Harnessing your zombies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#6-juggling-life-and-learning&quot; id=&quot;markdown-toc-6-juggling-life-and-learning&quot;&gt;6/ Juggling Life and Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In a world that’s constantly evolving, the ability to learn efficiently and effectively is a skill that can unlock doors to personal and professional growth. Whether you’re a student striving for academic excellence, a professional seeking to stay competitive in your career, or simply someone who wants to enhance their ability to acquire new knowledge, the &lt;strong&gt;&lt;a href=&quot;https://www.coursera.org/learn/learning-how-to-learn&quot;&gt;Learn How to Learn course&lt;/a&gt;&lt;/strong&gt; is a valuable resource that can empower you to do just that.&lt;/p&gt;

&lt;p&gt;This course has garnered attention from learners of all backgrounds. It delves into the science behind learning and equips you with proven strategies to enhance your learning abilities. As we explore the key takeaways from this course, you’ll discover how to overcome common obstacles, develop effective study habits, and harness the power of your brain to grasp new concepts with confidence.&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-what-is-learning&quot;&gt;1/ What is learning?&lt;/h3&gt;

&lt;p&gt;There are two different modes of thinking:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Focused&lt;/strong&gt;: concentrate intently on something you’re trying to learn or to understand&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Diffuse&lt;/strong&gt;: relaxed thinking style related to a set of neural resting states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you’re learning something new, especially something that’s a bit challenging, your mind needs to be able to go back and forth between the two different learning modes. Study something hard by focusing intently. Then take a break or at least change your focus to something different for a while. During this time of seeming relaxation, your brain’s diffuse mode has a chance to work away in the background and help you out with your conceptual understanding.&lt;/p&gt;

&lt;p&gt;Salvador Dali and Thomas Edison are two well-known innovators who leveraged interleaving between these phases. They would hold onto objects while falling asleep in their chairs. They would think of a problem and once they lost consciousness the object they held would fall to the floor and wake them up. Many of their imaginative ideas came from this state.&lt;/p&gt;

&lt;h3 id=&quot;2-memory&quot;&gt;2/ Memory&lt;/h3&gt;

&lt;h4 id=&quot;working-memory&quot;&gt;Working Memory&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;The part of memory that has to do with what you’re immediately and consciously processing in your mind. It is centred out of the prefrontal cortex. There are also connections to other parts of your brain so you can access long-term memories.&lt;/li&gt;
  &lt;li&gt;Working memory holds only about &lt;strong&gt;four items&lt;/strong&gt; at a time.&lt;/li&gt;
  &lt;li&gt;You often need to keep repeating what you’re trying to work with so it stays in your working memory; for example repeat a phone number to yourself until you have a chance to write it down. You may find yourself shutting your eyes to keep any other items from intruding into the limited slots of your working memory as you concentrate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;long-term-memory&quot;&gt;Long Term Memory&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Wide storage warehouse distributed over the brain.&lt;/li&gt;
  &lt;li&gt;It is immense and has so many items that they can bury each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To move information into long-term memory, it often takes time and practice. To help with this process use a technique called &lt;strong&gt;Spaced Repetition&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Research has shown that when you first try to put an item of information in long-term memory, you need to revisit it at least a few times to increase the chances that you’ll be able to find it later when you might need it.&lt;/p&gt;

&lt;p&gt;You might be surprised to learn that just plain being awake creates toxic products in your brain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sleep&lt;/strong&gt; is your brain’s way of keeping itself healthy. When you sleep, your brain cells shrink. This causes an increase in the space between your brain cells. Fluid can flow past these cells and wash the toxins out.&lt;/p&gt;

&lt;p&gt;Sleep is also an important part of the memory and learning process. It seems that during sleep, your brain tidies up ideas and concepts you’re thinking about and learning. It erases the less important parts of memory and simultaneously strengthens areas that you need or want to remember. During sleep, your brain also rehearses some of the tougher parts of whatever you’re trying to learn, going over and over neural patterns to deepen and strengthen them.&lt;br /&gt;
Sleep has also been shown to make a remarkable difference in your ability to figure out difficult problems and to understand what you’re trying to learn.&lt;/p&gt;

&lt;h3 id=&quot;3-how-to-improve-your-memory&quot;&gt;3/ How to improve your memory&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Simply looking away and seeing what you can recall from the material you’ve just read is a more productive approach than simply rereading. Using recall, mental retrieval of the key ideas, rather than passive rereading, will make your study time more focused and effective. The only time rereading text seems to be effective, is if you let time pass between the rereading, so that it becomes more of an exercise in spaced repetition.&lt;/li&gt;
      &lt;li&gt;Another tip is recalling material when you are outside your usual place of study can also help you strengthen your grasp of the material. You don’t realize it, but when you are learning something new you can often take in subliminal cues for the room and the space around you at the time you were originally learning the material.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mnemonics&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Let’s say you want to remember four plants that help ward off vampires; garlic, rose, hawthorn, and mustard. The first letters abbreviate to GRHM. so all you need to do to remember is to use the image of Graham cracker.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Highlighting and Underlining&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;It must be done very carefully. Otherwise it can not only be ineffective, but also misleading. If you do mark up the text, try to look for main ideas before making any marks. And try to keep your underlining or highlighting to a minimum. On the other hand, words or notes in a margin that synthesizes key concepts are a very good idea.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual/Spatial Memory&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Our mind is built for visual/spatial memory. For example, if you were asked to look around a house you never visited before, you’d soon have a sense of the general furniture layout. We can tap into this memory by associating strange and funny images to memorize a concept. We can use spaced repetition to commit this relationship to long term memory. The more senses you use the easier it is to commit to memory.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory Palace Technique&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Is a particularly powerful way of grouping things you want to remember. It involves calling to mind a familiar place like the layout of your house and using it as a visual notepad where you can deposit the concept images that you want to remember. You would imagine yourself walking through a place you know well, coupled with shockingly memorable images of what you want to remember.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Memory tricks allow people to expand their working memory with easy access to long term memory. What’s more, the memory process itself becomes an exercise in creativity. The more you memorize using these innovative techniques, the more creative you become.&lt;/p&gt;

&lt;h3 id=&quot;4-chunking&quot;&gt;4/ Chunking&lt;/h3&gt;

&lt;h4 id=&quot;what-is-a-chunk&quot;&gt;What is a Chunk?&lt;/h4&gt;

&lt;p&gt;Chunking is the mental leap that helps you unite bits of information together through meaning. The new logical whole makes the chunk easier to remember, and also makes it easier to fit the chunk into the larger picture of what you’re learning.&lt;/p&gt;

&lt;p&gt;Once you chunk an idea, a concept, or an action, you don’t need to remember all the little underlying details. You’ve got the main idea, the chunk, and that’s enough.&lt;/p&gt;

&lt;h4 id=&quot;how-to-form-a-chunk&quot;&gt;How to form a chunk?&lt;/h4&gt;

&lt;p&gt;The best chunks are the ones that are so well ingrained that you don’t even have to consciously think about connecting the neural patterns together. That actually is the point of making complex ideas, movements or reactions into a single chunk.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Focus your undivided attention on the information you want to chunk (limited short term memory)&lt;/li&gt;
  &lt;li&gt;Understand the basic idea of what you want to chunk.
    &lt;ul&gt;
      &lt;li&gt;It helps hold the underlying memory traces together&lt;/li&gt;
      &lt;li&gt;It creates broad encompassing traces that can link to other memory traces&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Practice and repetition with context
    &lt;ul&gt;
      &lt;li&gt;Context means going beyond the initial problem and seeing more broadly&lt;/li&gt;
      &lt;li&gt;Helps you see how your new formed chunks fit in the bigger picture&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is as if you have an attention octopus that slips its tentacles through those four slots of working memory when necessary to help you make connections to information that you might have in various parts of your brain.&lt;/p&gt;

&lt;h4 id=&quot;the-value-of-a-library-of-chunks&quot;&gt;The value of a Library of Chunks&lt;/h4&gt;

&lt;p&gt;Basically what people do to enhance their knowledge and gain expertise, is to gradually build the number of chunks in their mind, valuable bits of information they can piece together in new and creative ways.&lt;/p&gt;

&lt;p&gt;Chunks can also help you understand new concepts. This is because when you grasp one chunk, you’ll find that that chunk can be related in surprising ways to similar chunks, not only in that field but also in very different fields. This idea is called transfer.&lt;/p&gt;

&lt;p&gt;If you have a library of concepts and solutions internalized as chunked patterns, you can think of it as a collection or a library of neural patterns. When you’re trying to figure something out, if you have a good library of these chunks, you can more easily skip to the right solution by, metaphorically speaking, listening to whispers from your diffuse mode. Your diffuse mode can help you connect two or more chunks together in new ways to solve novel problems. Another way to think of it is this, as you build each chunk it is filling in a part of your larger knowledge picture, but if you don’t practice with your growing chunks, they can remain faint and it’s harder to put together the big picture of what you’re trying to learn.&lt;/p&gt;

&lt;p&gt;There are two ways to figure something out or to solve problems. First, through sequential step-by-step reasoning and second, through a more holistic intuition. Sequential thinking where each small step leads deliberately towards a solution, involves the focused mode. Intuition on the other hand often seems to require this creative diffuse mode linking of several seemingly different focused mode thoughts. Most difficult problems and concepts are grasped through intuition, because these new ideas make a leap away from what you’re familiar with. Keep in mind that the diffuse modes, semi-random way of making connections means that the solutions it provides should be very carefully verified using the focused mode. Intuitive insights aren’t always correct.&lt;/p&gt;

&lt;h4 id=&quot;interleaving&quot;&gt;Interleaving&lt;/h4&gt;

&lt;p&gt;Mastering a new subject means learning not only the basic chunks, but also learning how to select and use different chunks. The best way to learn is by practicing jumping back and forth between problems or situations that require different techniques or strategies. 
Interleaving is extraordinarily important. Although practice and repetition is important in helping build solid neural patterns to draw on, it’s interleaving that starts building flexibility and creativity.&lt;/p&gt;

&lt;h3 id=&quot;5-procrastination&quot;&gt;5/ Procrastination&lt;/h3&gt;

&lt;p&gt;Procrastination is the act of unnecessarily and voluntarily delaying or postponing something despite knowing that there will be negative consequences for doing so.&lt;/p&gt;

&lt;p&gt;Procrastination is an easy habit to develop because of the reward. It shares features with addiction. It offers temporary excitement and relief from sometimes boring reality.&lt;/p&gt;

&lt;h4 id=&quot;what-is-a-habit&quot;&gt;What is a habit?&lt;/h4&gt;

&lt;p&gt;Habit is an energy saver for us. It allows us to free our mind for other types of activities. You go into this habitual zombie far more often than you might think, that’s the point of habit. You don’t have to think in a focused manner about what you are doing while you are performing the habit, it saves energy. 
Habits can be good and bad, they can be brief like absently brushing back your hair or they can be long for example when you take a walk or watch television for a few hours after you get home from work.&lt;/p&gt;

&lt;p&gt;You can think of habits as having 4 parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;The cue&lt;/strong&gt;: this is the trigger that launches you into zombie mode&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The routine&lt;/strong&gt;: what we do in reaction to that cue. This is your zombie mode. Zombie responses can be useful, harmless or sometimes harmful.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The reward&lt;/strong&gt;: every habit develops and continues because it rewards us. It gives us an immediate little feeling of pleasure.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The belief&lt;/strong&gt;: habits have power because your belief in them. To change a habit you need to change your underlying belief.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you find yourself avoiding certain tasks because they make you feel uncomfortable, you should know there’s another helpful way to re-frame things and that’s to learn to focus on process not product. Process means the flow of time and the habits and actions associated with that flow of time. As in, I’m going to spend 20 minutes working. Product is an outcome, for example, a homework assignment that you need to finish. To prevent procrastination you want to avoid concentrating on the product. Instead your attention should be on building processes. 
The essential idea here is that the zombie habitual part of your brain likes processes because it can march mindlessly along.&lt;/p&gt;

&lt;h4 id=&quot;harnessing-your-zombies&quot;&gt;Harnessing your zombies&lt;/h4&gt;

&lt;p&gt;The trick to overriding a habit is to look to change your reaction to a cue. The only place you need to apply will power is to change your reaction to the cue. To understand that, it helps to go back through the four components of habit and we analyze them from the perspective of procrastination:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cue
    &lt;ul&gt;
      &lt;li&gt;It usually falls into one of the following categories: location, time, how you feel and reactions, either to other people or to something that just happened. Do you look something up on the web and then find yourself web surfing? The issue with procrastination is that, because it’s an automatic habit, you’re often unaware that you’ve begun to procrastinate. You can prevent the most damaging cues by shutting off your cell phone or keeping away from the internet and other distractions for brief periods of time, as when you’re doing a pomodoro.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Routine
    &lt;ul&gt;
      &lt;li&gt;The key to rewiring is to have a plan. Developing a new ritual can be helpful. Some students make it a habit to leave their phone in their car when they head for class which removes a potent distraction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The reward
    &lt;ul&gt;
      &lt;li&gt;It helps to add a new reward if you want to overcome your previous cravings. Only once your brain starts expecting that reward, will the important rewiring take place that will allow you to create new habits.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The belief
    &lt;ul&gt;
      &lt;li&gt;The most important part of changing your procrastination habit is the belief that you can do it.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;6-juggling-life-and-learning&quot;&gt;6/ Juggling Life and Learning&lt;/h3&gt;

&lt;p&gt;A good way for you to keep perspective about what you’re trying to learn and accomplish is to once a week write a brief weekly list of key tasks in a planner journal. Then each day on another page of your planner, write a list of the tasks that you can reasonably work on or accomplish (The list should be short, 6 items for example where some are process oriented and others product oriented). Try to write this daily task list the evening before. Why? Research has shown that this helps your subconscious to grapple with the tasks on the list, so you can figure out how to accomplish them. Writing the list before you go to sleep enlists your zombies to help you accomplish the items on the list the next day. If you don’t write your tasks down on a list, they lurk at the edge of the four or so slots in your working memory, taking up valuable mental real estate. But once you make a task list, it frees working memory for problem-solving.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading, I hope you enjoyed it!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://barbaraoakley.com/books/learning-how-to-learn/&quot;&gt;https://barbaraoakley.com/books/learning-how-to-learn/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.brainfacts.org/&quot;&gt;https://www.brainfacts.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Procrastination&quot;&gt;https://en.wikipedia.org/wiki/Procrastination&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="notes" /><category term="General" /><summary type="html">Notes from the online course learning how to learn: powerful mental tools to help you master tough subjects by Barbara Oakley.</summary></entry><entry><title type="html">Data Processing Architectures: Lambda vs Kappa</title><link href="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html" rel="alternate" type="text/html" title="Data Processing Architectures: Lambda vs Kappa" /><published>2023-09-24T00:00:00+00:00</published><updated>2023-09-24T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/24/data-processing-architectures-lambda-vs-kappa.html">&lt;p&gt;&lt;em&gt;In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our data-driven era, the ability to harness the power of data has become a pivotal competitive advantage for businesses and organizations across industries. The vast volumes of information generated daily present both opportunities and challenges. How do we efficiently process, analyze, and derive insights from this deluge of data in real-time, without drowning in complexity?&lt;/p&gt;

&lt;p&gt;This is where data processing architectures come into play, offering structured approaches to these challenges. In this blog post, we will explore two prominent contenders in the realm of data processing: the &lt;strong&gt;Lambda&lt;/strong&gt; and &lt;strong&gt;Kappa&lt;/strong&gt; architectures.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-architecture&quot; id=&quot;markdown-toc-lambda-architecture&quot;&gt;Lambda Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#advantages-and-disadvantages&quot; id=&quot;markdown-toc-advantages-and-disadvantages&quot;&gt;Advantages and Disadvantages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kappa-architecture&quot; id=&quot;markdown-toc-kappa-architecture&quot;&gt;Kappa Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#advantages-and-disadvantages-1&quot; id=&quot;markdown-toc-advantages-and-disadvantages-1&quot;&gt;Advantages and Disadvantages&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#choosing-the-right-architecture&quot; id=&quot;markdown-toc-choosing-the-right-architecture&quot;&gt;Choosing the Right Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;lambda-architecture&quot;&gt;Lambda Architecture&lt;/h2&gt;

&lt;p&gt;Lambda architecture was introduced by &lt;em&gt;Nathan Marz&lt;/em&gt; to address the challenges of data processing in a scalable and fault-tolerant manner.
The architecture takes an event stream and forks/duplicates it into two relatively independent layers called the &lt;strong&gt;Batch Layer&lt;/strong&gt; and the &lt;strong&gt;Speed Layer&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Batch layer&lt;/strong&gt; takes incoming data, combines it with historical data, and recomputes the results by iterating over the entire dataset thus allowing the system to give the most accurate results. However, the results are achieved at the expense of high latency due to the long computation time.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Speed Layer&lt;/strong&gt; on the other hand is used to provide a low-latency, near-real-time result. It performs incremental updates on data that was not processed in the last batch of the Batch Layer.&lt;/p&gt;

&lt;p&gt;The results from both systems constitute the &lt;strong&gt;Serving Layer&lt;/strong&gt;. It is responsible for serving queryable, up-to-date results to users or applications. In this layer, the query aims at merging and analyzing data from both the Batch Layer view and the incremental flow view from the Speed Layer.&lt;/p&gt;

&lt;p&gt;There are two variations on this: a &lt;strong&gt;unified serving layer&lt;/strong&gt; with one database for both outputs or &lt;strong&gt;separate serving layers&lt;/strong&gt; with two different databases, one optimized for real time and the other optimized for batch updates.&lt;/p&gt;

&lt;p&gt;The following diagram shows the lambda architecture at a high level:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_lambda_architecture.png&quot; alt=&quot;High Level Lambda Architecture&quot; /&gt;
&lt;em&gt;Figure 1: High Level Lambda Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;One of the key challenges in streaming is the reprocessing of the data. This can be due to code changes because your application evolves and you need to update the business logic or because you found a bug and you need to fix it. In either way, you will need to recompute your output to see the effect of these changes. The batch layer in the lambda architecture addresses this challenge by having a complete history of immutable data. In addition, the usage of two separate systems for processing data makes the lambda architecture flexible, easily scalable and fault tolerant. For instance, it can be used for a variety of use cases, including real-time analytics using the stream processing system and machine learning where models can leverage the large volume of data through the batch layer to generate more accurate results. If one system fails, say the batch processing system, the other can continue to operate providing real time insights into the data. Lastly, both systems can be scaled independently by either adding more nodes to the cluster or adding more streams.&lt;/p&gt;

&lt;p&gt;However, managing two separate processing systems is very complex. We need to provision and manage the infrastructure for two distributed systems including monitoring and logging which increases the cost and operations efforts of storage, compute and networking. Also, we need to align the business logic across streaming and batch codebases resulting in writing the same logic in two places with, most likely, different languages. This leads to difficult debugging and a challenge in validating data quality and making sure that the algorithms in each layer are matching.&lt;/p&gt;

&lt;p&gt;So, what’s different in Kappa architecture?&lt;/p&gt;

&lt;h2 id=&quot;kappa-architecture&quot;&gt;Kappa Architecture&lt;/h2&gt;

&lt;p&gt;The Kappa architecture was introduced by &lt;em&gt;Jay Kreps&lt;/em&gt;, co-founder and CEO at Confluent, a company built around the open source messaging system Apache Kafka, as a response to some of the challenges and complexities associated with the Lambda Architecture. 
The Kappa Architecture primarily focuses on stream processing simplifying the complexity of maintaining two systems with a single technology stack, referred to as &lt;strong&gt;Stream Processing Layer&lt;/strong&gt; in the diagram below, that can perform both real-time and batch processing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/16_kappa_architecture.png&quot; alt=&quot;High Level Kappa Architecture&quot; /&gt;
&lt;em&gt;Figure 2: High Level Kappa Architecture&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;advantages-and-disadvantages-1&quot;&gt;Advantages and Disadvantages&lt;/h3&gt;

&lt;p&gt;The shift to a single stream processing system to handle both real-time and batch processing makes the Kappa architecture simpler, more efficient and cost effective than the lambda architecture. Having a single source of truth to all the data reduces both the burden of maintaining two separate systems and two codebases as well as the underlying costs. People can now develop, test, debug, and operate their systems on top of a single processing framework such as Apache Kafka.&lt;/p&gt;

&lt;p&gt;Stream processing is considered a paradigm shift from the traditional batch data processing. Therefore, it goes without saying that the Kappa architecture presents some challenges and limitations. In fact, processing out of order data or intricate joins combining many streams causes difficulties when transforming data in a streaming method. On top of that, data reprocessing which is now running using a single codebase, on the same framework, and with the same input data, still comes with some tradeoffs. As Jay Kreps detailed in his 
&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;original post&lt;/a&gt;, we can leverage Apache Kafka retention period (30 days for example) and take the retained data as an input to a second instance of the streaming process that will produce a new output table with the reprocessed data. However, this approach depends heavily on the configured retention period value and is limited in cases where we need to fix the algorithm or deploy a change like adding a new field that goes beyond the span of the retention period.&lt;/p&gt;

&lt;h2 id=&quot;choosing-the-right-architecture&quot;&gt;Choosing the Right Architecture&lt;/h2&gt;

&lt;p&gt;So when should we use one architecture or the other? As is often the case, it depends on some peculiarities of the implemented application.&lt;/p&gt;

&lt;p&gt;A very simple case is when the algorithms used for real-time and historical data are identical and implementable on streaming. It is then clearly very advantageous to use the same codebase to process historical and real-time data, and hence use the Kappa Architecture.
If the algorithms used to process historical data and real-time data are not always identical. Here, the choice between Lambda and Kappa becomes a tradeoff between the performance benefits of batch processing over a simpler codebase.&lt;/p&gt;

&lt;p&gt;Some examples of use cases where Kappa architecture is a good fit include fraud detection to detect fraudulent transactions in real time, process data from IoT devices in real time or recommendation engines used to to provide personalized recommendations to users in real time.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Lambda and Kappa are two popular data processing architectures that can be used to handle huge data. The ideal architecture relies on the individual needs for a given use case. We have discussed some benefits as well as drawbacks and limitations of each design to assist you in making your decision. Real-time insights are more important for businesses that want to become data-driven, which has increased the popularity of event streaming architecture. Batch processing will not go away, therefore it is essential to view the two architectures as complementing solutions rather than one being a cure for all ills.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&quot;&gt;https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&quot;&gt;https://www.oreilly.com/radar/questioning-the-lambda-architecture/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&quot;&gt;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><summary type="html">In this article we will explore two popular data processing architectures: Lambda and Kappa. We will take a look at their components, key differences and how to choose between them.</summary></entry><entry><title type="html">Understanding Modern Data Stack</title><link href="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html" rel="alternate" type="text/html" title="Understanding Modern Data Stack" /><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/09/10/understanding-modern-data-stack.html">&lt;p&gt;&lt;em&gt;In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The modern data stack is designed to empower organizations to harness the full potential of their data assets, make data-driven decisions, and stay competitive in today’s data-centric business landscape. It represents a shift towards more agile, integrated, and scalable data management practices.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#a-brief-history&quot; id=&quot;markdown-toc-a-brief-history&quot;&gt;A Brief History&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-the-modern-data-stack&quot; id=&quot;markdown-toc-what-is-the-modern-data-stack&quot;&gt;What is the Modern Data Stack?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#objectives-of-the-modern-data-stack&quot; id=&quot;markdown-toc-objectives-of-the-modern-data-stack&quot;&gt;Objectives of the Modern Data Stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#components-of-the-modern-data-stack&quot; id=&quot;markdown-toc-components-of-the-modern-data-stack&quot;&gt;Components of the Modern Data Stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;a-brief-history&quot;&gt;A Brief History&lt;/h2&gt;

&lt;p&gt;The modern data stack as we currently know it is a very recent development in data. In fact the rise of the cloud data warehouse triggered by the release of Amazon Redshift in late 2012 is considered one of the key developments that led to the adoption of the modern data stack. Redshift was one of the early cloud-based data warehousing solutions that offered a highly scalable and cost-effective platform for storing and analyzing large datasets. Its introduction marked a shift away from traditional on-premises data warehousing and towards cloud-based solutions. All of the other solutions in the market today like Google BigQuery and Snowflake followed the revolution set by Amazon.&lt;/p&gt;

&lt;p&gt;Consequently this shift led to the move from &lt;em&gt;Extract Transform Load (ETL)&lt;/em&gt; to &lt;em&gt;Extract Load Transform (ELT)&lt;/em&gt; pipelines. As storage becomes cheaper and more accessible, there is no need to deal with data transformations before saving it in the traditional data warehouse. Organizations just dump their data in its raw format and only apply the transformations later when needed.&lt;/p&gt;

&lt;p&gt;The rise of the cloud data warehouse has not only contributed to the transition from ETL to ELT but also the widespread adoption of BI tools. These self serve solutions democratize data usage allowing more and more personas to access it and make data-driven business decisions.‍&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-modern-data-stack&quot;&gt;What is the Modern Data Stack?&lt;/h2&gt;

&lt;p&gt;The modern data stack is a collection of tools and technologies used together to support the data flow starting from ingestion and integration of different data sources up to analysis in order to extract insights and help create data driven decisions. The particularity resides in the plug and play nature of its components and the overall ease of use without much infrastructure and data platform management overhead so that data is accessible for everyone to turn it into knowledge.&lt;/p&gt;

&lt;h2 id=&quot;objectives-of-the-modern-data-stack&quot;&gt;Objectives of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;The objectives of the modern data stack revolve around building a data infrastructure that enables organizations to efficiently and effectively manage their data, derive valuable insights, and make data-driven decisions.&lt;/p&gt;

&lt;p&gt;Here are the primary objectives of implementing a modern data stack:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Accommodate Growing Data - The modern data stack should be able to handle increasing data volumes, whether structured or unstructured, as organizations collect more data from various sources.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Flexibility and Agility&lt;/strong&gt;: Easily Adapt to Changing Needs - It should be flexible enough to adapt to changing business requirements, data sources, and processing methods without the need for a complete overhaul.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Seamlessly Connect Data Sources: The stack should provide tools and processes for integrating data from diverse sources, including databases, applications, APIs, and more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cost-Efficiency&lt;/strong&gt;: Optimize Resource Usage by minimising unnecessary resource usage by efficiently managing data storage and processing to control costs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: Ensure Compatibility - Ensure that the various components of the stack can interoperate smoothly with each other and with external systems or tools.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cloud-Ready&lt;/strong&gt;: Leverage Cloud Infrastructure - Be compatible with cloud-based infrastructure to take advantage of scalability, cost-effectiveness, and the latest data services provided by cloud providers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance and Reliability&lt;/strong&gt;: Maintain High Performance - Deliver reliable and high-performance data processing to support critical business operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adaptability to Emerging Technologies&lt;/strong&gt;: Be Open to Innovation - Keep an eye on emerging technologies and trends, allowing for easy integration with new tools or platforms that may enhance the stack’s capabilities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;components-of-the-modern-data-stack&quot;&gt;Components of the Modern Data Stack&lt;/h2&gt;

&lt;p&gt;In a previous blog post about the &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;fundamentals of data engineering&lt;/a&gt; we tried to identify a common data flow that identifies the different stages including ingestion, storage, transformation, data management/governance, visualization and exploration  which are  involved in making data easily accessible.&lt;/p&gt;

&lt;p&gt;The components of the modern data stack can be perfectly mapped to the same diagram from the mentioned article, at least at a high level first, as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/15_modern_data_stack_example.png&quot; alt=&quot;Example of Componentes of Modern Data Stack in Standard Data Flow&quot; /&gt;
&lt;em&gt;Figure 1: Example of Componentes of Modern Data Stack in Standard Data Flow&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that the specific tools are changing and evolving rapidly but they usually include some of the ones we chose as an example in the diagram. In addition, some vendor technologies fit beyond a single stage as presented in the diagram and offer more capabilities such as data governance and/or machine learning.&lt;/p&gt;

&lt;p&gt;However, this only captures the stack at a high level. In fact, there is no one-size-fits-all approach when it comes to selecting the best tools and technologies to deal with your data. Every organization has a different level of data maturity, different data teams, different structures, processes, and so on.&lt;/p&gt;

&lt;p&gt;Therefore, the stack can be enriched with a &lt;strong&gt;workflow  orchestration tool&lt;/strong&gt; such as &lt;a href=&quot;https://airflow.apache.org/&quot;&gt;Apache Airflow&lt;/a&gt; or &lt;a href=&quot;https://dagster.io/&quot;&gt;Dagster&lt;/a&gt; needed to schedule your transformation in an automated fashion depending on your required frequency.&lt;/p&gt;

&lt;p&gt;Also &lt;strong&gt;data observability&lt;/strong&gt; has become a key part of the modern data stack. It ensures data reliability by monitoring data quality and identifying potential data issues throughout the stack. Explaining this concept is beyond the scope of this article but we will have a dedicated blog post about. Some of the tools worth mentioning here include &lt;a href=&quot;https://www.montecarlodata.com/&quot;&gt;Monte Carlo&lt;/a&gt; and &lt;a href=&quot;https://www.datadoghq.com/&quot;&gt;Datadog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we explored the concept of the modern data stack and its significance in the contemporary data landscape. We provided an overview of its main components and how they correlate to our standard data flow established in a previous article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/modern-data-stack/&quot;&gt;https://preset.io/blog/modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://preset.io/blog/reshaping-data-engineering/&quot;&gt;https://preset.io/blog/reshaping-data-engineering/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&quot;&gt;https://www.getdbt.com/blog/future-of-the-modern-data-stack/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&quot;&gt;https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure-2020/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><category term="Data Architecture" /><category term="Cloud Computing" /><summary type="html">In this article we will explore the modern data stack, a brief history that led to its adoption and a brief walkthrough of its components and objectives.</summary></entry><entry><title type="html">Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide</title><link href="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html" rel="alternate" type="text/html" title="Turning Your Jekyll Blog into a Progressive Web App (PWA) on GitHub Pages: A Step-by-Step Guide" /><published>2023-08-21T00:00:00+00:00</published><updated>2023-08-21T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/08/21/turning-your-jekyll-blog-into-a-progressive-web-app-on-github-pages.html">&lt;p&gt;&lt;em&gt;In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Progressive Web Apps (PWAs) represent a significant advancement in web application development, offering users a seamless and engaging experience that combines the best aspects of both web and native mobile applications. 
By transforming your Jekyll-based blog into a PWA, you’ll enhance user engagement, improve performance, and enable key features such as offline access.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-are-progressive-web-apps&quot; id=&quot;markdown-toc-what-are-progressive-web-apps&quot;&gt;What are Progressive Web Apps?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#key-attributes-of-pwas&quot; id=&quot;markdown-toc-key-attributes-of-pwas&quot;&gt;Key Attributes of PWAs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#successful-pwa-examples&quot; id=&quot;markdown-toc-successful-pwa-examples&quot;&gt;Successful PWA Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#technical-features-of-pwas&quot; id=&quot;markdown-toc-technical-features-of-pwas&quot;&gt;Technical features of PWAs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#creating-a-manifest-file&quot; id=&quot;markdown-toc-creating-a-manifest-file&quot;&gt;Creating a Manifest File&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#implementing-service-workers&quot; id=&quot;markdown-toc-implementing-service-workers&quot;&gt;Implementing Service Workers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#deploying-service-workers&quot; id=&quot;markdown-toc-deploying-service-workers&quot;&gt;Deploying Service Workers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;what-are-progressive-web-apps&quot;&gt;What are Progressive Web Apps?&lt;/h2&gt;

&lt;p&gt;Platform-specific apps are developed for a specific operating system (OS) and/or class of devices, like an iOS or Android device. They are usually installed on the user’s device using the vendor’s app store. Websites on the other hand, can only be accessed by the user opening the browser and navigating to the site, and is highly dependent on network connectivity.&lt;/p&gt;

&lt;p&gt;So how does this relate to PWAs?&lt;/p&gt;

&lt;p&gt;Progressive web apps combine the best features of traditional websites and platform-specific apps. At their core, PWAs are web applications that take advantage of modern web technologies to deliver a reliable, fast, and immersive experience to users. Unlike traditional websites, PWAs can be installed on users’ devices, giving them a direct pathway to your content, even without a traditional app store. PWAs can be accessed through web browsers, but they offer the responsiveness and fluidity that users typically expect from native mobile applications.&lt;/p&gt;

&lt;h2 id=&quot;key-attributes-of-pwas&quot;&gt;Key Attributes of PWAs&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Offline Access&lt;/strong&gt;: One of the most significant advantages of PWAs is their ability to work offline or in low-network conditions. Users can still access content and navigate within the app, even when they’re not connected to the internet. This offline capability ensures that your content remains accessible, enhancing user satisfaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fast Loading&lt;/strong&gt;: PWAs are designed to load quickly, providing an almost instant experience to users. This is achieved through techniques like efficient caching, optimized assets, and lazy loading of content. Fast loading times lead to lower bounce rates and higher user retention.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Responsive Design&lt;/strong&gt;: PWAs are responsive by default, adapting to various screen sizes and orientations. This responsiveness ensures a consistent and visually appealing experience across devices, including smartphones, tablets, and desktops.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Engagement and Retention&lt;/strong&gt;: PWAs can be “installed” on users’ home screens or app drawers, creating a sense of ownership and encouraging repeated visits. This increased engagement can lead to higher retention rates, as users have easy access to your PWA.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;successful-pwa-examples&quot;&gt;Successful PWA Examples&lt;/h2&gt;

&lt;p&gt;Numerous companies have embraced PWAs to enhance user experience and drive business growth. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pinterest&lt;/strong&gt;: Pinterest’s PWA increased user engagement, with faster load times leading to a 60% increase in user engagement and a 44% increase in user-generated ad revenue.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alibaba&lt;/strong&gt;: Alibaba.com’s PWA achieved a 76% increase in conversions across browsers, with 14% more monthly active users on iOS and a 30% increase in mobile users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, check the links in the resources section.&lt;/p&gt;

&lt;h2 id=&quot;technical-features-of-pwas&quot;&gt;Technical features of PWAs&lt;/h2&gt;

&lt;p&gt;Because PWAs are websites, they have the same basic features as any other website: at least one HTML page, which very probably loads some CSS and JavaScript.&lt;/p&gt;

&lt;p&gt;Beyond that, a PWA has some additional features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;web app manifest file&lt;/strong&gt;, which, at a minimum, provides information that the browser needs to install the PWA, such as the app name and icon.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;service worker&lt;/strong&gt;, which, at a minimum, provides a basic offline experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having this in mind, a blog is nothing more than static HTML, CSS, and Javascript files. This makes it a prime candidate for adding PWA features which is the focus of the rest of this blog post.&lt;/p&gt;

&lt;p&gt;We will be adding these features to a blog built using Jekyll, a free and open source static site generator, and hosted on Github Pages. For more in depth guides on how I started my journey building this blog, you can check my 4 Parts series starting from &lt;a href=&quot;https://www.firasesbai.com/articles/2021/10/07/how-i-started-this-blog.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-manifest-file&quot;&gt;Creating a Manifest File&lt;/h2&gt;

&lt;p&gt;The Web App Manifest is a JSON document that provides application metadata such as its name, icon, and other details, which browsers can use when adding the PWA to the home screen.&lt;/p&gt;

&lt;p&gt;If you have previously generated a favicon to your blog using &lt;a href=&quot;https://realfavicongenerator.net/&quot;&gt;https://realfavicongenerator.net/&lt;/a&gt; or similar, you should already have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;site.webmanifest&lt;/code&gt; file that might be complete or miss a few properties. Otherwise, you can just use the following link &lt;a href=&quot;https://app-manifest.firebaseapp.com/&quot;&gt;https://app-manifest.firebaseapp.com/&lt;/a&gt; for reference or to generate its content.&lt;/p&gt;

&lt;p&gt;In either cases, the next step would be to add a reference to the manifest file under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file in the head section as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;      
   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;manifest&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/assets/favicon/site.webmanifest&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
   
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The location of your manifest file might differ and you should update it accordingly. In my case, the manifest file is placed under assets along with the icons and favicon.&lt;/p&gt;

&lt;h2 id=&quot;implementing-service-workers&quot;&gt;Implementing Service Workers&lt;/h2&gt;

&lt;p&gt;A service worker is a powerful web technology that acts as a scriptable network proxy between a web application (such as a website) and the browser. It runs in the background, separate from the main web page, and allows you to intercept and control network requests and responses, enabling advanced features like offline access, caching, and push notifications in web applications.&lt;/p&gt;

&lt;p&gt;We will be using &lt;a href=&quot;https://developer.chrome.com/docs/workbox/&quot;&gt;Workbox&lt;/a&gt; which is a set of Javascript modules created by Google that simplifies and addresses a specific aspect of service worker development.&lt;/p&gt;

&lt;p&gt;1- Download and install the latest version of Node.js from the &lt;a href=&quot;https://nodejs.org/&quot;&gt;official website&lt;/a&gt;, and npm will be installed automatically as part of the Node.js package.&lt;/p&gt;

&lt;p&gt;2- Install Workbox CLI by running the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;npm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cli&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3- Use the Workbox Wizard to create a service worker by executing the following command inside the directory of your blog:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wizard&lt;/span&gt; 
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The Workbox CLI wizard will guide you through setting up the service worker by choosing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; directory as the root of your app and selecting the type of files the service worker should pre-cache.&lt;/p&gt;

&lt;p&gt;At the end of this step, you should have a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-config.js&lt;/code&gt; created at the root of the project.&lt;/p&gt;

&lt;p&gt;4- Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;workbox-confiog.js&lt;/code&gt; file to generate the service worker by executing the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;  &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generateSW&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A new file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder will be generated.&lt;/p&gt;

&lt;p&gt;5- Create a new javascript file used to register your service worker. Place this file under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/js/service-worker.js&lt;/code&gt; in your Jekyll project’s root directory.&lt;/p&gt;

&lt;p&gt;The content of this file is the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;    &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Only&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trigger&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;supported&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;browser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'serviceWorker'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;navigator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Wait&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;until&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loaded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;before&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registering&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;addEventListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'load'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Register&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'s scope.
        navigator.serviceWorker.register('&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;js&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', { scope: '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' })
            // Output success/failure of registration.
            .then(() =&amp;gt; console.log('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registered&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'))
            .catch(() =&amp;gt; console.error('&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Service&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Worker&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;registration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In order to register the service worker and enable its functionality, you’ll need to include this code snippet to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default.html&lt;/code&gt; file, which is a layout file included in all your pages, as follow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;     
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/js/service-worker.js&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/script&amp;gt;
  
  &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point if your development server is already running, you can navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:4000&lt;/code&gt; and use developer tools to verify that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;By checking the Offline box under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Manifest -&amp;gt; Service worker&lt;/code&gt; you can still access your site.&lt;/li&gt;
  &lt;li&gt;Under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Application -&amp;gt; Cache Storage&lt;/code&gt; new entry have been created&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The newly created cache entry indicates that the predetermined assets in the workbox-config file are added to the cache during the service worker installation. This is called &lt;strong&gt;precaching&lt;/strong&gt; and is commonly used to ensure that essential resources are available offline and to improve the initial loading performance of your application.&lt;/p&gt;

&lt;p&gt;For more details on some precaching considerations you can check this &lt;a href=&quot;https://developer.chrome.com/docs/workbox/precaching-dos-and-donts/&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another type of caching is &lt;strong&gt;runtime caching&lt;/strong&gt;. It  involves caching resources dynamically during the runtime of your web application, i.e., when users interact with the application. Unlike precaching, runtime caching allows you to define caching strategies for specific URLs or URL patterns based on different criteria such as network requests, HTTP methods, and more.&lt;/p&gt;

&lt;p&gt;There some common runtime caching strategies that are out of the scope of this blog post. For more details you can refer to the &lt;a href=&quot;https://developer.chrome.com/docs/workbox/modules/workbox-strategies/&quot;&gt;workbox documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deploying-service-workers&quot;&gt;Deploying Service Workers&lt;/h2&gt;

&lt;p&gt;Knowing that Jekyll regenerates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt; folder with each change you make to your files, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw.js&lt;/code&gt; will be lost and we have to regenerate it each time. Since our site is hosted on Github Pages, we can leverage &lt;strong&gt;Github Actions&lt;/strong&gt; to automate the execution of these commands as part of the pipeline building and deploying your site.&lt;/p&gt;

&lt;p&gt;An example of a Github Actions workflow can be found &lt;a href=&quot;https://github.com/firasesbai/firasesbai.github.io/blob/master/.github/workflows/github-pages.yml&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;p&gt;If you have any remarks or questions, please don’t hesitate and do drop a comment below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;Congratulations! You’ve successfully transformed your Jekyll blog into a powerful Progressive Web App. By implementing service workers and a manifest file, you’ve unlocked offline access and responsive design, making your content accessible to users even when they’re offline. Remember, this is just the beginning and there’s a wealth of additional features and optimizations you can explore to further enhance your PWA.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&quot;&gt;https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps/Guides/What_is_a_progressive_web_app&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/what-are-pwas/&quot;&gt;https://web.dev/what-are-pwas/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&quot;&gt;https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/alibaba/&quot;&gt;https://web.dev/alibaba/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&quot;&gt;https://fredrickb.com/2019/07/25/turning-jekyll-site-into-a-progressive-web-app/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sevic.dev/caching-service-worker-workbox/&quot;&gt;https://sevic.dev/caching-service-worker-workbox/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Blogging" /><summary type="html">In this guide, we’ll explore how to transform your Jekyll-based blog into a Progressive Web App, unlocking features such as offline access, fast loading, and a seamless user experience.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 2</title><link href="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 2" /><published>2023-03-12T00:00:00+00:00</published><updated>2023-03-12T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/12/data-engineering-201-part-2.html">&lt;p&gt;&lt;em&gt;This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will discover how data in motion and data at rest is handled through different techniques and methods and the importance of choosing the right storage technology.&lt;/p&gt;

&lt;p&gt;If you have missed the first part, you can find it &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So buckle up, folks! This is going to be a long ride. But don’t worry, it’s worth it. Grab a snack, get comfy, and let’s dive in!”&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ingestion&quot; id=&quot;markdown-toc-ingestion&quot;&gt;Ingestion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transformation&quot; id=&quot;markdown-toc-transformation&quot;&gt;Transformation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storage&quot; id=&quot;markdown-toc-storage&quot;&gt;Storage&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#oltp-vs-olap&quot; id=&quot;markdown-toc-oltp-vs-olap&quot;&gt;OLTP vs OLAP&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#oltp-databases&quot; id=&quot;markdown-toc-oltp-databases&quot;&gt;OLTP Databases&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#olap-databases&quot; id=&quot;markdown-toc-olap-databases&quot;&gt;OLAP Databases&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-warehouse&quot; id=&quot;markdown-toc-data-warehouse&quot;&gt;Data warehouse&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-lake&quot; id=&quot;markdown-toc-data-lake&quot;&gt;Data lake&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#data-lakehouse&quot; id=&quot;markdown-toc-data-lakehouse&quot;&gt;Data Lakehouse&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;ingestion&quot;&gt;Ingestion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ETL (Extract, Transform, Load)&lt;/strong&gt; and &lt;strong&gt;ELT (Extract, Load, Transform)&lt;/strong&gt; are both data integration techniques used to move data from multiple sources to a single destination, such as a data warehouse or a data lake (more on this later in the Storage section).&lt;/p&gt;

&lt;p&gt;In ETL, data is extracted from source systems, transformed into a format suitable for the destination system, and then loaded into the destination system. 
The transformation is actually done in what is often referred to as a staging area. 
This approach is typically used in traditional data warehousing systems. 
Any data you load into your data warehouse must be transformed into a relational format before the data warehouse can ingest it. 
As a part of this data transformation process, data mapping may also be necessary to combine multiple data sources based on correlating information. 
In addition, ETL can help with data privacy and compliance by cleaning sensitive and secure data even before loading into the data warehouse.&lt;/p&gt;

&lt;p&gt;In ELT on the other hand, data is first extracted from source systems and loaded into the destination system in its raw form, where it is then transformed into the desired format. 
This approach is typically used in big data environments, where the target system, such as a data lake, is designed to handle large amounts of unstructured and semi-structured data and can perform the transformations in parallel.&lt;/p&gt;

&lt;p&gt;Rather than obsessing over this ETL vs ELT cage fight, just try to take away the following:&lt;/p&gt;

&lt;p&gt;Sometimes you may want to optimize/reshape your data sooner (because you know that’s how everyone wants to use it). 
Other times, you want to leave the schema flexible (and just let the user’s queries/views do the work) to avoid having to maintain lots of tables/views/jobs.&lt;/p&gt;

&lt;h2 id=&quot;transformation&quot;&gt;Transformation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Batch&lt;/strong&gt; and &lt;strong&gt;Stream&lt;/strong&gt; processing are two popular methods for data processing and transformation.&lt;/p&gt;

&lt;p&gt;In batch processing, we wait for a certain amount of raw data to “pile up” before running an ETL job. 
Typically this means data is between an hour to a few days old before it is made available for analysis. 
Batch ETL jobs will typically be run on a set schedule (e.g. every 24 hours), or in some cases once the amount of data reaches a certain threshold.
You should lean towards batch processing when:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data freshness is not a mission-critical issue&lt;/li&gt;
  &lt;li&gt;You are working with large datasets and are running a complex algorithm that requires access to the entire batch – e.g., sorting the entire dataset&lt;/li&gt;
  &lt;li&gt;You get access to the data in batches rather than in streams&lt;/li&gt;
  &lt;li&gt;When you are joining tables in relational databases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In stream processing, we process data as soon as it arrives in the storage layer – which would often also be very close to the time it was generated (although this would not always be the case). 
This would typically be in sub-second timeframes, so that for the end user the processing happens in real-time. 
These operations would typically not be stateful, or would only be able to store a ‘small’ state, so would usually involve a relatively simple transformation or calculation.&lt;/p&gt;

&lt;p&gt;Another alternative is &lt;strong&gt;micro-batch&lt;/strong&gt; processing. 
In micro-batch processing, we run batch processes on much smaller accumulations of data – typically less than a minute’s worth of data. 
This means data is available in near real-time. 
In practice, there is little difference between micro-batching and stream processing, and the terms would often be used interchangeably in data architecture descriptions and software platform descriptions.
Microbatch processing is useful when we need very fresh data, but not necessarily real-time – meaning we can’t wait an hour or a day for a batch processing to run, but we also don’t need to know what happened in the last few seconds. 
Example scenarios could include web analytics (clickstream) or user behavior.&lt;/p&gt;

&lt;h2 id=&quot;storage&quot;&gt;Storage&lt;/h2&gt;

&lt;p&gt;Two different types of data are used today in an organization; &lt;strong&gt;Operational&lt;/strong&gt; and &lt;strong&gt;Analytical&lt;/strong&gt;. 
Both operational and analytical data are important for organizations, but they serve different purposes and are used in different ways.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;​​Operational data refers to the data that is used in day-to-day business operations to support critical functions such as sales, marketing, and customer service. Operational data is often used to support short-term decision making and is focused on current and immediate needs.&lt;/li&gt;
  &lt;li&gt;Analytical data, on the other hand, is used for long-term strategic decision making. Analytical data is used to support trend analysis, business intelligence, and other data-driven decision making processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;oltp-vs-olap&quot;&gt;OLTP vs OLAP&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;OLTP (Online Transaction Processing)&lt;/strong&gt; and &lt;strong&gt;OLAP (Online Analytical Processing)&lt;/strong&gt; are two different types of data processing systems that are often used to manage operational and analytical data, respectively.&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLTP systems are used to process operational data and are designed to support rapid data insertion, updates, and retrievals.&lt;/li&gt;
  &lt;li&gt;Make sure that the systems can keep up with high volumes of transactions but often very small and fast in nature (e.g. online banking, FinTech application).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OLAP systems are used to process analytical data and are designed to support large-scale data analysis.&lt;/li&gt;
  &lt;li&gt;Make sure that you can crunch through millions or billions of rows of data for your complex and large theories, where they need to run some fancy aggregation or calculations for data Analytics purposes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Example: Online Store&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;OLTP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Store user data, passwords, previous transactions, find user, change its name,… basically perform INSERT, UPDATE, DELETE operations&lt;/li&gt;
  &lt;li&gt;Store actual products, their associated prices&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OLAP&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Find out the “total money spent by all users”&lt;/li&gt;
  &lt;li&gt;Find out “what is the most sold product”&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;oltp-databases&quot;&gt;OLTP Databases&lt;/h4&gt;

&lt;p&gt;OLTP databases can use either &lt;strong&gt;SQL (Structured Query Language)&lt;/strong&gt; or &lt;strong&gt;NoSQL (Not only SQL)&lt;/strong&gt; technologies.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;SQL Database&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;NoSQL Database&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Data storage model&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tables with fixed rows and columns&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: JSON Documents &lt;br /&gt; Key-value: key-value pairs &lt;br /&gt; Wide-Columns: Tables with rows and dynamic columns &lt;br /&gt; Graph&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Development history&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the 1970s with a focus on reducing data duplication&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Developed in the late 2000s with a focus on scaling and allowing for rapid application change driven by agile and DevOps practices&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Primary purpose&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;General purpose&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Document: general purpose &lt;br /&gt; Key-value: large amounts of data with simple lookup queries &lt;br /&gt; Wide-column: large amounts of data with predictable query patterns &lt;br /&gt; Graph: analyzing and traversing relationships between connected data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Rigid&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Flexible (Implicit schema!, querying time)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scaling&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Vertical (scale-up with a larger server)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Horizontal (scale-out across commodity servers)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Joins&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically required&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Typically not required&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;SQL databases are based on a relational model and use a structured data model, where data is organized into tables, rows, and columns. 
This makes it easy to enforce data constraints, such as unique keys, and ensures data consistency. 
They are well tested and proven, and they have a long history of use in OLTP systems. 
This makes them a reliable choice for OLTP systems. 
However, SQL databases can be complex to set up and maintain and can be challenging to scale, particularly for large OLTP systems that require horizontal scalability and the rigid schema of SQL databases can make it difficult to accommodate changing requirements or new data types.&lt;/p&gt;

&lt;p&gt;NoSQL databases on the other hand are designed for horizontal scalability, which makes them well suited for OLTP systems that need to scale to handle large amounts of data and users. 
In addition, they use a variety of data models which makes them more flexible than SQL databases. 
This allows NoSQL databases to better handle unstructured data and changing data requirements. 
However , NoSQL databases may not provide the same level of transactional consistency as SQL databases, which can result in data inconsistencies. 
Plus they can be complex to set up and maintain as well, even though this can be addressed since they are designed to work well in cloud environments, which makes them a good choice for OLTP systems that need to scale quickly and elastically.&lt;/p&gt;

&lt;p&gt;We kept mentioning &lt;strong&gt;transactional consistency&lt;/strong&gt; and you might be wondering what is it?&lt;/p&gt;

&lt;p&gt;A transaction is a sequence of operations performed (using one or more SQL statements) on a database as a single logical unit of work. 
Transactions have the following four standard properties, usually referred to by the acronym &lt;strong&gt;ACID&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt; − ensures that all operations within the work unit are completed successfully. Otherwise, the transaction is aborted at the point of failure and all the previous operations are rolled back to their former state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; − ensures that the database properly changes states upon a successfully committed transaction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; − enables transactions to operate independently of and transparent to each other.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt; − ensures that the result or effect of a committed transaction persists in case of a system failure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;olap-databases&quot;&gt;OLAP Databases&lt;/h4&gt;

&lt;p&gt;In this section we will discuss two important components that are used for advanced analysis and decision-making; &lt;strong&gt;data warehouses&lt;/strong&gt; and &lt;strong&gt;data lakes&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;data-warehouse&quot;&gt;Data warehouse&lt;/h4&gt;

&lt;p&gt;A data warehouse is a centralized repository for storing and managing large amounts of data from various sources. 
Data warehouses are designed to support business intelligence (BI) and analytics applications, by providing a single source of data that can be queried, analyzed, and used to make informed decisions. 
The following are the key characteristics of a data warehouse:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Integration&lt;/em&gt; - Data warehouses integrate data from multiple sources, such as transactional systems, log files, and external data sources, into a single, unified view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data warehouses are designed to handle large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Modeling&lt;/em&gt; - Data warehouses use a specific data model, such as the star or snowflake schema, to organize data and make it easier to query and analyze.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Historical data&lt;/em&gt; - Data warehouses store historical data, allowing users to analyze trends and changes over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Performance optimization&lt;/em&gt; - Data warehouses are optimized for fast querying and analysis, by using techniques such as indexing, materialized views, and aggregations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data Cleansing&lt;/em&gt; - Data warehouses often include data cleansing and normalization to ensure that data is consistent and accurate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Security and access control&lt;/em&gt; - Data warehouses have robust security and access control features to ensure that sensitive data is protected and only authorized users have access to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The explosion of big data, including the growth of structured, semi-structured, and unstructured data, made it increasingly difficult for traditional data warehouses to store and process all the data being generated. 
In addition, Traditional data warehouses required expensive hardware and software to store and process data. 
The cost of these solutions made it difficult for organizations to store all their data, which led to the adoption of data lakes as a more cost-effective alternative.&lt;/p&gt;

&lt;h4 id=&quot;data-lake&quot;&gt;Data lake&lt;/h4&gt;

&lt;p&gt;A data lake is a centralized repository that stores large amounts of raw, structured and unstructured data. The data is stored in its native format and can be accessed, processed, and analyzed later as needed. 
The following are the key characteristics of a data lake:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Flexibility&lt;/em&gt; - Data lakes allow organizations to store a wide variety of data types and formats, including structured, semi-structured, and unstructured data, without having to worry about pre-defining schemas.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scalability&lt;/em&gt; - Data lakes are designed to handle very large amounts of data, which can grow over time.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Cost-effectiveness&lt;/em&gt; - Data lakes are often implemented on low-cost, commodity hardware&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Raw data preservation&lt;/em&gt; - Data lakes preserve raw data, allowing organizations to perform in-depth analysis and retain the original data for future use.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Decentralized processing&lt;/em&gt; - Data lakes can be used to distribute processing tasks across multiple nodes in a network, allowing for increased processing speed and scalability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Self-service analytics&lt;/em&gt; - Data lakes allow business users and data scientists to perform their own data analysis, without having to rely on IT or data engineering teams.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integration with big data tools&lt;/em&gt; - Data lakes can be integrated with big data tools, such as Apache Spark and Apache Flink, allowing organizations to perform complex data processing and analysis tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following table summarizes the differences between data warehouse and data lake.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/11_data_warehouse_vs_data_lake.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Warehouse vs Data Lake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As organizations move data infrastructure to the cloud, the choice of data warehouse vs. data lake, or the need for complex integrations between the two, is less of an issue. 
It is becoming natural for organizations to have both, and move data flexibly from lakes to warehouses to enable business analysis.&lt;/p&gt;

&lt;p&gt;Here is a list of some known cloud-based solutions from different cloud providers:&lt;/p&gt;

&lt;p&gt;Cloud data warehousing solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon Redshift&lt;/em&gt; - a fully-managed, analytical data warehouse that can handle petabyte-scale data, and enable querying it in seconds.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Google BigQuery&lt;/em&gt; - an enterprise-grade cloud-native data warehouse, which runs fast interactive and ad-hoc queries on datasets of petabyte-scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cloud data lake solutions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Amazon S3&lt;/em&gt; - an object storage platform built to store and retrieve any amount of data from any data source, and designed for 99.999999999% durability.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Azure Blob Storage&lt;/em&gt; - stores billions of objects in hot, cool, or archive tiers, depending on how often data is accessed. Data ranges from structured (converted to object form) to any unstructured format - images, videos, audio, documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The data lake architecture was introduced as a solution to some challenges of data warehousing with the rise of big data offering the ability to store and process big data in a cost-effective and scalable manner. 
However, it had its own set of challenges, such as the lack of reliability and transactional consistency and the complex data quality problems making it difficult for GDPR compliance and to use for critical business decisions.&lt;/p&gt;

&lt;p&gt;Can we get the best of both worlds without the complexity of managing both a data lake and a data warehouse or perhaps multiple ones?&lt;/p&gt;

&lt;h4 id=&quot;data-lakehouse&quot;&gt;Data Lakehouse&lt;/h4&gt;

&lt;p&gt;A data Lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. 
Data Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/12_data_warehouse_vs_data_lake_vs_data_lakehouse.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Data Warehouse vs Data Lake vs Data Lakehouse&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Some data management solutions such as &lt;a href=&quot;https://delta.io/&quot;&gt;Delta Lake&lt;/a&gt;, which is an implementation of Data Lakehouse from Databricks, offer the ability to store and process big data in a reliable and consistent manner, while also providing the scalability and cost savings of a data lake.&lt;/p&gt;

&lt;p&gt;The data lakehouse is a relatively new concept and is still evolving, but it has the potential to become an important technology for big data processing and analysis in the future.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article, we went through some popular patterns such as ETL vs ELT and stream vs batch processing that are used in data ingestion and transformation and where they can be applied. 
Then we discussed the different types of data storage and some concrete implementations of them in the cloud and how they fit in an organization based on its requirements, data maturity and purpose.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://martinfowler.com/articles/data-mesh-principles.html&quot;&gt;https://martinfowler.com/articles/data-mesh-principles.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jie-tao.com/delta-lake-step-by-step1/&quot;&gt;https://www.jie-tao.com/delta-lake-step-by-step1/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&quot;&gt;https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://delta.io/&quot;&gt;https://delta.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">This is part 2 of our in-depth article discussing the different stages of data flow inside an organisation.</summary></entry><entry><title type="html">Data engineering 201: In-depth Guide - Part 1</title><link href="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html" rel="alternate" type="text/html" title="Data engineering 201: In-depth Guide - Part 1" /><published>2023-03-11T00:00:00+00:00</published><updated>2023-03-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/11/data-engineering-201</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/11/data-engineering-201.html">&lt;p&gt;&lt;em&gt;In this two parts article we will deep dive into each of the general steps of the data flow.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In a previous &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;blog post&lt;/a&gt;, we saw the importance of data engineering in democratizing data inside an organization and enabling data driven decision making. 
In addition, we outlined the different stages data goes through in order to be able to extract insights out of it.&lt;/p&gt;

&lt;p&gt;Here is a reminder of our data flow blueprint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will start by taking a bird’s eye view, focusing on data sources, data governance and analysis before zooming in on the other stages of the data flow in part 2.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data-sources&quot; id=&quot;markdown-toc-data-sources&quot;&gt;Data Sources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-governance&quot; id=&quot;markdown-toc-data-governance&quot;&gt;Data Governance&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-governance-vs-data-management&quot; id=&quot;markdown-toc-data-governance-vs-data-management&quot;&gt;Data Governance vs Data Management&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#benefits-of-data-governance&quot; id=&quot;markdown-toc-benefits-of-data-governance&quot;&gt;Benefits of Data Governance&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#analysis&quot; id=&quot;markdown-toc-analysis&quot;&gt;Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;data-sources&quot;&gt;Data Sources&lt;/h2&gt;

&lt;p&gt;Data sources refer to the various systems and platforms that generate data and constitute the origin of all data in an organization and play a crucial role in the data engineering and big data landscape. 
These sources can be internal systems such as transactional databases, or external sources such as social media platforms, sensors, or log files.&lt;/p&gt;

&lt;p&gt;The variety and volume of data sources have dramatically increased with the growth of big data, making it important for organizations to have a comprehensive strategy for managing, processing, and analyzing data from multiple sources.&lt;/p&gt;

&lt;p&gt;For instance, data must be transformed and stored in a format that enables efficient processing and analysis. 
There are a variety of file formats, such as Parquet and Avro, that are common to big data use cases and which provide a means of storing and exchanging large volumes of structured and semi-structured data in a standardized way, making it easier for organizations to work with data from multiple sources.&lt;/p&gt;

&lt;p&gt;Following is a table summarizing some properties and how do they compare between some big data file formats:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Properties&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;CSV&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;JSON&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Parquet&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Avro&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Human readability&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Format&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Key-Value&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Columunar&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Row-based&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compressible&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Splittable&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Complex data structure&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Schema evolution&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;No&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Read performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Write performance&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Fast&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Slow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Average&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;CSV should typically be the fastest to write, JSON the easiest to understand for humans, and Parquet the fastest to read a subset of columns, while Avro is the fastest to read all columns at once.&lt;/p&gt;

&lt;h2 id=&quot;data-governance&quot;&gt;Data Governance&lt;/h2&gt;

&lt;p&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It includes the actions people must take, the processes they must follow, and the technology that supports them throughout the data lifecycle to ensure data is secure, private, accurate, available, and usable.&lt;/p&gt;

&lt;p&gt;Some of the key components of the data governance include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data compliance&lt;/strong&gt;: Establishing clear policies, procedures, and standards to govern data collection, storage, and usage. In addition, regularly monitoring and auditing data to ensure compliance with relevant regulations and laws&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data security&lt;/strong&gt;: Implementing security measures to protect data from unauthorized access and breaches.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data privacy&lt;/strong&gt;: Ensuring that personal and confidential data is managed in accordance with relevant regulations and laws.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data Quality&lt;/strong&gt;: Refers to the development and implementation of activities that apply quality management techniques to data to make sure it is suitable to be used in a specific context, thus considered to be high quality data. Data quality is generally judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data cataloging&lt;/strong&gt;: Maintaining a comprehensive and up-to-date inventory of data assets, including metadata, definitions, and relationships.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data lineage&lt;/strong&gt;: Tracing the origin and evolution of data to understand its history and potential impact on decision-making.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-governance-vs-data-management&quot;&gt;Data Governance vs Data Management&lt;/h3&gt;

&lt;p&gt;The scope of Data management is broader than data governance. 
Data management includes all aspects of the full data lifecycle from collection and storage to usage and oversight. 
This is inclusive of data governance, which can be considered as a core component of it, but it also includes other areas such as data architecture and data modeling.&lt;/p&gt;

&lt;h3 id=&quot;benefits-of-data-governance&quot;&gt;Benefits of Data Governance&lt;/h3&gt;

&lt;p&gt;Implementing a data governance framework can increase the value of data within your organization and therefore make better and more timely decisions. 
In fact, effective data governance aims to maintain high quality data that’s both secure and compliant and easily accessible for deeper business insights.&lt;/p&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Visualization&lt;/em&gt; and &lt;em&gt;reporting&lt;/em&gt; play a critical role in helping organizations to make informed decisions. 
By presenting data in an easily understandable format, stakeholders can gain insights into trends, patterns, and relationships that would not be evident from raw data alone. 
Effective visualization and reporting also make it easier for organizations to communicate the results of data analysis to others, including stakeholders, partners, and customers.&lt;/p&gt;

&lt;p&gt;Visualization is the process of creating visual representations of data and information, such as charts, graphs, and maps. 
Data visualization can be done using a wide range of tools including plotting libraries such as matplotlib or seaborn which are used especially by data scientists to more powerful analytics dashboards such as Tableau and Microsoft Power BI.&lt;/p&gt;

&lt;p&gt;Reporting, on the other hand, is the presentation of data and information in a structured format that is designed to be easily consumable by stakeholders. 
Reports are often used to provide an overview of key metrics, trends, and insights, and can be used to support decision making and strategic planning.&lt;/p&gt;

&lt;p&gt;Another aspect of the data analysis we mentioned in &lt;a href=&quot;https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html&quot;&gt;this article&lt;/a&gt; is the use of &lt;em&gt;machine learning algorithms&lt;/em&gt;. 
However, it is not feasible to fully cover the topic of machine learning within the scope of this article. 
Though a good starting point for anyone new to this field is the &lt;a href=&quot;https://www.deeplearning.ai/courses/machine-learning-specialization/&quot;&gt;machine learning specialization course&lt;/a&gt; from Andrew Ng.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we saw the different types of big data file formats and how they compare to each other which can help shape our choice to achieve more efficient data processing. 
In addition, we discussed some of the key components of data governance and its benefit to achieve better data quality and therefore better data analysis through the usage of visualization and reporting.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://devopedia.org/data-serialization&quot;&gt;https://devopedia.org/data-serialization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.google.com/learn/what-is-data-governance&quot;&gt;https://cloud.google.com/learn/what-is-data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/topics/data-governance&quot;&gt;https://www.ibm.com/topics/data-governance&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.heavy.ai/technical-glossary/data-quality&quot;&gt;https://www.heavy.ai/technical-glossary/data-quality&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">In this two parts article we will deep dive into each of the general steps of the data flow.</summary></entry><entry><title type="html">Data engineering 101: Understanding the Fundementals</title><link href="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html" rel="alternate" type="text/html" title="Data engineering 101: Understanding the Fundementals" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/03/01/data-engineering-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html">&lt;p&gt;&lt;em&gt;In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-data-democratization&quot; id=&quot;markdown-toc-what-is-data-democratization&quot;&gt;What is Data Democratization?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-data-engineering&quot; id=&quot;markdown-toc-what-is-data-engineering&quot;&gt;What is Data Engineering?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#role-of-data-engineer&quot; id=&quot;markdown-toc-role-of-data-engineer&quot;&gt;Role of Data Engineer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-engineering-vs-big-data-engineering&quot; id=&quot;markdown-toc-data-engineering-vs-big-data-engineering&quot;&gt;Data Engineering vs Big Data Engineering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-flow&quot; id=&quot;markdown-toc-data-flow&quot;&gt;Data Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;what-is-data-democratization&quot;&gt;What is Data Democratization?&lt;/h2&gt;

&lt;p&gt;Data democratization is the process of making data accessible to all members of an organization, regardless of their role or level of technical expertise. 
It enables organizations to make better and more informed decisions by providing access to data to a wide range of users. 
Data democratization also helps to promote data-driven culture within an organization, as it enables them to explore and leverage their data to create new products and services.&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-engineering&quot;&gt;What is Data Engineering?&lt;/h2&gt;

&lt;p&gt;While data democratization is the foundation for organizations to become data-driven and make better decisions, data engineering plays a vital role in making it happen.&lt;/p&gt;

&lt;p&gt;Data engineering is the process of acquiring, storing, and preparing raw data for analysis to extract valuable insights from their data and make informed business decisions.&lt;/p&gt;

&lt;h2 id=&quot;role-of-data-engineer&quot;&gt;Role of Data Engineer&lt;/h2&gt;

&lt;p&gt;A data engineer is responsible for the “plumbing” that helps derive value from data. 
The specific responsibilities of a data engineer may vary depending on the organization and industry, but generally include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Designing and building data pipelines&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Acquire data from various sources into the appropriate storage and processing systems&lt;/li&gt;
      &lt;li&gt;Coordination of the many jobs through orchestration tools&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data storage and management&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data storage systems&lt;/li&gt;
      &lt;li&gt;Design and implement data models&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data processing&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work with different types of data processing methods such as batch processing and streaming processing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Data governance&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Ensure data is accurate, reliable, and easily accessible to users&lt;/li&gt;
      &lt;li&gt;Establish, implement, and maintain policies and procedures for data quality, compliance, and security&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Manage data infrastructure&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Collaboration and communication&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Work closely with data scientists, analysts, and other stakeholders to understand their data needs and develop solutions that meet their requirements&lt;/li&gt;
      &lt;li&gt;Communicate with other teams such as DevOps, IT and security to ensure that the data infrastructure is aligned with the organization’s overall strategy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-engineering-vs-big-data-engineering&quot;&gt;Data Engineering vs Big Data Engineering&lt;/h2&gt;

&lt;p&gt;According to a study by IDC, the global data sphere is projected to grow from 33 zettabytes (ZB) in 2018 to 175 ZB by 2025, with a compound annual growth rate of 23%. 
It’s also important to note that the vast majority of this data (about 90%) is unstructured data, such as social media posts, images, and videos.
Another study by IBM estimates that 2.5 quintillion bytes of data are created every day, and this amount is increasing exponentially.&lt;/p&gt;

&lt;p&gt;These statistics highlight the sheer volume of data that is being generated on a daily basis and the significant growth in the amount of data generated over time which leads us to the definition of &lt;strong&gt;Big Data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Big data refers to extremely large and complex data sets that are difficult to process and analyze using traditional data processing and management techniques. These data sets are characterized by the “3Vs” - &lt;em&gt;volume, velocity, and variety&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Volume&lt;/em&gt; refers to the sheer amount of data that is generated and collected, this could be in terabytes, petabytes, or even exabytes.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Velocity&lt;/em&gt; refers to the speed at which data is generated and collected. With the growing number of devices and sensors that are connected to the internet, data is being generated at an unprecedented rate.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Variety&lt;/em&gt; refers to the different types of data that are generated and collected, such as structured data, unstructured data, and semi-structured data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So is data engineering always “Big”?&lt;/p&gt;

&lt;p&gt;Data engineering is the broader concept of designing, building, and maintaining systems and infrastructure to support data-driven applications and services, while big data engineering is a specific subfield that is focused on the management and analysis of extremely large and complex data sets that requires the use of advanced processing tools and analytics techniques.&lt;/p&gt;

&lt;p&gt;Overall, data engineering is needed to democratize data inside an organization for informed business decisions, not just when we are dealing with big data.&lt;/p&gt;

&lt;p&gt;After understanding the “What” and the “Why” behind data engineering, it is now time to delve into the “How” of data engineering.&lt;/p&gt;

&lt;h2 id=&quot;data-flow&quot;&gt;Data Flow&lt;/h2&gt;

&lt;p&gt;The data flow outlines the different stages involved in making data easily accessible in a reliable and accurate way for analysis and decision making.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/10_data_flow.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Data Flow Stages&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The general steps include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Data sources are an important and challenging step due to the wide range of potential integration points.&lt;/li&gt;
      &lt;li&gt;Depending on the organization’s needs, some common data sources include:
        &lt;ul&gt;
          &lt;li&gt;Relational and non relational databases&lt;/li&gt;
          &lt;li&gt;Flat files: Data stored in CSV, Excel, or other flat file formats&lt;/li&gt;
          &lt;li&gt;Streaming data: Data generated in real-time, such as data from social media feeds or IoT devices&lt;/li&gt;
          &lt;li&gt;External data: Data from third-party sources, such as government data or data from other companies&lt;/li&gt;
          &lt;li&gt;Legacy systems: Data stored in older systems that need to be migrated to the new data infrastructure&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Governance&lt;/strong&gt;
&lt;br /&gt;Data governance is the systematic management of the availability, usability, integrity and security of the data used in an organization. 
It is not particularly a separate step in the data flow, but rather a broad concept that involves processes, people and technology that support these processes in order to maintain high quality data and make better data driven decisions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data in Motion&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Ingestion&lt;/em&gt; 
&lt;br /&gt;Ingestion is the process of acquiring and importing data from various sources into the organization’s data infrastructure.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; 
&lt;br /&gt;This is the process of cleaning, normalizing, and transforming the data acquired during the ingestion step so that it is in a format that can be used for analysis and reporting.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data at Rest: Storage&lt;/strong&gt; 
&lt;br /&gt;This is the process of storing and managing the data in a way that it can be accessed, queried, and updated easily. Therefore, choosing the right type of storage and technology is a key element not only to the following steps, but also to previous ones due to the fact that we might require multiple storages in different steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Analysis&lt;/strong&gt; 
&lt;br /&gt;This is the step where we extract insights and knowledge from the processed data. This involves:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Visualization and Reporting&lt;/em&gt;
&lt;br /&gt;Creating visual representations of the data to help communicate insights and make it easy to understand&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Machine learning&lt;/em&gt;
&lt;br /&gt;Using algorithms to learn from the data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this article we have seen how valuable it is to democratize data inside an organization to make the shift towards data-driven decision making and the main role played by data engineers in the different steps of the raw data goes through to reach this goal. 
In the following blog post we will dive deeper into each step to uncover best practices, tools and paradigms used and how they have evolved in an ever changing field. Stay tuned!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&quot;&gt;https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&quot;&gt;https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/andkret/Cookbook&quot;&gt;https://github.com/andkret/Cookbook&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Data Engineering" /><summary type="html">In this article we will explore the different aspects of data engineering and what makes it crucial in today’s data-driven world and how it can benefit organizations of all sizes and industries.</summary></entry><entry><title type="html">Prometheus Monitoring: How to Collect and Analyze Metrics</title><link href="https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring.html" rel="alternate" type="text/html" title="Prometheus Monitoring: How to Collect and Analyze Metrics" /><published>2023-01-15T00:00:00+00:00</published><updated>2023-01-15T00:00:00+00:00</updated><id>https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring</id><content type="html" xml:base="https://www.firasesbai.com/articles/2023/01/15/prometheus-monitoring.html">&lt;p&gt;&lt;em&gt;In this article we will deep dive into prometheus, the open-source monitoring and alerting system, and see how we can use it to monitor a simple python application.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All the source code is available &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;So let’s get started!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#prometheus-architecture&quot; id=&quot;markdown-toc-prometheus-architecture&quot;&gt;Prometheus Architecture&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#collecting-and-storing-metrics&quot; id=&quot;markdown-toc-collecting-and-storing-metrics&quot;&gt;Collecting and Storing Metrics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#integrating-with-other-tools&quot; id=&quot;markdown-toc-integrating-with-other-tools&quot;&gt;Integrating with Other Tools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#implementation&quot; id=&quot;markdown-toc-implementation&quot;&gt;Implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#set-up&quot; id=&quot;markdown-toc-set-up&quot;&gt;Set up&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recap&quot; id=&quot;markdown-toc-recap&quot;&gt;Recap&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Prometheus is an open-source monitoring and alerting system. 
It is often used to monitor the performance of various systems and services, and can send alerts if certain thresholds are exceeded. 
Prometheus also has a powerful query language that can be used to analyze the data it collects.&lt;/p&gt;

&lt;p&gt;Prometheus uses a pull model to collect metrics from targets. 
This means that Prometheus actively scrapes metrics from specified targets at regular intervals, rather than waiting for the targets to push metrics to it.&lt;/p&gt;

&lt;h3 id=&quot;prometheus-architecture&quot;&gt;Prometheus Architecture&lt;/h3&gt;

&lt;p&gt;The main components of Prometheus are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prometheus Server&lt;/strong&gt;: This is the core component of Prometheus. It is responsible for scraping metrics from specified targets, storing metrics in its built-in time-series database, and processing and answering queries via PromQL.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Target&lt;/strong&gt;: A target is a system or service that Prometheus scrapes metrics from. Targets typically expose metrics in a specific format over HTTP, which Prometheus can understand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time-Series Database&lt;/strong&gt;: Prometheus stores all the metrics it collects in its built-in time-series database. This allows Prometheus to quickly query and analyze metrics, and to provide historical data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PromQL&lt;/strong&gt;: Prometheus has a powerful query language called PromQL, which can be used to retrieve and analyze metrics stored in its database. PromQL allows you to filter and aggregate metrics, and to create complex queries and alerts.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alertmanager&lt;/strong&gt;: Prometheus has a built-in alerting system called Alertmanager that can be used to trigger alerts based on metrics and send notifications through various channels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exporters&lt;/strong&gt;: Prometheus exporters are third-party tools that convert metrics from various systems and technologies into a format that Prometheus can understand. This allows Prometheus to scrape metrics from a wide variety of sources, such as JMX for Java applications, StatsD, SNMP, and more. For more details, check this blog &lt;a href=&quot;https://www.firasesbai.com/articles/2021/01/01/kafka-monitoring.html&quot;&gt;post&lt;/a&gt; on how to use JMX exporter to monitor an apache kafka cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These components work together in the following way:&lt;/p&gt;

&lt;p&gt;The prometheus server scrapes metrics from specified targets, stores the metrics in its built-in time-series database, and uses PromQL to process and answer queries. 
The Alertmanager is used to trigger alerts based on metrics and send notifications. 
In addition, exporters can be used to convert metrics from various systems and technologies into a format that Prometheus can understand.&lt;/p&gt;

&lt;h2 id=&quot;collecting-and-storing-metrics&quot;&gt;Collecting and Storing Metrics&lt;/h2&gt;

&lt;p&gt;Prometheus periodically scrapes metrics from specified targets, which are typically the applications or services that you want to monitor. 
These targets expose metrics in a specific format, usually over HTTP, that Prometheus can understand.&lt;/p&gt;

&lt;p&gt;Prometheus supports several types of metrics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Counter&lt;/strong&gt;: A cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gauge&lt;/strong&gt;: A metric that represents a single numerical value that can arbitrarily go up and down.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;: A metric that samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: Similar to a histogram, a summary samples observations and provides both the sum of all observed values and the number of observations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Untyped&lt;/strong&gt;: A catch-all metric type that can be used to represent any sort of value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vector&lt;/strong&gt;: A collection of metrics with the same name and type, but with different labels. This allows users to aggregate and select time series based on their labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Prometheus also supports the concept of &lt;strong&gt;labels&lt;/strong&gt;, which are key-value pairs that can be added to metrics to provide more context. 
Labels can be used to group and filter metrics, making it easy to identify trends and patterns in the data.&lt;/p&gt;

&lt;h2 id=&quot;integrating-with-other-tools&quot;&gt;Integrating with Other Tools&lt;/h2&gt;

&lt;p&gt;There are several popular tools that can integrate with Prometheus:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Grafana&lt;/strong&gt;: A popular open-source visualization and dashboarding tool that can be used to create interactive and informative charts and graphs based on Prometheus metrics.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kapacitor&lt;/strong&gt;: A data processing engine that can be used to perform calculations on Prometheus metrics in real-time, such as anomaly detection or aggregations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thanos&lt;/strong&gt;: A set of components that can be used to scale Prometheus and make it highly available, by providing features such as long-term storage and querying across multiple Prometheus instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;In our &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;example&lt;/a&gt;, we created a simple python application using fastapi. 
Our application has two endpoints; one that returns the prometheus metrics and another one that returns a random fact about cats. 
When querying the latter, we also show how to create your own prometheus metrics by measuring the number of times our endpoint was triggered. 
This is an example of a metric of type counter.&lt;/p&gt;

&lt;h3 id=&quot;set-up&quot;&gt;Set up&lt;/h3&gt;

&lt;p&gt;1- Clone this Github &lt;a href=&quot;https://github.com/firasesbai/fastapi-prometheus-monitoring&quot;&gt;repository&lt;/a&gt;, change into the corresponding directory and run the following command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose up -d&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will start two docker containers: one corresponding to the &lt;em&gt;prometheus service&lt;/em&gt; and the other is our python application container called &lt;em&gt;fastapi-app&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;2- Access the prometheus dashboard in your web browser through: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:9090/targets&lt;/code&gt; where you will see our python application as a target for the prometheus service to monitor as shown in the screenshot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_prometheus_interface.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Prometheus Web Interface&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;3- Go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://127.0.0.1:8000/docs&lt;/code&gt; where you will see the Swagger UI of our python application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_fastapi_swagger_ui.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: FastAPI Swagger UI&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As indicated in the screenshot above, our application has two endpoints:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt;: Makes the monitoring metrics available for prometheus to scrape&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/cat_facts&lt;/code&gt;: Returns a random fact about cats.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As mentioned when triggering the last endpoint, we calculate in the background a prometheus metric called: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random_facts_api_execution_counter&lt;/code&gt; which measures the number of times this endpoint was triggered.&lt;/p&gt;

&lt;p&gt;When we request the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/metrics&lt;/code&gt; endpoint, we see a list of metrics measuring several things such as the &lt;em&gt;request time&lt;/em&gt;, &lt;em&gt;python garbage collection objects&lt;/em&gt;, etc…&lt;/p&gt;

&lt;p&gt;These metrics were made available by adding the &lt;strong&gt;prometheus middleware&lt;/strong&gt; to our application. 
  In addition, we can also see our own created metric as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/9_custom_prometheus_metric.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 3: Custom Prometheus Metrics Example&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;That’s it! You have successfully created a custom metric in your python application and you were able to scrape it and make it visible to prometheus.&lt;/p&gt;

&lt;p&gt;With this we have reached the end of this post, I hope you enjoyed it!&lt;/p&gt;

&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;

&lt;p&gt;In this blog post, we discussed Prometheus monitoring, a widely used and powerful monitoring and alerting system. 
We covered topics such as Prometheus architecture, how to collect the different types of metrics and gave an example of popular tools that can integrate with Prometheus. 
In addition, we went through an example of how to deploy a python application in which we created a custom metric and we were able to scrap it and visualize it using prometheus.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Happy learning!&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://prometheus.io/docs/introduction/overview/&quot;&gt;https://prometheus.io/docs/introduction/overview/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://grafana.com/grafana/&quot;&gt;https://grafana.com/grafana/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.influxdata.com/kapacitor/v1.6/working/scraping-and-discovery/&quot;&gt;https://docs.influxdata.com/kapacitor/v1.6/working/scraping-and-discovery/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thanos.io/&quot;&gt;https://thanos.io/&lt;/a&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="articles" /><category term="Observability" /><summary type="html">In this article we will deep dive into prometheus, the open-source monitoring and alerting system, and see how we can use it to monitor a simple python application.</summary></entry><entry><title type="html">The Lean Startup</title><link href="https://www.firasesbai.com/notes/2022/09/11/the-lean-startup.html" rel="alternate" type="text/html" title="The Lean Startup" /><published>2022-09-11T00:00:00+00:00</published><updated>2022-09-11T00:00:00+00:00</updated><id>https://www.firasesbai.com/notes/2022/09/11/the-lean-startup</id><content type="html" xml:base="https://www.firasesbai.com/notes/2022/09/11/the-lean-startup.html">&lt;p&gt;&lt;em&gt;Notes from the Lean Startup Book By Eric Ries&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_the_lean_startup.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 1: Lean Startup Validated Learnings to Achieve Growth&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt; Contents &lt;/h1&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-summary&quot; id=&quot;markdown-toc-1-summary&quot;&gt;1. Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-favorite-quotes&quot; id=&quot;markdown-toc-2-favorite-quotes&quot;&gt;2. Favorite Quotes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-main-takeaways&quot; id=&quot;markdown-toc-3-main-takeaways&quot;&gt;3. Main Takeaways&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#part-1-vision&quot; id=&quot;markdown-toc-part-1-vision&quot;&gt;Part 1: Vision&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#part-2-steer&quot; id=&quot;markdown-toc-part-2-steer&quot;&gt;Part 2: Steer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#part-3-accelerate&quot; id=&quot;markdown-toc-part-3-accelerate&quot;&gt;Part 3: Accelerate&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;1-summary&quot;&gt;1. Summary&lt;/h2&gt;

&lt;p&gt;The Lean startup takes its name from the lean manufacturing revolution that originated in Japan with the Toyota Production System. 
Since not only building cars requires management but also building a startup, the lean startup movement came to existence. 
Entrepreneurs are afraid of implementing traditional management practices early in a startup thinking that this will limit innovation. 
As a consequence, entrepreneurs take a ”just do it” attitude avoiding all forms of management, process and discipline. Unfortunately this approach leads to chaos more often then it does to success.&lt;/p&gt;

&lt;p&gt;Eric Ries describes in this book the why and the how that led to the birth of this movement and the tools that will teach you how to drive a Startup.&lt;/p&gt;

&lt;h2 id=&quot;2-favorite-quotes&quot;&gt;2. Favorite Quotes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Think Big, Start Small.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;An Experiment is a Product.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The lean startup is a framework not a blueprint of steps to follow. It is designed to be adapted to the specific conditions of each specific company.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-main-takeaways&quot;&gt;3. Main Takeaways&lt;/h2&gt;

&lt;p&gt;The Lean Startup has &lt;em&gt;12 Principles&lt;/em&gt; that are categorised into three main parts making up the lifecycle of a Startup.&lt;/p&gt;

&lt;h3 id=&quot;part-1-vision&quot;&gt;Part 1: Vision&lt;/h3&gt;

&lt;p&gt;Explore the importance of learning as the measure of progress for a startup.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 1: Start&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Goal of a startup&lt;/em&gt;: Figure out the right thing to build; the thing customers want and will pay for as quickly as possible.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 2: Define&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;What is a Startup?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;A startup is a human institution(process) designed to create a new product or service(innovation is part of the broad term product to encompass any source of value for the people) under conditions of extreme uncertainty.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;What is an Entrepreneur?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Anyone who is creating a new product or business under conditions of extreme uncertainty is an entrepreneur regardless of the size of the company, the industry or the sector of the economy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 3: Learn&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Validated Learning:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Learning is the essential unit of progress for startups. The effort that is not necessary for learning what customers want can be eliminated.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Productivity in a Startup:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Not in terms of how much stuff we are building but in terms of how much validated learning we’re getting for our efforts.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The audacity of zero:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;The irony is that it is often easier to raise money or acquire other resources when you have zero revenu, zero customers and zero traction than when you have a small amount. 
 Zero invites imagination, but small numbers invite questions about whether large numbers will ever materialise.&lt;/p&gt;

    &lt;p&gt;We can mitigate the waste that happens because of the audacity of zero with &lt;strong&gt;validated learning&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;In the lean startup model, every product, every feature, every marketing campaign –everything a startup does – is understood to be an &lt;strong&gt;experiment&lt;/strong&gt; designed to achieve validated learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 4: Experiment&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A true experiment follows the scientific method. It begins with a clear hypothesis that makes predictions about what is supposed to happen. It then tests those predictions empirically.&lt;/p&gt;

    &lt;p&gt;The goal of every startup’s experiment is to discover how to build a sustainable business around that &lt;strong&gt;vision&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;How?&lt;/em&gt; The second part will tell us that.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;part-2-steer&quot;&gt;Part 2: Steer&lt;/h3&gt;

&lt;p&gt;How vision leads to steering?&lt;/p&gt;

&lt;p&gt;Build-Measure-Learn feedback loop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_learn_build_measure.png&quot; alt=&quot;image&quot; /&gt;
&lt;br /&gt;&lt;em&gt;Figure 2: Learn Build Measure Loop&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It is at the core of the lean startup model.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 5: Leap&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Every business plan begins with a set of assumptions. 
 The riskiest elements of a startup’s plan are called &lt;strong&gt;leap of faith assumptions&lt;/strong&gt;.
 They are called leaps of faith because the success of the entire venture rests on them. If they are true, tremendous opportunity awaits. If they are false, the startup risks total failure.&lt;/p&gt;

    &lt;p&gt;The most important leap-of-faith questions any new startup faces are &lt;strong&gt;value hypothesis&lt;/strong&gt; and &lt;strong&gt;growth hypothesis&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;The value hypothesis tests whether a product or service really delivers value to customers once they are using it.&lt;/p&gt;

    &lt;p&gt;Growth hypothesis tests how new customers will discover a product or service.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Example: why facebook impressed the investors?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;More than half of the users came back to the site every single day -&amp;gt; validated value hypothesis: the customers find the product valuable.&lt;/p&gt;

    &lt;p&gt;By the end of the month of launch 3/4 of harvard’s undergraduates were using it without a dollar of marketing or advertising have been spent -&amp;gt; validated growth hypothesis.&lt;/p&gt;

    &lt;p&gt;We need to confirm that your leap-of-faith questions are based in reality, that the customer has a significant problem worth solving.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;How?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Early contact with customers&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;The goal of such early contact with customers is to understand our potential customers and what problems they have. With that understanding we can craft a &lt;strong&gt;customer archetype&lt;/strong&gt;, a brief document that seeks to humanise the proposed target customer. &lt;br /&gt;
 The point is not to find the average customer but to find early adopters; the customers who feel the need for the product most accurately and experiment with them.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;When?&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Enter the build phase as quickly as possible with a &lt;strong&gt;Concierge Minimum Viable Product&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 6: Test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The goal of the &lt;strong&gt;MVP&lt;/strong&gt; is to begin the process of learning, not to end it. Unlike a prototype or concept test, an MVP is designed not just to answer product design or technical questions. 
 Its goal is to test fundamental business hypotheses.&lt;/p&gt;

    &lt;p&gt;Before new products can be sold successfully to the mass market, they have to be sold to early adopters.&lt;/p&gt;

    &lt;p&gt;MVP ranges in complexity from extremely simple smoke tests (little more than an advertisement) to actual early prototypes complete with problems and missing features.&lt;/p&gt;

    &lt;p&gt;The lesson of the MVP is that any additional work beyond what was required to start learning is waste, no matter how important it might have seemed at the time.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Dropbox had a video as an MVP. It validated the leap-of-faith assumption that customers wanted the product by signing up to the beta waiting list on the website after watching the video.&lt;/p&gt;

    &lt;p&gt;For startups I believe in the following &lt;strong&gt;quality principle&lt;/strong&gt;: If we do not know who the customer is we do not know what quality is. So remove any feature, process or effort that does not contribute directly to the learning you seek.&lt;/p&gt;

    &lt;p&gt;The most commun speed bumps a Startup faces with an MVP are legal issues, fears about competitors (no need, present the competition with your idea and they won’t build it),  branding (different brand) risks and the impact on morale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 7: Measure&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;At the beginning a startup is little more than a business model (projections on how many customers the company expects to attract, how much it will spend, how much revenue and profit that will lead to) on a piece of paper.&lt;/p&gt;

    &lt;p&gt;A startup’s job is to:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Rigorously measure where it is right now&lt;/li&gt;
      &lt;li&gt;Devise experiments to learn how to move the real numbers closer to the ideal reflected in the business plan&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;How to measure progress ?  &lt;strong&gt;Innovation Accounting&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Innovation accounting is a quantitative approach that allows us to see whether our engine tuning efforts are bearing fruit. 
 It also allows us to create learning milestones useful for entrepreneurs as a way of assessing their progress accurately and objectively. 
 They are also invaluable to managers and investors who must hold entrepreneurs accountable.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Use an MVP to establish real data on where the company is right now.&lt;/li&gt;
      &lt;li&gt;Attempt to tune the engine (micro changes and product optimizations) from the baseline toward the ideal.&lt;/li&gt;
      &lt;li&gt;Pivot or Persevere: When a company pivots, it starts the process all over again. The sign of a successful pivot is that these engine-tuning activities are more productive after the pivot then before.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;Innovation accounting will not work if a startup is being misled by &lt;strong&gt;Vanity Metrics&lt;/strong&gt;: gross number of customers and so on. 
 The alternative is the kind of metrics we use to judge our business and our learning milestones; &lt;strong&gt;Actionable Metrics&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;Good metrics:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Actionable
        &lt;ul&gt;
          &lt;li&gt;It must demonstrate clear cause and effect. When cause and effect is clearly understood, people are better able to learn from their actions.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Accessible
        &lt;ul&gt;
          &lt;li&gt;All too many reports are not understood. The easiest way to make reports comprehensible is to use tangible concrete units. What is a website hit? Nobody is really sure but everyone knows what a person visiting the website is.&lt;/li&gt;
          &lt;li&gt;Accessibility also refers to widespread access to the reports. One way is to automatically generate a document containing each experiment and its results explained and mail it to every employee.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Auditable
        &lt;ul&gt;
          &lt;li&gt;We must ensure that the data is credible to employees. How? We need to be able to test the data by hand by talking to customers. Metrics are people too. This is the only way to check if the reports contain true facts.&lt;/li&gt;
          &lt;li&gt;The mechanisms that generate the reports are not too complex.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Solutions?&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Instead of looking at gross metrics, use &lt;strong&gt;cohort-based&lt;/strong&gt; metrics.&lt;/li&gt;
      &lt;li&gt;Instead of looking for cause-and-effect relationships after the fact, we launch each new feature as a true &lt;strong&gt;split-test experiment&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kanban&lt;/strong&gt;; Teams working in this system begin to measure their productivity according to validated learning not in terms of the production of new features.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;ins&gt;Cohort analysis&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;This is one of the most important tools of a startup analytics. 
 Instead of looking at cumulative totals or gross numbers such as total revenue and total number of customers one looks at the performance of each group of customers that comes into contact with the product independently. 
 Each group is called a cohort.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;Split-test experiment&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;This is one in which different versions of a product are offered to customers at the same time. By observing the changes in behavior between the two groups one can make inferences about the impact of the different variations. 
 Although split testing often is thought of as a marketing-specific practice, lean startups incorporate it directly into product development.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;Kanban&lt;/ins&gt;:&lt;/p&gt;

    &lt;p&gt;User stories are not considered complete until they led to validated learning 
 Stories can be in one of 4 states of development:&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th&gt;BACKLOG&lt;/th&gt;
          &lt;th&gt;IN PROGRESS&lt;/th&gt;
          &lt;th&gt;BUILT&lt;/th&gt;
          &lt;th&gt;VALIDATED&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;A&lt;/td&gt;
          &lt;td&gt;D&lt;/td&gt;
          &lt;td&gt;F&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;B&lt;/td&gt;
          &lt;td&gt;E&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
          &lt;td&gt;C&lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
          &lt;td&gt; &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;No bucket can contain more than 3 projects at a time.&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Validated&lt;/em&gt; is defined as knowing whether the story was a good idea to have been done in the first place. 
 This validation usually would come in the form of a split test showing a change in customer behavior but also might include customer interviews or surveys.
 If the validation fails and it turns out the story is a bad idea, the relevant feature is removed from the product.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 8: Pivot (or Persevere)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;A pivot is not just an exhortation to change. It is a special kind of structured change designed to test a new fundamental hypothesis about the product business model and engine of growth.&lt;/p&gt;

    &lt;p&gt;A pivot is better understood as a new strategic hypothesis that will require a new MVP to test.&lt;/p&gt;

    &lt;p&gt;Every startup should have a regular pivot or persevere meeting. Less than a few weeks is too often and more than a few months is too infrequent. Each startup needs to find its own pace.&lt;/p&gt;

    &lt;p&gt;Normally, the &lt;strong&gt;runway&lt;/strong&gt; is defined as the remaining cash in the bank divided by the monthly burn rate or net drain on that account balance. 
 The true measure of runway in a startup is how many pivots the startup has left: the number of opportunities it has to make a fundamental change to its business strategy. 
 The startup has to find a way to achieve the same amount of validated learning at lower cost or in a shorter time.&lt;/p&gt;

    &lt;p&gt;A catalogue of pivots:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Zoom-in pivot&lt;/li&gt;
      &lt;li&gt;Zoom-out pivot&lt;/li&gt;
      &lt;li&gt;Customer segment pivot&lt;/li&gt;
      &lt;li&gt;Customer need pivot&lt;/li&gt;
      &lt;li&gt;Platform pivot&lt;/li&gt;
      &lt;li&gt;Business architecture pivot&lt;/li&gt;
      &lt;li&gt;Value capture pivot&lt;/li&gt;
      &lt;li&gt;Engine of growth pivot&lt;/li&gt;
      &lt;li&gt;Channel pivot&lt;/li&gt;
      &lt;li&gt;Technology pivot &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;part-3-accelerate&quot;&gt;Part 3: Accelerate&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 9: Batch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The small batch approach produces a finished product every few seconds whereas the large batch approach must deliver all the products at once at the end.&lt;/p&gt;

    &lt;p&gt;The biggest advantage of working in small batches is that quality problems can be identified much sooner.&lt;/p&gt;

    &lt;p&gt;What if it turns out that the customer doesn’t want the product we’re building? Working in small batches ensures that a startup can minimize the expenditure of time, money and effort that ultimately turns out to have been wasted.&lt;/p&gt;

    &lt;p&gt;The essential lesson is not that everyone should be shipping fifty times per day but that by reducing batch size we can get through the Build-Measure-Learn feedback loop more quickly than our competitors can. 
 The ability to learn faster from customers is the essential competitive advantage that startups must possess.&lt;/p&gt;

    &lt;p&gt;Lean production solves the problem of stockouts with a technique called &lt;em&gt;Pull&lt;/em&gt;. 
 When companies switch to this kind of production, their warehouses immediately shrink, as the amount of just-in-case inventory (called work-in-progress (WIP) inventory) is reduced dramatically.&lt;/p&gt;

    &lt;p&gt;Some people misunderstand the lean startup model as simply applying pull to customer wants. This assumes that customers could tell us what products to build and that this would act as the pull signal to product developpement to make them. Customers often don’t know what they want.&lt;/p&gt;

    &lt;p&gt;Product development process in a lean startup is responding to pull requests in the form of experiments that need to be run. 
 Thus it is not the customer but rather our &lt;em&gt;hypothesis about the customer&lt;/em&gt; that pulls work from product development and other functions. Any other work is waste.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/articles/14_the_startup_way.png&quot; alt=&quot;image&quot; /&gt;
 &lt;br /&gt;&lt;em&gt;Figure 3: The Startup Way, Image from &lt;a href=&quot;http://www.thestartupway.com/&quot;&gt;theleanstartup&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principal 10: Growth&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The engine of growth is the mechanism that startups use to achieve sustainable growth. 
 &lt;strong&gt;Sustainable growth&lt;/strong&gt; is characterized by one simple rule: new customers come from the actions of past customers.&lt;/p&gt;

    &lt;p&gt;Four primary ways past customers drive sustainable growth:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Word of mouth&lt;/li&gt;
      &lt;li&gt;As a side effect of product usage&lt;/li&gt;
      &lt;li&gt;Through funded advertising&lt;/li&gt;
      &lt;li&gt;Through repeat purchase or use &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;These sources of sustainable growth power the feedback loops called &lt;strong&gt;engines of growth&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;Three engines of growth 
 Engines of growth are designed to give startups a relatively small set of metrics on which to focus their energies.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The sticky engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Attract and retain customers for the long term.&lt;/p&gt;

    &lt;p&gt;Companies using the sticky engine of growth track their &lt;strong&gt;attrition rate&lt;/strong&gt; or churn rate very carefully. The churn rate is defined as the fraction of customers in any period who fail to remain engaged with the company’s product. 
 The rules that govern the sticky engine of growth are pretty simple: if the rate of new customer acquisition exceeds the churn rate, the product will grow. The speed of growth is determined by what is called the &lt;strong&gt;rate of compounding&lt;/strong&gt; which is simply the natural growth rate minus the churn rate. 
 The way to find growth is to focus on existing customers to make the product even more engaging to them.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The viral engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;The viral engine is powered by a feedback loop that can be quantified. It is called the &lt;strong&gt;viral loop&lt;/strong&gt; and its speed is determined by a single mathematical term called the &lt;strong&gt;viral coefficient&lt;/strong&gt;. 
 The viral coefficient measures how many new customers will use a product as a consequence of each new customer who signs up. For a product with a viral coefficient of 0.1, one in every ten customers will recruit one of his or her friends.
 Companies that rely on the viral engine of growth must focus on increasing the viral coefficient more than anything else. 
 A consequence of this is that many viral products do not charge customers directly but rely on indirect sources of revenue such as advertising. This is because viral products cannot afford to have any friction impede the process of signing customers up and recruiting their friends.&lt;/p&gt;

    &lt;p&gt;&lt;ins&gt;The paid engine of growth&lt;ins&gt;&lt;/ins&gt;:&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Each customer pays a certain amount of money for the product over his or her “lifetime” as a customer. Once variable costs are deducted this usually is called the customer lifetime value (LTV). This revenue can be invested in growth by buying advertising. 
 If the company wants to increase its rate of growth it can do so in one of two ways:
 Increase the revenue from each customer or drive down the cost of acquiring a new customer. 
 Suppose an advertisement costs 100$ and causes 50 new customers to sign up for the service. This ad has a cost per acquisition (CPA) of 2$. In this example if the product has an LTV &amp;gt; 2$ the product will grow. The margin between the LTV and CPA determines how fast the paid engine of growth will turn (this is called the marginal profit)&lt;/p&gt;

    &lt;p&gt;I strongly recommend that startups focus on one engine at a time. Only after pursuing one engine thoroughly should a startup consider a pivot to one of the others.&lt;/p&gt;

    &lt;p&gt;Product/market fit = moment when a startup finally finds a widespread set of customers that resonate with its product. 
 Since each engine of growth can be defined quantitatively, each has a unique set of metrics that can be used to evaluate whether a startup is on the verge of achieving product/market fit.&lt;/p&gt;

    &lt;p&gt;Two assumptions that are wrong:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Our startup has failed to achieve product/market fit -&amp;gt; a pivot is a failure event&lt;/li&gt;
      &lt;li&gt;Once our product has achieved product/market fit we won’t have to pivot anymore &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Every engine of growth is tied to a given set of customers and their related habits, preferences, advertising channels and interconnections. 
 At some point that set of customers will be exhausted. This may take a long time or a short time depending on one’s industry and timing.&lt;/p&gt;

    &lt;p&gt;Companies of any size can suffer from the perpetual affliction. 
 They need to manage a portfolio of activities simultaneously tuning their engine of growth and developing new sources of growth for when the engine inevitably runs its course.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 11: Adapt&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The wisdom of the five whys:&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;Repeating why five times can help uncover the root problem and correct it. 
 The core idea of five whys is to tie investments directly to the prevention of the most problematic symptoms. 
 Here is how to use 5 whys analysis to build an adaptive organization: consistently make a proportional investment at each of the five levels of the hierarchy. In other words the investment should be smaller when the symptom is minor and larger when the symptom is more painful. 
 Startup teams should go through the five whys whenever they encounter any kind of failure, including technical faults, failures to achieve business results or unexpected changes in customer behaviour.&lt;/p&gt;

    &lt;p&gt;The curse of the 5 blames
 Instead of using the 5 whys to find and fix problems managers and employees can fall into the trap of using the 5 blames as a means for venting their frustrations and calling out colleagues for systemic failures 
 To escape the 5 blames make sure that everyone affected by the problem is in the room during the analysis of the root cause.&lt;/p&gt;

    &lt;p&gt;Once you are ready to begin, start with a narrowly targeted class of symptoms. It’s better to give the team a chance to learn how to do the process first and then expand into higher-stakes areas later.&lt;/p&gt;

    &lt;p&gt;Appoint a five whys master for each area in which the method is being used. This individual is tasked with being the moderator for each 5 whys meeting making decisions about which prevention steps to take, and assigning the follow up work from the meeting.  The master must be senior enough to have the authority to ensure that those assignments get done but should not be so senior that he or she will not be able to be present at the meetings because of conflicting responsibilities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;Principle 12: Innovate&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Successful innovation teams must be structured correctly in order to succeed.&lt;/p&gt;

    &lt;p&gt;Startup teams require 3 structural attributes:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Scarce but secure resources&lt;/strong&gt;: Startups require much less capital overall, but that capital must be absolutely secure from tampering&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Independent authority to develop their business&lt;/strong&gt;: Startup teams must be completely cross functional. Have full-time representation from every functional department in the company that will be involved in the creation or launch of their early products. They have to be able to build and ship actual functioning products and services, not just prototypes. Approvals slow down the build-measure-learn feedback loop.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Personal stake in the outcome&lt;/strong&gt;: This is usually achieved through stock options or other forms of equity ownership but it doesn’t have to be financial. Make it clear who the innovator is and make sure the innovator receives credit for having brought the new product to life if it is successful. &lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;ins&gt;Creating an innovation sandbox&lt;ins&gt;&lt;/ins&gt;&lt;/ins&gt;&lt;/p&gt;

    &lt;p&gt;Create a sandbox for innovation that will contain the impact of the new innovation but not constrain the methods of the startup team. It works as follows:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;Any team can create a true split-test experiment that affects only the sandboxed parts of the product or service or only certain customer segments or territories&lt;/li&gt;
      &lt;li&gt;One team must see the whole experiment through from end to end -&amp;gt; cross-functional team&lt;/li&gt;
      &lt;li&gt;No experiment can run longer than a specified amount of time&lt;/li&gt;
      &lt;li&gt;No experiment can affect more than a specified number of customers (usually expressed as a percentage of the company’s total mainstream customer base)&lt;/li&gt;
      &lt;li&gt;Every experiment has to be evaluated on the basis of a single standard report of five to 10 no more actionable metrics&lt;/li&gt;
      &lt;li&gt;Every team that works inside the sandbox and every product that is built must use the same metrics to evaluate success&lt;/li&gt;
      &lt;li&gt;Any team that creates an experiment must monitor the metrics and customer reactions while the experiment is in progress and abort it if something catastrophic happens&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading, I hope you enjoyed it!&lt;/em&gt;&lt;/p&gt;</content><author><name>Firas Esbai</name></author><category term="notes" /><category term="General" /><summary type="html">Notes from the Lean Startup Book By Eric Ries</summary></entry></feed>