<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.firasesbai.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.firasesbai.com/" rel="alternate" type="text/html"/><updated>2025-12-07T16:50:32+00:00</updated><id>https://www.firasesbai.com/feed.xml</id><title type="html">Firas Esbai</title><subtitle>The Blog and Portfolio of Firas Esbai. </subtitle><author><name>Firas Esbai</name></author><entry><title type="html">Data Engineering Design Patterns</title><link href="https://www.firasesbai.com/articles/2025/11/13/data-engineering-design-patterns.html" rel="alternate" type="text/html" title="Data Engineering Design Patterns"/><published>2025-11-13T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/11/13/data-engineering-design-patterns</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/11/13/data-engineering-design-patterns.html"><![CDATA[<p><em>In this article, we will look at common data engineering design patterns.</em></p> <p>Design patterns are standard solutions to common problems. They represent best practices and templates that can be reused in multiple situations. In this article, we will explore four common patterns that can be leveraged to ensure reliability, performance and observability in data pipelines.</p> <p><em>So let’s get started!</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#write-audit-publish-wap" id="markdown-toc-write-audit-publish-wap">Write-Audit-Publish (WAP)</a></li> <li><a href="#change-data-capture-cdc" id="markdown-toc-change-data-capture-cdc">Change Data Capture (CDC)</a> <ul> <li><a href="#log-based-cdc" id="markdown-toc-log-based-cdc">Log-based CDC</a></li> <li><a href="#trigger-based-cdc" id="markdown-toc-trigger-based-cdc">Trigger-based CDC</a></li> <li><a href="#timestamp-based-cdc" id="markdown-toc-timestamp-based-cdc">Timestamp-based CDC</a></li> </ul> </li> <li><a href="#dead-letter-queue-dlq" id="markdown-toc-dead-letter-queue-dlq">Dead-Letter Queue (DLQ)</a></li> <li><a href="#cumulative-aggregate-table" id="markdown-toc-cumulative-aggregate-table">Cumulative Aggregate Table</a></li> <li><a href="#recap" id="markdown-toc-recap">Recap</a></li> <li><a href="#resources" id="markdown-toc-resources">Resources</a></li> </ul> <hr/> <h2 id="write-audit-publish-wap">Write-Audit-Publish (WAP)</h2> <p>Weather it is a schema evolution in the upstream data or third party APIs or incorrect join clause in your pipeline, preventing the resulting bad data and ensuring data integrity when writing to production systems requires rigorous validation.</p> <p>The WAP (Write-Audit-Publish) pattern is a critical data strategy that prevents such data quality issues. This method involves writing data to a staging table, auditing it for quality, and only publishing it to production if it passes checks. This ensures that the end users or consumers of this data can trust it.</p> <div class="jekyll-diagrams diagrams mermaid"> <svg id="my-svg" width="100%" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" class="flowchart" style="max-width: 518.008px; background-color: white;" viewBox="0 0 518.0078125 582" role="graphics-document document" aria-roledescription="flowchart-v2"><style>#my-svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px;fill:#333}@keyframes edge-animation-frame{from{stroke-dashoffset:0}}@keyframes dash{to{stroke-dashoffset:0}}#my-svg .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round}#my-svg .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round}#my-svg .error-icon{fill:#522}#my-svg .error-text{fill:#522;stroke:#522}#my-svg .edge-thickness-normal{stroke-width:1px}#my-svg .edge-thickness-thick{stroke-width:3.5px}#my-svg .edge-pattern-solid{stroke-dasharray:0}#my-svg .edge-thickness-invisible{stroke-width:0;fill:none}#my-svg .edge-pattern-dashed{stroke-dasharray:3}#my-svg .edge-pattern-dotted{stroke-dasharray:2}#my-svg .marker{fill:#333;stroke:#333}#my-svg .marker.cross{stroke:#333}#my-svg svg{font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:16px}#my-svg p{margin:0}#my-svg .label{font-family:"trebuchet ms",verdana,arial,sans-serif;color:#333}#my-svg .cluster-label text{fill:#333}#my-svg .cluster-label span{color:#333}#my-svg .cluster-label span p{background-color:transparent}#my-svg .label text,#my-svg span{fill:#333;color:#333}#my-svg .node rect,#my-svg .node circle,#my-svg .node ellipse,#my-svg .node polygon,#my-svg .node path{fill:#ececff;stroke:#9370db;stroke-width:1px}#my-svg .rough-node .label text,#my-svg .node .label text,#my-svg .image-shape .label,#my-svg .icon-shape .label{text-anchor:middle}#my-svg .node .katex path{fill:#000;stroke:#000;stroke-width:1px}#my-svg .rough-node .label,#my-svg .node .label,#my-svg .image-shape .label,#my-svg .icon-shape .label{text-align:center}#my-svg .node.clickable{cursor:pointer}#my-svg .root .anchor path{fill:#333!important;stroke-width:0;stroke:#333}#my-svg .arrowheadPath{fill:#333}#my-svg .edgePath .path{stroke:#333;stroke-width:2px}#my-svg .flowchart-link{stroke:#333;fill:none}#my-svg .edgeLabel{background-color:rgba(232,232,232,0.8);text-align:center}#my-svg .edgeLabel p{background-color:rgba(232,232,232,0.8)}#my-svg .edgeLabel rect{opacity:.5;background-color:rgba(232,232,232,0.8);fill:rgba(232,232,232,0.8)}#my-svg .labelBkg{background-color:rgba(232,232,232,0.5)}#my-svg .cluster rect{fill:#ffffde;stroke:#aa3;stroke-width:1px}#my-svg .cluster text{fill:#333}#my-svg .cluster span{color:#333}#my-svg div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:"trebuchet ms",verdana,arial,sans-serif;font-size:12px;background:hsl(80,100%,96.2745098039%);border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#my-svg .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333}#my-svg rect.text{fill:none;stroke-width:0}#my-svg .icon-shape,#my-svg .image-shape{background-color:rgba(232,232,232,0.8);text-align:center}#my-svg .icon-shape p,#my-svg .image-shape p{background-color:rgba(232,232,232,0.8);padding:2px}#my-svg .icon-shape rect,#my-svg .image-shape rect{opacity:.5;background-color:rgba(232,232,232,0.8);fill:rgba(232,232,232,0.8)}#my-svg .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em}#my-svg .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert}#my-svg :root{--mermaid-font-family:"trebuchet ms",verdana,arial,sans-serif}</style><g><marker id="my-svg_flowchart-v2-pointEnd" class="marker flowchart-v2" viewBox="0 0 10 10" refX="5" refY="5" markerUnits="userSpaceOnUse" markerWidth="8" markerHeight="8" orient="auto"><path d="M 0 0 L 10 5 L 0 10 z" class="arrowMarkerPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/></marker><marker id="my-svg_flowchart-v2-pointStart" class="marker flowchart-v2" viewBox="0 0 10 10" refX="4.5" refY="5" markerUnits="userSpaceOnUse" markerWidth="8" markerHeight="8" orient="auto"><path d="M 0 5 L 10 10 L 10 0 z" class="arrowMarkerPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/></marker><marker id="my-svg_flowchart-v2-circleEnd" class="marker flowchart-v2" viewBox="0 0 10 10" refX="11" refY="5" markerUnits="userSpaceOnUse" markerWidth="11" markerHeight="11" orient="auto"><circle cx="5" cy="5" r="5" class="arrowMarkerPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/></marker><marker id="my-svg_flowchart-v2-circleStart" class="marker flowchart-v2" viewBox="0 0 10 10" refX="-1" refY="5" markerUnits="userSpaceOnUse" markerWidth="11" markerHeight="11" orient="auto"><circle cx="5" cy="5" r="5" class="arrowMarkerPath" style="stroke-width: 1; stroke-dasharray: 1, 0;"/></marker><marker id="my-svg_flowchart-v2-crossEnd" class="marker cross flowchart-v2" viewBox="0 0 11 11" refX="12" refY="5.2" markerUnits="userSpaceOnUse" markerWidth="11" markerHeight="11" orient="auto"><path d="M 1,1 l 9,9 M 10,1 l -9,9" class="arrowMarkerPath" style="stroke-width: 2; stroke-dasharray: 1, 0;"/></marker><marker id="my-svg_flowchart-v2-crossStart" class="marker cross flowchart-v2" viewBox="0 0 11 11" refX="-1" refY="5.2" markerUnits="userSpaceOnUse" markerWidth="11" markerHeight="11" orient="auto"><path d="M 1,1 l 9,9 M 10,1 l -9,9" class="arrowMarkerPath" style="stroke-width: 2; stroke-dasharray: 1, 0;"/></marker><g class="root"><g class="clusters"/><g class="edgePaths"><path d="M259.004,62L259.004,66.167C259.004,70.333,259.004,78.667,259.004,86.333C259.004,94,259.004,101,259.004,104.5L259.004,108" id="L_A_B_0" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" style=";" data-edge="true" data-et="edge" data-id="L_A_B_0" data-points="W3sieCI6MjU5LjAwMzkwNjI1LCJ5Ijo2Mn0seyJ4IjoyNTkuMDAzOTA2MjUsInkiOjg3fSx7IngiOjI1OS4wMDM5MDYyNSwieSI6MTEyfV0=" marker-end="url(#my-svg_flowchart-v2-pointEnd)"/><path d="M259.004,166L259.004,170.167C259.004,174.333,259.004,182.667,259.004,190.333C259.004,198,259.004,205,259.004,208.5L259.004,212" id="L_B_C_0" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" style=";" data-edge="true" data-et="edge" data-id="L_B_C_0" data-points="W3sieCI6MjU5LjAwMzkwNjI1LCJ5IjoxNjZ9LHsieCI6MjU5LjAwMzkwNjI1LCJ5IjoxOTF9LHsieCI6MjU5LjAwMzkwNjI1LCJ5IjoyMTZ9XQ==" marker-end="url(#my-svg_flowchart-v2-pointEnd)"/><path d="M196.91,294L187.091,300.167C177.273,306.333,157.637,318.667,147.818,330.333C138,342,138,353,138,358.5L138,364" id="L_C_D_0" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" style=";" data-edge="true" data-et="edge" data-id="L_C_D_0" data-points="W3sieCI6MTk2LjkwOTc5NjQ2MzgxNTc4LCJ5IjoyOTR9LHsieCI6MTM4LCJ5IjozMzF9LHsieCI6MTM4LCJ5IjozNjh9XQ==" marker-end="url(#my-svg_flowchart-v2-pointEnd)"/><path d="M321.098,294L330.916,300.167C340.735,306.333,360.371,318.667,370.19,332.333C380.008,346,380.008,361,380.008,368.5L380.008,376" id="L_C_F_0" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" style=";" data-edge="true" data-et="edge" data-id="L_C_F_0" data-points="W3sieCI6MzIxLjA5ODAxNjAzNjE4NDIsInkiOjI5NH0seyJ4IjozODAuMDA3ODEyNSwieSI6MzMxfSx7IngiOjM4MC4wMDc4MTI1LCJ5IjozODB9XQ==" marker-end="url(#my-svg_flowchart-v2-pointEnd)"/><path d="M380.008,434L380.008,440.167C380.008,446.333,380.008,458.667,380.008,468.333C380.008,478,380.008,485,380.008,488.5L380.008,492" id="L_F_G_0" class="edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link" style=";" data-edge="true" data-et="edge" data-id="L_F_G_0" data-points="W3sieCI6MzgwLjAwNzgxMjUsInkiOjQzNH0seyJ4IjozODAuMDA3ODEyNSwieSI6NDcxfSx7IngiOjM4MC4wMDc4MTI1LCJ5Ijo0OTZ9XQ==" marker-end="url(#my-svg_flowchart-v2-pointEnd)"/></g><g class="edgeLabels"><g class="edgeLabel"><g class="label" data-id="L_A_B_0" transform="translate(0, 0)"><foreignObject width="0" height="0"><div xmlns="http://www.w3.org/1999/xhtml" class="labelBkg" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel"><g class="label" data-id="L_B_C_0" transform="translate(0, 0)"><foreignObject width="0" height="0"><div xmlns="http://www.w3.org/1999/xhtml" class="labelBkg" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g><g class="edgeLabel" transform="translate(138, 331)"><g class="label" data-id="L_C_D_0" transform="translate(-17.7890625, -12)"><foreignObject width="35.578125" height="24"><div xmlns="http://www.w3.org/1999/xhtml" class="labelBkg" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>Pass</p></span></div></foreignObject></g></g><g class="edgeLabel" transform="translate(380.0078125, 331)"><g class="label" data-id="L_C_F_0" transform="translate(-12.890625, -12)"><foreignObject width="25.78125" height="24"><div xmlns="http://www.w3.org/1999/xhtml" class="labelBkg" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"><p>Fail</p></span></div></foreignObject></g></g><g class="edgeLabel"><g class="label" data-id="L_F_G_0" transform="translate(0, 0)"><foreignObject width="0" height="0"><div xmlns="http://www.w3.org/1999/xhtml" class="labelBkg" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="edgeLabel"></span></div></foreignObject></g></g></g><g class="nodes"><g class="node default" id="flowchart-A-0" transform="translate(259.00390625, 35)"><rect class="basic label-container" style="fill:#e0f7fa !important;stroke:#00bcd4 !important" x="-91.796875" y="-27" width="183.59375" height="54"/><g class="label" style="" transform="translate(-61.796875, -12)"><rect/><foreignObject width="123.59375" height="24"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>New Data Arrives</p></span></div></foreignObject></g></g><g class="node default" id="flowchart-B-1" transform="translate(259.00390625, 139)"><rect class="basic label-container" style="" x="-107.9765625" y="-27" width="215.953125" height="54"/><g class="label" style="" transform="translate(-77.9765625, -12)"><rect/><foreignObject width="155.953125" height="24"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Write to Staging Table</p></span></div></foreignObject></g></g><g class="node default" id="flowchart-C-3" transform="translate(259.00390625, 255)"><rect class="basic label-container" style="" x="-130" y="-39" width="260" height="78"/><g class="label" style="" transform="translate(-100, -24)"><rect/><foreignObject width="200" height="48"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;"><span class="nodeLabel"><p>Perform Data Quality Checks</p></span></div></foreignObject></g></g><g class="node default" id="flowchart-D-5" transform="translate(138, 407)"><rect class="basic label-container" style="fill:#c8e6c9 !important;stroke:#4caf50 !important" x="-130" y="-39" width="260" height="78"/><g class="label" style="" transform="translate(-100, -24)"><rect/><foreignObject width="200" height="48"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;"><span class="nodeLabel"><p>Write Data to Production Table</p></span></div></foreignObject></g></g><g class="node default" id="flowchart-F-7" transform="translate(380.0078125, 407)"><rect class="basic label-container" style="fill:#ffe0b2 !important;stroke:#ff9800 !important" x="-62.0078125" y="-27" width="124.015625" height="54"/><g class="label" style="" transform="translate(-32.0078125, -12)"><rect/><foreignObject width="64.015625" height="24"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;"><span class="nodeLabel"><p>Fire Alert</p></span></div></foreignObject></g></g><g class="node default" id="flowchart-G-9" transform="translate(380.0078125, 535)"><rect class="basic label-container" style="" x="-130" y="-39" width="260" height="78"/><g class="label" style="" transform="translate(-100, -24)"><rect/><foreignObject width="200" height="48"><div xmlns="http://www.w3.org/1999/xhtml" style="display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;"><span class="nodeLabel"><p>Manually Troubleshoot DQ Issue</p></span></div></foreignObject></g></g></g></g></g></svg> </div> <p style="text-align:center;">Figure 1: Write Audit Publish Workflow</p> <p>In order to understand how the WAP pattern can be implemented we will use a key feature of the open table format Apache Iceberg: <a href="https://iceberg.apache.org/docs/1.6.1/branching/">Branches</a>.<br/> For more information on Apache Iceberg and how it compares to the other well known formats, you can check <a href="https://www.firasesbai.com/articles/2024/11/17/evolution-analytical-data-architectures.html">this article</a>.</p> <p>The metadata of an Iceberg table stores a history of snapshots. These snapshots are changes applied to the table and are the basis for reader isolation and time travel. Iceberg branches are another named reference to a snapshot of a table. They can be used for handling GDPR requirements and retaining important historical snapshots for auditing or be part of an ETL pipeline enabling validation of new incoming data.</p> <p>Implementing the 3 steps WAP pattern using Iceberg branches would result in the following:</p> <ol> <li><strong>Write</strong>: switch branch from <strong><em>main</em></strong> to an <strong><em>audit</em></strong> branch and commits updates there. Data is not yet accessible to downstream users who can only access main branch.</li> <li><strong>Audit</strong>: Run data quality checks on the audit branch.</li> <li><strong>Publish</strong>: The <strong><em>main</em></strong> branch can be <code class="language-plaintext highlighter-rouge">fastForward</code> to the head of the audit <strong><em>branch</em></strong> to update the main table state.</li> </ol> <figure> <picture> <source srcset="/assets/images/articles/28_wap_iceberg_audit_branch.webp" type="image/webp"/> <img src="/assets/images/articles/28_wap_iceberg_audit_branch.png" alt="example diagram of audit branch" loading="lazy"/> </picture> <figcaption>Figure 2: Example Diagram of Audit Branch - <a href="https://iceberg.apache.org/docs/1.6.1/branching/#audit-branch">Image Source</a></figcaption> </figure> <p>Using the WAP pattern, downstream pipelines or dashboards can intuitively depend on the production table directly. However, the main challenges is the introduced delays and increased latency by the multistep process which might not be idea for near real time use cases.</p> <h2 id="change-data-capture-cdc">Change Data Capture (CDC)</h2> <p>Change Data Capture (CDC) is a data integration pattern that captures all changes in a source database; creates, updates and deletes and makes them available for downstream systems.</p> <p>Organizations typically have a variety of operational data spread across different systems and applications needed for running the business. The purpose of CDC is to keep this data synchronized by capturing changes from source databases and moving it to your target data warehouse or data lake.</p> <p>In contrast to batch processing, where a pipeline is scheduled periodically to replicate data from one system to another, CDC is considered more efficient. It eliminates the bulk load during specified windows and enables an incremental loading making the data available in near real time. In addition, it reduces the impact on the source database</p> <p>CDC is typically implemented using two main approaches: <strong>push</strong> and <strong>pull</strong>. In the push approach, the source database pushes the updates to the downstream systems. This has the advantage that the target systems have the latest data in near real time. In the pull approach on the other hand, the source database only logs the changes and it is the responsibility of the target systems to continuously poll it. This results in a delay of the data availability. In both approaches, if the source or target systems are not available for some reason, the data is lost. To overcome this, a messaging system can be used in between to buffer these changes.</p> <p>There are 3 common methods for how changes in data are detected using CDC:</p> <ul> <li>Log-based</li> <li>Timestamp-based</li> <li>Trigger-based</li> </ul> <h3 id="log-based-cdc">Log-based CDC</h3> <p>Transactional databases have internal log files where they record all changes committed against the database. These log files are primarily used for backup and disaster recovery purposes. By reading from these transaction logs we can propagate the changes to target systems without adding computational overhead. However, since each vendor’s logs have a different format, this method can’t be easily reusable.</p> <h3 id="trigger-based-cdc">Trigger-based CDC</h3> <p>Another feature that is widely supported by databases is trigger functions. These are stored procedures that are automatically executed once a specific event occurs on a table. We need to create per table one trigger for each operation and store the data changes in a separate table. This adds additional write operations and might impact the performance of the database. Also managing a large number of triggers can become challenging.</p> <h3 id="timestamp-based-cdc">Timestamp-based CDC</h3> <p>This method requires changes to the database schema to include a timestamp column to record when was an entry last updated. Changes are captured by selecting items with a timestamp newer than the previous check timestamp. Being probably the easiest to implement, it fails at capturing delete operations and could add an additional overhead to the database.</p> <h2 id="dead-letter-queue-dlq">Dead-Letter Queue (DLQ)</h2> <p>In streaming environments, a dead-letter queue is a special message queue used to store messages that were not successfully processed by the primary system. These include corrupted messages due to network or system failure or ones that failed the data quality checks.</p> <p>By having a DLQ in place, we provide a mechanism for analyzing and identifying common patterns of errors enabling better observability in order to improve the reliability of the system. In addition, we can reduce the data loss by reprocessing messages that couldn’t be delivered from the DLQ.</p> <h2 id="cumulative-aggregate-table">Cumulative Aggregate Table</h2> <p>The cumulative aggregate table design pattern is very useful when we want to solve the performance issue that comes with calculating aggregates over a large rolling time window.</p> <p>For example, if we want to calculate the number of active users for the last 30-days window, the straightforward solution would be to write a query that scans <em>30 days of raw event data for <strong>all users</strong> every single day</em>. This query is slow and expensive especially if run regularly.</p> <p>This design pattern proposes a solution to overcome this. The idea is to create a new table that stores the pre-computed state often using arrays or structs. Following the active users example, our table called <code class="language-plaintext highlighter-rouge">user_activity_cumulative</code> will have an array column that is updated daily by performing a <code class="language-plaintext highlighter-rouge">FULL OUTER JOIN</code> between today’s active users and yesterday’s cumulative table.</p> <table> <thead> <tr> <th><strong>user_id</strong></th> <th><strong>date</strong></th> <th><strong>activity_array_last_30_days</strong></th> </tr> </thead> <tbody> <tr> <td>123</td> <td>2025-11-12</td> <td><code class="language-plaintext highlighter-rouge">[1, 0, 1, 0, ..., 0]</code></td> </tr> <tr> <td>456</td> <td>2025-11-12</td> <td><code class="language-plaintext highlighter-rouge">[0, 1, 1, 0, ..., 0]</code></td> </tr> <tr> <td><strong>123</strong></td> <td><strong>2025-11-13</strong></td> <td><strong><code class="language-plaintext highlighter-rouge">[1, 1, 0, 1, ..., 0]</code></strong></td> </tr> <tr> <td><strong>456</strong></td> <td><strong>2025-11-13</strong></td> <td><strong><code class="language-plaintext highlighter-rouge">[0, 0, 1, 1, ..., 0]</code></strong></td> </tr> <tr> <td><strong>789</strong></td> <td><strong>2025-11-13</strong></td> <td><strong><code class="language-plaintext highlighter-rouge">[1, 0, 0, 0, ..., 0]</code></strong></td> </tr> </tbody> </table> <p style="text-align:center;">Table 1: Example of user_activity_cumulative Table</p> <p>Using this new table like shown above, finding out the 30-day active users for November 30th can be done by running the following query:</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">      
   <span class="no">SELECT</span>
     <span class="no">COUNT</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>
   <span class="no">FROM</span> <span class="n">user_activity_cumulative</span>
   <span class="no">WHERE</span>
     <span class="n">date</span> <span class="o">=</span> <span class="s1">'2025-11-13'</span>
     <span class="o">--</span> <span class="no">Sum</span> <span class="n">the</span> <span class="mi">30</span> <span class="n">elements</span> <span class="k">in</span> <span class="n">the</span> <span class="n">array</span><span class="p">.</span>
     <span class="nf">-</span><span class="o">-</span> <span class="no">If</span> <span class="n">the</span> <span class="n">sum</span> <span class="n">is</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">they</span> <span class="n">were</span> <span class="n">active</span> <span class="n">at</span> <span class="n">least</span> <span class="n">once</span><span class="o">.</span>
    <span class="no">AND</span> <span class="n">array_sum</span><span class="p">(</span><span class="n">activity_array_last_30_days</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
   
   </code></pre></figure> <p>For more details and concrete hands on example, you can check <a href="https://github.com/DataExpert-io/cumulative-table-design">this repository</a>.</p> <p>With this we have reached the end of this post, I hope you enjoyed it!</p> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p> <h2 id="recap">Recap</h2> <p>In this article, we explored four essential data engineering design patterns that help build reliable and efficient data pipelines. The <strong>Write-Audit-Publish (WAP)</strong> pattern ensures data quality by validating data in a staging area before publishing to production and leveraged Apache Iceberg branches for an example implementation of this pattern. <strong>Change Data Capture (CDC)</strong> enables near real-time data synchronization by capturing incremental changes from source databases using log-based, trigger-based, or timestamp-based methods. <strong>Dead-Letter Queues (DLQ)</strong> provide a safety net for failed messages in streaming systems, improving observability and reducing data loss. Finally, the <strong>Cumulative Aggregate Table</strong> pattern optimizes performance for rolling window calculations by pre-computing and storing state, eliminating the need to scan large amounts of raw data repeatedly.</p> <p><em>Happy learning!</em></p> <h2 id="resources">Resources</h2> <p><a href="https://www.confluent.io/learn/change-data-capture/#why-change-data-capture">https://www.confluent.io/learn/change-data-capture/#why-change-data-capture</a></p> <p><a href="https://aws.amazon.com/blogs/big-data/build-write-audit-publish-pattern-with-apache-iceberg-branching-and-aws-glue-data-quality/">https://aws.amazon.com/blogs/big-data/build-write-audit-publish-pattern-with-apache-iceberg-branching-and-aws-glue-data-quality/</a></p> <p><a href="https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/">https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/</a></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><category term="Data Architecture"/><summary type="html"><![CDATA[Explore essential data engineering design patterns including WAP, CDC, DLQ and cumulative aggregate tables.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/28_wap_iceberg_audit_branch.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/28_wap_iceberg_audit_branch.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Serverless Architecture for Data Engineering</title><link href="https://www.firasesbai.com/articles/2025/10/24/serverless-architecture-for-data-engineering.html" rel="alternate" type="text/html" title="Serverless Architecture for Data Engineering"/><published>2025-10-24T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/10/24/serverless-architecture-for-data-engineering</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/10/24/serverless-architecture-for-data-engineering.html"><![CDATA[<p><em>Understanding serverless architecture in data engineering by example.</em></p> <p>Following the AWS Lambda launch in 2014 and the release of Amazon’s API gateway in 2015, “serverless” grew in popularity to become a new buzzword in the industry. Today, many organizations are considering serverless architectures as a way to save costs and operational overhead in building and running their applications.</p> <p>But what does it really mean to build with serverless, especially in data engineering? That is what we will try to cover in this article through concrete hands on example.</p> <p>All the source code is available <a href="https://github.com/firasesbai/data-engineering-examples/tree/main/nyc-taxi-etl-pipeline">here</a></p> <p><em>So let’s get started!</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#what-is-serverless" id="markdown-toc-what-is-serverless">What is Serverless?</a></li> <li><a href="#simplified-workflow-example" id="markdown-toc-simplified-workflow-example">Simplified Workflow Example</a></li> <li><a href="#serverless-and-software-design" id="markdown-toc-serverless-and-software-design">Serverless and Software Design</a></li> <li><a href="#challenges-and-limitations" id="markdown-toc-challenges-and-limitations">Challenges and Limitations</a></li> <li><a href="#how-to-choose" id="markdown-toc-how-to-choose">How to Choose?</a></li> <li><a href="#resources" id="markdown-toc-resources">Resources</a></li> </ul> <hr/> <h2 id="what-is-serverless">What is Serverless?</h2> <p><a href="https://aws.amazon.com/serverless/">According to AWS</a>, serverless computing is a set of “technologies for running code, managing data, and integrating applications, all without managing servers.”</p> <p>To elaborate, let’s first differentiate serverless offerings from fully-managed services where some server configuration is required. A good example would be <a href="https://aws.amazon.com/rds/aurora/">Amazon Aurora</a> which offers two modes:</p> <ul> <li><strong>Provisioned</strong>: a managed “instance-based” model where the instance size is configured by the developer and the cost model is usually a fixed hourly rate.</li> <li><strong>Serverless</strong>: no servers are configured by the developer. The database scales automatically and the cost model is pay-per-use.</li> </ul> <p>The core principles of a true serverless architecture are <strong>no server management</strong> and a <strong>pay-for-value</strong> cost model.</p> <h2 id="simplified-workflow-example">Simplified Workflow Example</h2> <p>The <a href="https://github.com/firasesbai/data-engineering-examples/tree/main/nyc-taxi-etl-pipeline">repository</a> contains an example of an ETL pipeline for processing NYC Taxi Trip data using AWS services. The goal is to ingest raw CSV data, transform it into a clean, query-ready format, and make it available for analysis. The entire architecture is built on a foundation of serverless AWS services.</p> <p>The entire infrastructure is defined as code using the <a href="https://docs.aws.amazon.com/cdk/v2/guide/home.html">AWS CDK</a> making the deployment of the whole stack straightforward with a single command.</p> <p>The solution implements the following architecture workflow:</p> <ul> <li><strong>Data Ingestion (Amazon S3):</strong> Raw CSV files land in an S3 bucket. S3 acts as our scalable, serverless data lake.</li> <li><strong>Orchestration (Amazon EventBridge &amp; AWS Lambda):</strong> An Amazon EventBridge rule, a serverless cron job, kicks off the pipeline on a schedule (e.g., daily at 4 AM UTC). The rule triggers a Lambda function which in return starts the main ETL job with the correct parameters.</li> <li><strong>ETL Processing (AWS Glue):</strong> The Lambda starts an AWS Glue job, which is the core of our pipeline. Glue is a serverless ETL service that automatically provisions compute resources to run our transformation script, and then shuts them down immediately after.</li> <li><strong>Data Cataloging (AWS Glue Crawler):</strong> After the job completes, a Glue Crawler inspects the processed data (now in optimized Parquet format), infers its schema, and updates the AWS Glue Data Catalog.</li> <li><strong>Querying (Amazon Athena):</strong> With the metadata available in the catalog, anyone can query the processed data in S3 using standard SQL with Athena, a serverless query engine.</li> </ul> <h2 id="serverless-and-software-design">Serverless and Software Design</h2> <p>Can you imagine a technology that lets you build automatically scaling, highly available software while optimizing for costs? This is the serverless proposition. But its real value is strategic, profoundly impacting how we approach software design. Specifically, how can you make any choices about your architecture, if your future use-cases aren’t known?</p> <p>When you don’t know the future, a <a href="https://martinfowler.com/bliki/SacrificialArchitecture.html">sacrificial architecture</a> is invaluable. Martin Fowler defines sacrificial architecture as an architecture designed to be thrown away if the concept proves successful. In our context, cloud environments and especially serverless offerings make sacrificial architecture more attractive in order to build a Minimum Viable Product (MVP). Our taxi pipeline is a perfect MVP. We can deploy it quickly, gather feedback, and if requirements change, we can discard it with minimal sunk cost.</p> <p>Now the success of a system depends on its ability to evolve. Designing a system isn’t a one-off operation. When the product direction, maturity, business requirements and team topologies change, then the success of a system will depend on its ability to evolve along with its demands. According to Martin Fowler’s foreword of the evolutionary architecture book: “The heart of doing evolutionary architecture is to make small changes, and put in feedback loops that allow everyone to learn from how the system is developing.”</p> <p>A conjecture that can help us think about this is the well-known <a href="https://martinfowler.com/bliki/DesignStaminaHypothesis.html">design stamina hypothesis</a>, which stipulates that a system that has not been designed may be easy to develop in the beginning, but after a certain point (the so-called “design pay-off line”), a well-designed system will outperform it.</p> <p>As shown in the diagram below, Serverless enables a “<strong>post-MVP redesign</strong>” strategy. You can start with a simple, serverless-first approach to ship quickly. As you learn and the system matures, you can refactor or redesign parts of the architecture without being locked into the initial design—a perfect example of an evolutionary architecture in practice.</p> <figure> <img src="/assets/images/articles/27_post_mvp_redesign.png" alt="post mvp redesign cumulative functionality"/> <figcaption>Figure 1: Post MVP Redesign Cumulative Functionality</figcaption> </figure> <p>The post-MVP redesign is comparably fast as the no-design approach delivering a lot of functionality and then shifts to the ideal scenario after redesigning to continuously deliver functionality in the long run.</p> <h2 id="challenges-and-limitations">Challenges and Limitations</h2> <p>That being said, it’s important to be aware of the challenges that come with serverless architectures.</p> <ul> <li> <p><strong>Cost Unpredictability:</strong> Unpredictable cost is a key aspect of serverless architecture that can be also a problem, especially for large enterprises operating on annually approved budgets. Cost unpredictability can make stakeholders reluctant to using a product where costs are mostly variable. For a scheduled workload like our ETL, the cost is quite predictable but that is not always the case and this is where active monitoring becomes a key to keep costs in check.</p> </li> <li> <p><strong>Platform Limitations:</strong> Serverless platforms are always evolving. In the past, the <a href="https://aws.amazon.com/glue/">AWS Glue</a> service for example had limitations in terms of runtime choice and memory customization. While many of these have been addressed, it’s a reminder that you are dependent on the provider’s roadmap.</p> </li> <li> <p><strong>Vendor Lock-in:</strong> Building on a specific cloud provider’s serverless offerings can tightly couple your application to their ecosystem, creating a trade-off between development speed and portability.</p> </li> </ul> <h2 id="how-to-choose">How to Choose?</h2> <p>So, should you go serverless? Consider these questions:</p> <ul> <li>Are you in a greenfield project where you need to iterate quickly and flexibly while building an MVP?</li> <li>Do you have a shortage of infrastructure developers in your team? Serverless empowers application developers to manage their own services.</li> </ul> <p>If you answered yes to any of these, a serverless architecture is a powerful option. It’s not just a technical choice but a strategic one that prioritizes speed, agility, and the ability to evolve with changing business requirements.</p> <p>With this we have reached the end of this post, I hope you enjoyed it!</p> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p> <h2 id="resources">Resources</h2> <p><a href="https://www.datadoghq.com/state-of-serverless/">https://www.datadoghq.com/state-of-serverless/</a></p> <p><a href="https://martinfowler.com/articles/serverless.html">https://martinfowler.com/articles/serverless.html</a></p> <p><a href="https://www.thoughtworks.com/radar/techniques/serverless-architecture">https://www.thoughtworks.com/radar/techniques/serverless-architecture</a></p> <p><a href="https://www.thoughtworks.com/en-gb/radar/techniques/lambda-pinball">https://www.thoughtworks.com/en-gb/radar/techniques/lambda-pinball</a></p> <p><a href="https://www.freecodecamp.org/news/serverless-fully-managed-service-difference/">https://www.freecodecamp.org/news/serverless-fully-managed-service-difference/</a></p> <p><a href="https://blogs.perficient.com/2021/06/17/aws-cost-analysis-comparing-lambda-ec2-fargate/">https://blogs.perficient.com/2021/06/17/aws-cost-analysis-comparing-lambda-ec2-fargate/</a></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><category term="Data Architecture"/><category term="Cloud Computing"/><summary type="html"><![CDATA[understanding serverless architecture in data engineering by building etl pipeline for nyc taxi trip data.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/27_post_mvp_redesign.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/27_post_mvp_redesign.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">101 Apache Spark Cheatsheet</title><link href="https://www.firasesbai.com/articles/2025/10/10/apache-spark-101.html" rel="alternate" type="text/html" title="101 Apache Spark Cheatsheet"/><published>2025-10-10T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/10/10/apache-spark-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/10/10/apache-spark-101.html"><![CDATA[<p><em>101 Apache Spark Cheatsheet</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#what-is-apache-spark" id="markdown-toc-what-is-apache-spark">What is Apache Spark</a></li> <li><a href="#key-features-of-spark" id="markdown-toc-key-features-of-spark">Key features of Spark</a></li> <li><a href="#resilient-distributed-datasets" id="markdown-toc-resilient-distributed-datasets">Resilient Distributed Datasets</a> <ul> <li><a href="#transformations" id="markdown-toc-transformations">Transformations</a></li> <li><a href="#actions" id="markdown-toc-actions">Actions</a></li> <li><a href="#rdd-vs-dataframe" id="markdown-toc-rdd-vs-dataframe">RDD vs DataFrame</a></li> </ul> </li> <li><a href="#fault-tolerance" id="markdown-toc-fault-tolerance">Fault Tolerance</a></li> <li><a href="#spark-architecture" id="markdown-toc-spark-architecture">Spark Architecture</a></li> <li><a href="#spark-components" id="markdown-toc-spark-components">Spark Components</a></li> <li><a href="#lazy-evaluation" id="markdown-toc-lazy-evaluation">Lazy Evaluation</a></li> <li><a href="#dag-in-spark" id="markdown-toc-dag-in-spark">DAG in Spark</a></li> <li><a href="#caching" id="markdown-toc-caching">Caching</a></li> <li><a href="#broadcast-variables" id="markdown-toc-broadcast-variables">Broadcast Variables</a></li> <li><a href="#partitioning" id="markdown-toc-partitioning">Partitioning</a> <ul> <li><a href="#storage-partitioning-vs-spark-partitioning" id="markdown-toc-storage-partitioning-vs-spark-partitioning">Storage Partitioning vs Spark Partitioning</a></li> </ul> </li> <li><a href="#spark-applications-optimization" id="markdown-toc-spark-applications-optimization">Spark Applications Optimization</a></li> </ul> <hr/> <h2 id="what-is-apache-spark">What is Apache Spark</h2> <p>Apache Spark is an open-source, distributed computing system designed for fast and general-purpose data processing. It was developed to address the limitations of Hadoop MapReduce, offering significant performance improvements and a more flexible programming model.</p> <h2 id="key-features-of-spark">Key features of Spark</h2> <ul> <li><strong>Speed</strong>: Spark can be up to 100 times faster than Hadoop MapReduce for certain workloads, primarily due to its in-memory processing capabilities.</li> <li><strong>Ease of Use</strong>: Spark provides high-level APIs in Java, Scala, Python, and R, making it accessible to a wide range of developers and data scientists.</li> <li><strong>Unified Engine</strong>: Spark can handle diverse workloads including batch processing, interactive queries, streaming, machine learning, and graph processing, all within the same engine.</li> <li><strong>Fault Tolerance</strong>: Spark achieves fault tolerance through the use of Resilient Distributed Datasets (RDDs) and their lineage information.</li> <li><strong>Scalability</strong>: Spark can scale from one to thousands of nodes, allowing for efficient processing of large datasets.</li> <li><strong>Flexibility</strong>: Spark can run in various environments, including Hadoop, Mesos, standalone, or in the cloud and it can access diverse data sources.</li> </ul> <h2 id="resilient-distributed-datasets">Resilient Distributed Datasets</h2> <p>Resilient Distributed Datasets (RDD) is the fundamental data structure in Apache Spark.</p> <ul> <li><strong>Resilient</strong>: RDDs are fault-tolerant. If a partition of an RDD is lost due to node failure, it can be reconstructed using the lineage information.</li> <li><strong>Distributed</strong>: Data in RDDs is divided into partitions and distributed across nodes in a cluster.</li> <li><strong>Immutable</strong>: Once created, RDDs cannot be modified. Any transformation on an RDD creates a new RDD.</li> <li><strong>Lazy Evaluation</strong>: Transformations on RDDs are lazily evaluated. They are not computed until an action is called.</li> <li><strong>In-memory Computation</strong>: RDDs can be cached in memory for faster access in iterative algorithms.</li> </ul> <p>RDDs support two types of operations: <strong><em>Transformations</em></strong> and <strong><em>Actions</em></strong>.</p> <h3 id="transformations">Transformations</h3> <p>These are operations that create a new RDD from an existing one and they are lazy (not computed immediately).</p> <p>Examples of transformations:</p> <ul> <li><code class="language-plaintext highlighter-rouge">map(func)</code>: Apply a function to each element in the RDD. The result is an RDD with the same number of elements as the original.</li> <li><code class="language-plaintext highlighter-rouge">filter(func)</code>: Return a new RDD containing only the elements that pass the filter condition.</li> <li><code class="language-plaintext highlighter-rouge">flatMap(func)</code>: Similar to map, but each input item can be mapped to 0 or more output items. The results are flattened into a single RDD.</li> <li><code class="language-plaintext highlighter-rouge">groupByKey()</code>: Group the values for each key in the RDD.</li> <li><code class="language-plaintext highlighter-rouge">reduceByKey(func)</code>: Combine values with the same key using the provided function.</li> </ul> <p>Example of <code class="language-plaintext highlighter-rouge">map</code> vs <code class="language-plaintext highlighter-rouge">flatMap</code> usage:</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">      
    <span class="n">rdd</span> <span class="o">=</span>  <span class="n">sc</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">([</span><span class="s2">"Hello World"</span><span class="p">,</span> <span class="s2">"How are you"</span><span class="p">])</span>
    
    <span class="n">map_result</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="nb">lambda</span> <span class="ss">x: </span><span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span>
    <span class="c1"># Result: [["Hello", "World"], ["How", "are", "you"]]</span>

    <span class="n">flatmap_result</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="nb">lambda</span> <span class="ss">x: </span><span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span>
    <span class="c1"># Result: ["Hello", "World", "How", "are", "you"]</span>
   
   </code></pre></figure> <h3 id="actions">Actions</h3> <p>These are operations that return a result to the driver program or write data to an external storage system. They trigger the execution of all the transformations that were called before it.</p> <p>Examples of actions:</p> <ul> <li><code class="language-plaintext highlighter-rouge">collect()</code>: Return all the elements of the RDD as an array to the driver program.</li> <li><code class="language-plaintext highlighter-rouge">count()</code>: Return the number of elements in the RDD.</li> <li><code class="language-plaintext highlighter-rouge">first()</code>: Return the first element of the RDD.</li> <li><code class="language-plaintext highlighter-rouge">take(n)</code>: Return an array with the first n elements of the RDD.</li> <li><code class="language-plaintext highlighter-rouge">reduce(func)</code>: Aggregate the elements of the RDD using a function.</li> <li><code class="language-plaintext highlighter-rouge">saveAsTextFile(path)</code>: Save the elements of the RDD as a text file.</li> </ul> <h3 id="rdd-vs-dataframe">RDD vs DataFrame</h3> <p>A Spark DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database or a data frame in R/Python. It’s built on top of RDDs and provides a more user-friendly API for structured and semi-structured data. It uses Catalyst Optimizer, which can significantly improve performance and has a defined Schema, allowing Spark to optimize query plans.</p> <h2 id="fault-tolerance">Fault Tolerance</h2> <p>Spark achieves fault tolerance primarily through the lineage of RDDs and the ability to recompute lost data.</p> <ul> <li><strong>RDD Lineage</strong>: <ul> <li>Each RDD maintains information about its lineage (how it was derived from other datasets).</li> <li>If a partition of an RDD is lost, Spark can rebuild it using this lineage information.</li> </ul> </li> <li><strong>Checkpointing</strong>: <ul> <li>For long lineage chains, Spark allows saving intermediate results to reliable storage (like HDFS).</li> <li>This reduces recovery time in case of failures.</li> </ul> </li> <li><strong>Speculative Execution</strong>: <ul> <li>Spark can run multiple copies of slower tasks to reduce the impact of stragglers.</li> </ul> </li> <li><strong>Stage Retry</strong>: <ul> <li>If a task fails, Spark will retry it on a different executor.</li> <li>If a whole stage fails, Spark can resubmit the entire stage.</li> </ul> </li> <li><strong>Data Replication</strong>: <ul> <li>When caching data, Spark can replicate it across nodes for added resilience.</li> </ul> </li> <li><strong>Driver and Worker Fault Tolerance</strong>: <ul> <li>Spark can recover from worker node failures.</li> <li>For driver failures, Spark supports checkpointing of the driver’s state in some deployment modes.</li> </ul> </li> </ul> <h2 id="spark-architecture">Spark Architecture</h2> <figure> <img src="/assets/images/articles/26_spark_architecture.png" alt="Spark Architecture"/> <figcaption>Figure 1: Spark Architecture - <a href="https://spark.apache.org/docs/latest/cluster-overview.html">Image Source</a></figcaption> </figure> <ul> <li><strong>Driver Program</strong>: Contains the main() function and creates a SparkContext.</li> <li><strong>SparkContext</strong>: The entry point for Spark functionality, representing the connection to the Spark cluster.</li> <li><strong>Cluster Manager</strong>: Allocates resources across applications.</li> <li><strong>Worker Nodes (Slaves)</strong>: <ul> <li>Execute the tasks assigned by the driver.</li> <li>Store data partitions.</li> </ul> </li> <li><strong>Executor</strong> <ul> <li>A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them.</li> <li>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.</li> </ul> </li> <li><strong>Task</strong>: A unit of work that will be sent to one executor.</li> <li><strong>Job</strong>: A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action.</li> <li><strong>Stage</strong>: Each job gets divided into smaller sets of tasks called stages that depend on each other.</li> </ul> <h2 id="spark-components">Spark Components</h2> <ul> <li><strong>Spark Core</strong>: The foundation of the entire Spark system, providing distributed task dispatching, scheduling, and basic I/O functionalities.</li> <li><strong>Spark SQL</strong>: Module for working with structured data. It allows querying data via SQL as well as the Hive Query Language (HQL) and supports various sources like Hive tables, Parquet, and JSON.</li> <li><strong>Spark Streaming</strong>: Enables processing of live streams of data. It provides a high-level abstraction called DStream (discretized stream).</li> <li><strong>MLlib (Machine Learning Library)</strong>: A distributed machine learning framework on top of Spark Core. It provides various utilities for machine learning tasks, including classification, regression, clustering, and collaborative filtering.</li> <li><strong>GraphX</strong>: A distributed graph-processing framework on top of Spark. It provides an API for expressing graph computation and can model user-defined graphs.</li> <li><strong>SparkR</strong>: An R package that provides a light-weight frontend to use Spark from R.</li> </ul> <h2 id="lazy-evaluation">Lazy Evaluation</h2> <p>Lazy evaluation is a key optimization technique used in Spark. When you apply a transformation on an RDD, Spark doesn’t compute the results immediately. Instead, it remembers the set of transformations applied to some base dataset. The transformations are only computed when an action requires a result to be returned to the driver program.</p> <p>Importance of lazy evaluation:</p> <ul> <li><strong>Optimization</strong>: Spark can optimize the execution plan by analyzing the full set of transformations before executing.</li> <li><strong>Efficiency</strong>: It reduces the number of passes over the data by grouping operations.</li> <li><strong>Reduced Computation</strong>: If the final action only needs to compute a small result, Spark can minimize the amount of data processed.</li> <li><strong>Fault Tolerance</strong>: Lazy evaluation allows Spark to reconstruct lost data by recomputing only the lost partitions from the original data.</li> </ul> <h2 id="dag-in-spark">DAG in Spark</h2> <p>In Spark, a Directed Acyclic Graph (DAG) is a conceptual model of the execution plan for a set of operations on RDDs.</p> <p>A DAG is a graph where each node represents an RDD partition, and the edges represent the operations to be performed on the RDD. The edges have a direction, indicating the flow of data from one operation to the next. There are no cycles in the graph, meaning the operations flow in one direction and don’t loop back.</p> <p>Spark’s DAG scheduler optimizes the execution plan by analyzing the graph and combining operations where possible. The DAG is divided into stages. A stage is a set of tasks that can be executed together without shuffling data. If a node fails, Spark can reconstruct the lost partitions using the lineage information stored in the DAG.</p> <p>The DAG is constructed when actions are called, not when transformations are defined.</p> <h2 id="caching">Caching</h2> <p>Caching in Spark is a technique used to store the intermediate results of RDD computations in memory or disk. This allows faster access when the same RDD is used multiple times.</p> <ul> <li><strong>Methods</strong>: <ul> <li><code class="language-plaintext highlighter-rouge">cache()</code>: Stores the RDD in memory.</li> <li><code class="language-plaintext highlighter-rouge">persist()</code>: Allows specifying the storage level (memory, disk, or both).</li> </ul> </li> <li><strong>Storage Levels</strong>: Spark provides different storage levels like MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc.</li> <li><strong>Lazy Evaluation</strong>: Caching is also lazily evaluated. The RDD isn’t cached until the first action that uses it.</li> <li><strong>Automatic Memory Management</strong>: Spark automatically manages the cached data using LRU (Least Recently Used) eviction policy.</li> <li><strong>Unpersist</strong>: You can manually remove cached data using the unpersist() method.</li> </ul> <h2 id="broadcast-variables">Broadcast Variables</h2> <p>Broadcast variables in Spark are read-only variables that are cached on each machine in the cluster rather than shipped with every task. They are used to efficiently share large read-only data across all nodes in a cluster. They are commonly used for lookup tables, machine learning models, or any large read-only data structure.</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby">      
    <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">broadcast</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span> <span class="s2">"A"</span><span class="p">,</span> <span class="mi">2</span><span class="ss">:"B"</span><span class="p">,</span> <span class="mi">3</span><span class="ss">:"C"</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">lookup_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">lookup_table</span><span class="p">.</span><span class="nf">value</span><span class="p">.</span><span class="nf">get</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">"Unknown"</span><span class="p">)</span>
    
    <span class="n">result</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">lookup_function</span><span class="p">)</span>
   
   </code></pre></figure> <h2 id="partitioning">Partitioning</h2> <p>Spark automatically partitions RDDs based on the input data source or the number of cores available. There are three types of partitioning:</p> <ul> <li><em>Hash Partitioning</em>: Based on the hash code of the key.</li> <li><em>Range Partitioning</em>: Based on ordered keys.</li> <li><em>Custom Partitioning</em>: User-defined partitioning logic using methods like <code class="language-plaintext highlighter-rouge">repartition()</code> or <code class="language-plaintext highlighter-rouge">coalesce()</code>.</li> </ul> <p>Partitioning is important because:</p> <ul> <li>It affects the level of parallelism in data processing</li> <li>It influences the amount of data transfer during shuffles</li> <li>It impacts the efficiency of certain operations (e.g., joins, aggregations)</li> </ul> <h3 id="storage-partitioning-vs-spark-partitioning">Storage Partitioning vs Spark Partitioning</h3> <p>These are two separate concepts, but understanding the difference is relevant as storage partitioning can impact Spark’s performance.</p> <p>While storage partitioning refers to how data is organized on disk (e.g. in S3 or HDFS), Spark partitioning is about how data is distributed across executors for processing.</p> <p>Storage partitioning allows partition pruning; Spark can skip reading irrelevant partitions based on query filters which leads to faster query processing times by reducing data scanned.</p> <p>In order to read partitioned data, Spark performs partition discovery to identify partitions based on directory structure. It can also leverage partition information for optimizations like partition pruning. However, Spark still creates its own internal partitions for processing, which may differ from storage partitions.</p> <h2 id="spark-applications-optimization">Spark Applications Optimization</h2> <ul> <li><strong>Data Serialization</strong>: Use Kryo serialization instead of Java serialization for better performance.</li> <li><strong>Proper Data Partitioning</strong>: Ensure data is well-distributed across partitions to avoid skew.</li> <li><strong>Caching and Persistence</strong>: Use <code class="language-plaintext highlighter-rouge">cache()</code> or <code class="language-plaintext highlighter-rouge">persist()</code> for RDDs used multiple times.</li> <li><strong>Avoid Shuffling</strong>: Minimize operations that cause data shuffling (e.g., groupByKey, reduceByKey).</li> <li><strong>Use Broadcast Variables</strong>: For large shared data that needs to be distributed to all nodes.</li> <li><strong>Optimize Data Formats</strong>: Use columnar formats like Parquet for better compression and query performance.</li> <li><strong>Tune Spark Configurations</strong>: Adjust executor memory, number of executors, and other Spark parameters.</li> <li><strong>Use Appropriate Join Strategies</strong>: Choose the right join strategy (broadcast joins for small-large table joins).</li> <li><strong>Avoid UDFs When Possible</strong>: Use built-in functions instead of User Defined Functions for better performance.</li> <li><strong>Monitor and Profile</strong>: Use Spark UI and other profiling tools to identify bottlenecks.</li> </ul> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Apache Spark is an open-source, distributed computing system designed for fast and general-purpose data processing.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/26_spark_architecture.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/26_spark_architecture.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Learnings from Vibe Coding</title><link href="https://www.firasesbai.com/articles/2025/09/25/vibe-coding-learnings.html" rel="alternate" type="text/html" title="Learnings from Vibe Coding"/><published>2025-09-25T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/09/25/vibe-coding-learnings</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/09/25/vibe-coding-learnings.html"><![CDATA[<p><em>In this article, I share some lessons learned from trying out vibe coding.</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#what-is-vibe-coding" id="markdown-toc-what-is-vibe-coding">What is Vibe Coding?</a></li> <li><a href="#expectations" id="markdown-toc-expectations">Expectations</a></li> <li><a href="#key-takeaways" id="markdown-toc-key-takeaways">Key Takeaways</a></li> <li><a href="#recap" id="markdown-toc-recap">Recap</a></li> </ul> <hr/> <h2 id="what-is-vibe-coding">What is Vibe Coding?</h2> <p>Vibe coding is a term coined by Andrej Karpathy in his X post that was shared on February 2, 2025. However, the term has definitely evolved in my opinion since the original description. It is indeed a new kind of coding where the developer has become more like a moderator of multiple chat sessions and agents running in the background resulting in a handful of new and updated files with new generated code by the large language model that are waiting for his review.</p> <figure> <img src="/assets/images/articles/25_vibe_coding_andrej_karpathy_post.png" alt="Andrej Karpathy X Post"/> <figcaption>Figure 1: Andrej Karpathy's Post - <a href="https://x.com/karpathy/status/1886192184808149383?lang=en">Image Source</a></figcaption> </figure> <h2 id="expectations">Expectations</h2> <p>This is not gonna be one of those <em>I built a SaaS app in 15 minutes</em>. It is easy and at times surprisingly good how fast you can generate a landing page for an idea or have a decent CRUD app running but things get more complicated when building complex SaaS.</p> <figure> <img src="/assets/images/articles/25_vibe_coding_leo_post.png" alt="Leo's posts about Vibe Coding"/> <figcaption>Figure 2: Leo's posts about Vibe Coding - <a href="https://x.com/leojr94_?lang=en">Image Source</a></figcaption> </figure> <p>In my experiment, I wanted to answer a simple question: <em>can AI assisted development make me move faster through the backlog?</em></p> <p>Obviously faster does not always mean better and the metric for measuring the outcome should not be the number of lines of code generated because if anything you won’t be disappointed: LLMs can truly quickly generate a lot of code!</p> <p>As this was a personal side project, the stakes were not that high but the role of the moderator as mentioned earlier is still of huge importance. As you can see from the diagram below, I dealt with times where the model generated thousands of lines of code but almost always not all of it was accepted. You have to be a gatekeeper protecting your codebase from bugs or introducing weird behaviour and making sure to always follow clear design patterns and software development best practices.</p> <figure> <img src="/assets/images/articles/25_total_line_changes.png" alt="total line changes from chat"/> <figcaption>Figure 3: Total Line Changes from Chat</figcaption> </figure> <p>So over the course of two weeks, I used <strong>Cursor</strong> to try to answer my question. I did not start from scratch and vibe coded my way into this project but rather build on top of an existing web application and extended it with new features. That means I already had a clear code structure with clearly defined interfaces and domain models that have greatly influenced the LLM’s code organization and structure.</p> <p>When it comes to the features, some of them were straightforward but many weren’t. These were a collection of cards with merely a title in a Trello board that came from ideas I thought were cool to implement someday so I just wrote them down there quickly to not lose track of them. This point is important as we’ll see later because it influences how you approach building these features and how to draft your prompts for that.</p> <p>A final thing to mention before moving on to the key takeaways is the distribution of the used programming languages in the project:</p> <figure> <img src="/assets/images/articles/25_programming_language_usage.png" alt="programming language usage"/> <figcaption>Figure 4: Programming Language Usage</figcaption> </figure> <p>I’m not a frontend developer and my Javascript skills at this point were a bit rusty to say the least but surprisingly this is the part where I made most of the progress that I couldn’t have done without the AI assistance in such a short time.</p> <h2 id="key-takeaways">Key Takeaways</h2> <p>Following are the observations and notes I took as I progressed in this experiment:</p> <ul> <li>Use user journeys and expected behaviour in your prompt when explaining a feature especially one that would require changes across both frontend and backend logic.</li> <li>Be specific and start with thin slices. Describing multiple features and expectations in the same prompt just because they are correlated will not result in better results but rather only confuse the model.</li> <li>Request analysis of the code structure and design by giving the whole codebase as context to the mode. This helps you reflect on the progress made so far, assess the list of features implemented and decide what to do next because it is easy to have decision paralysis in this honeymoon phase where you are in love with how productive you are and everything seems feasible that you just want to do it all at once.</li> <li>Asking the model to simplify any implementation is always good: rely on your judgment of assessing what looks good and what looks meh.</li> <li>If the model gets stuck with a particular implementation or request, expect to have duplicated code snippets and functions with similar logic but slightly different names resulting from multiple attempts at solving the issue. This means that the model was really bad at cleaning up dead code unless you specifically ask it to evaluate some code by highlighting it.</li> <li>After several interactions within the same chat, it is better to continue or start over in a new one: It helps you clear your thoughts with a fresh new prompt and therefore guide the model to better output.</li> <li>I can’t stress this enough but if you are not starting from scratch and you will be updating an existing codebase, have your unit tests ready as your defense mechanism.</li> <li>From time to time, it is useful to start your prompt by specifically asking the model not to make code changes. Ask it to give you multiple implementation options and review the suggestions before proceeding with any of them. If needed drill down on the one you found most appealing with follow up questions and clarifications.</li> <li>Commit changes frequently especially between requests when you are satisfied with the suggested changes. This way it is easier to review follow up code changes in particular those made to the same files.</li> <li>Review the whole codebase once you have committed new changes to keep your understanding of it in check and be able to track down potential issues or improvements.</li> <li>Some changes are really small and you can just make them in place using the tab functionality:</li> </ul> <figure> <img src="/assets/images/articles/25_total_tabs_accepted.png" alt="total tabs accepted"/> <figcaption>Figure 5: Total Tabs Accepted</figcaption> </figure> <p>With this we have reached the end of this post, I hope you enjoyed it!</p> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <h2 id="recap">Recap</h2> <p>Coming back to the original quest of this whole experiment: did AI supported me in doing things faster? for sure it did. Like any other tool when used properly and consciously it will help augment your abilities to do your work better and support you in many ways. Will programmers be replaced by AI? I’ll take that with a pinch of salt.</p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="AI Engineering"/><summary type="html"><![CDATA[Lessons from trying out vibe coding using cursor]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/25_total_line_changes.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/25_total_line_changes.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">101 Apache Kafka Cheatsheet</title><link href="https://www.firasesbai.com/articles/2025/09/15/apache-kafka-101.html" rel="alternate" type="text/html" title="101 Apache Kafka Cheatsheet"/><published>2025-09-15T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/09/15/apache-kafka-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/09/15/apache-kafka-101.html"><![CDATA[<p><em>101 Apache Kafka Cheatsheet</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#what-is-apache-kafka" id="markdown-toc-what-is-apache-kafka">What is Apache Kafka?</a></li> <li><a href="#core-concepts" id="markdown-toc-core-concepts">Core Concepts</a></li> <li><a href="#key-features" id="markdown-toc-key-features">Key Features</a></li> <li><a href="#message-delivery-semantics" id="markdown-toc-message-delivery-semantics">Message delivery semantics</a></li> <li><a href="#use-cases" id="markdown-toc-use-cases">Use Cases</a></li> </ul> <hr/> <h2 id="what-is-apache-kafka">What is Apache Kafka?</h2> <p>Apache Kafka is an open-source distributed event streaming platform. It consists of highly scalable and fault tolerant <strong>servers</strong> enabling real-time data ingestion and processing between <strong>clients</strong> that are decoupled (source and target) and can scale independently.</p> <h2 id="core-concepts">Core Concepts</h2> <ul> <li><strong>Cluster</strong>: A group of Kafka broker servers that work together to manage and distribute data.</li> <li><strong>Broker</strong>: A Kafka server that stores data and serves client requests (producers and consumers). Multiple brokers form a cluster.</li> <li><strong>Topic</strong>: A named stream of records to which producers send data and from which consumers read. Topics are split into partitions.</li> <li><strong>Partition</strong>: Each topic is divided into partitions, which are ordered, immutable sequences of messages. Partitions enable parallelism and scalability.</li> <li><strong>Producer</strong>: An application or service that sends (publishes) records to Kafka topics.</li> <li><strong>Message Key</strong>: producers can choose to send a key with records then messages for that key will always go to the same partition.</li> <li><strong>Consumer</strong>: An application or service that reads (subscribes to) records from Kafka topics.</li> <li><strong>Consumer Group</strong>: A group of consumers that work together to consume data from a topic, ensuring each message is processed only once by the group.</li> <li><strong>ZooKeeper</strong>: Used for managing and coordinating Kafka brokers (leader election, metadata, etc.). Note: Newer Kafka versions are moving toward removing ZooKeeper dependency.</li> <li><strong>Offset</strong>: A unique identifier for each message within a partition, used by consumers to keep track of read messages.</li> <li><strong>Replication</strong>: Each partition can be replicated across multiple brokers to ensure durability and high availability.</li> </ul> <figure> <img src="/assets/images/articles/24_apache_kafka_architecture.png" alt="Example Kafka Topic &amp; Producers "/> <figcaption>Figure 1: Example Kafka Topic &amp; Producers - <a href="https://kafka.apache.org/documentation/#gettingStarted">Image Source</a></figcaption> </figure> <h2 id="key-features">Key Features</h2> <ul> <li><strong>High Throughput</strong>: Capable of handling millions of messages per second.</li> <li><strong>Low Latency</strong>: Designed for real-time streaming and processing.</li> <li><strong>Scalability</strong>: Scales horizontally by adding brokers and partitions.</li> <li><strong>Fault Tolerance</strong>: Data is replicated across brokers; if one fails, another can take over.</li> <li><strong>Durability</strong>: Messages are persisted on disk and replicated.</li> <li><strong>Decoupling</strong>: Producers and consumers are independent, enabling flexible architectures.</li> <li><strong>Multiple APIs</strong>: <ul> <li><em>Admin API</em>: manage and inspect topics, brokers, and other Kafka objects.</li> <li><em>Producer API</em>: Publish data to topics.</li> <li><em>Consumer API</em>: Subscribe to and process data from topics.</li> <li><em>Streams API</em>: Build stream processing applications.</li> <li><em>Connect API</em>: Integrate with external systems (databases, file systems, etc.)</li> </ul> </li> </ul> <h2 id="message-delivery-semantics">Message delivery semantics</h2> <p>Apache Kafka provides three primary message delivery semantics:</p> <ul> <li><strong>At-most-once</strong>: Messages are delivered zero or one time. Some messages may be lost, but never delivered more than once. Typical usage in applicaitons with high-throughput, low latency requirements and risk of data loss.</li> <li><strong>At-least-once</strong>: Messages are delivered one or more times. No message is lost, but duplicates may occur. Typical usage in data pipelines where no data loss is acceptable, and duplicates can be handled.</li> <li><strong>Exactly-once</strong>: Each message is delivered once and only once. No data loss or duplicate delivery. Ensures no message loss or duplication, but with increased latency and configuration overhead Typical usage in Financial transactions and critical data flows.</li> </ul> <p>How kafka achieves these semantics:</p> <ul> <li>At-most-once: <ul> <li>Producer sends messages without waiting for acknowledgment (acks=0).</li> <li>If a failure occurs before delivery, messages may be lost.</li> <li>Consumer commits its offset before processing messages. If it crashes after committing but before processing, messages are lost.</li> </ul> </li> <li>At-least-once: <ul> <li>Producer waits for acknowledgment (acks=1 or acks=all).</li> <li>If acknowledgment is not received, the producer retries, which can result in duplicate messages.</li> <li>Consumers must be idempotent to handle possible duplicates</li> <li>Consumer commits offset after processing. If it crashes before committing, messages may be processed again after recovery (duplicates possible).</li> </ul> </li> <li>Exactly-once: <ul> <li>Producer uses idempotence and transactions; each message is written once even if retried.</li> <li>Consumers and producers must be properly configured for transactional processing. Offset commits and output are part of the same transaction, ensuring atomicity.</li> </ul> </li> </ul> <h2 id="use-cases">Use Cases</h2> <p>Some of the popular use cases for Apache Kafka include:</p> <ul> <li><strong>Messaging</strong>: replacement to traditional message broker for decoupling data processing between producers and consumers.</li> <li><strong>Website activity tracking</strong>: this is the original use case where a user’s site activity like page views, clicks and searches events are published to central topics and available for consumption from real time analytics and insights applications.</li> <li><strong>Metrics</strong>: similarly apache kafka is used in aggregating statistics from distributed applications to produce centralized feeds of operational data.</li> <li><strong>Log aggregation</strong>: used as a replacement for log aggregation solutions giving cleaner abstraction of log or event data as a stream of messages.</li> </ul> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Apache Kafka is a highly scaled, distributed and fault tolerant event streaming platform.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/24_apache_kafka_architecture.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/24_apache_kafka_architecture.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">101 Apache Airflow Cheatsheet</title><link href="https://www.firasesbai.com/articles/2025/09/08/apache-airflow-101.html" rel="alternate" type="text/html" title="101 Apache Airflow Cheatsheet"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/09/08/apache-airflow-101</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/09/08/apache-airflow-101.html"><![CDATA[<p><em>101 Apache Airflow Cheatsheet.</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#what-is-apache-airflow" id="markdown-toc-what-is-apache-airflow">What is Apache Airflow?</a></li> <li><a href="#core-concepts" id="markdown-toc-core-concepts">Core Concepts</a> <ul> <li><a href="#directed-acyclic-graphs-dags" id="markdown-toc-directed-acyclic-graphs-dags">Directed Acyclic Graphs (DAGs)</a></li> <li><a href="#dag-run" id="markdown-toc-dag-run">DAG Run</a></li> <li><a href="#tasks" id="markdown-toc-tasks">Tasks</a></li> <li><a href="#task-instances" id="markdown-toc-task-instances">Task Instances</a></li> <li><a href="#variables" id="markdown-toc-variables">Variables</a></li> </ul> </li> <li><a href="#architecture-components" id="markdown-toc-architecture-components">Architecture Components</a></li> <li><a href="#architecture-components-1" id="markdown-toc-architecture-components-1">Architecture Components</a></li> </ul> <hr/> <h2 id="what-is-apache-airflow">What is Apache Airflow?</h2> <p>Apache Airflow is an open-source workflow management platform designed to programmatically author, schedule, and monitor complex data pipelines.</p> <h2 id="core-concepts">Core Concepts</h2> <h3 id="directed-acyclic-graphs-dags">Directed Acyclic Graphs (DAGs)</h3> <p>The fundamental structure in Airflow, representing a collection of tasks with defined dependencies. Each DAG is defined in Python code and dictates the order of task execution based on their relationships. A DAG is a graph structure where tasks are represented as nodes, and the dependencies between these tasks are represented as directed edges. The “directed” aspect indicates that tasks have a specific order of execution, while “acyclic” means there are no loops or cycles, preventing infinite execution paths.</p> <h3 id="dag-run">DAG Run</h3> <p>A DAG Run is an object representing an instantiation of the DAG in time. Any time the DAG is executed, a DAG Run is created and all tasks inside it are executed. The status of the DAG Run depends on the tasks states. Each DAG Run is run separately from one another, meaning that you can have many runs of a DAG at the same time.</p> <h3 id="tasks">Tasks</h3> <p>Task is the individual units of work within a DAG. Each task represents a single operation, such as data extraction, transformation, or loading (ETL). The relationships between tasks are established using dependency definitions. This can be done through:</p> <ul> <li>Bitwise Operators: Using » to set downstream dependencies and « for upstream dependencies.</li> <li>Methods: Using set_upstream() and set_downstream() methods to explicitly define task relationships.</li> </ul> <p>There are three common types of task:</p> <ul> <li><strong>Operators</strong>, conceptually a template for predefined tasks that you can string together quickly to build most parts of your DAGs.</li> <li><strong>Sensors</strong>, a special subclass of Operators which are entirely about waiting for an external event to happen.</li> <li>A <strong>TaskFlow-decorated</strong> @task, which is a custom Python function packaged up as a Task.</li> </ul> <p>To pass data between tasks you have three options:</p> <ul> <li><strong>XComs</strong> (“Cross-communications”), a system where you can have tasks push and pull small bits of metadata identified by a <strong>key</strong> as well as the <strong>task_id</strong> and <strong>dag_id</strong> it came from.</li> <li>Uploading and downloading large files from a storage service (either one you run, or part of a public cloud)</li> <li><strong>TaskFlow API</strong> automatically passes data between tasks via implicit XComs</li> </ul> <h3 id="task-instances">Task Instances</h3> <p>Much in the same way that a DAG is instantiated into a DAG Run each time it runs, task instances are specific executions of tasks at particular times, which can vary based on the DAG’s scheduling.</p> <h3 id="variables">Variables</h3> <p>Variables are Airflow’s runtime configuration concept - a general key/value store that is global and can be queried from your tasks, and easily set via Airflow’s user interface, or bulk-uploaded as a JSON file. Variables are <strong>global</strong>, and should only be used for overall configuration that covers the entire installation; to pass data from one Task/Operator to another, you should use XComs instead.</p> <h2 id="architecture-components">Architecture Components</h2> <figure> <img src="/assets/images/articles/23_apache_airflow_architecture.png" alt="Apache Airflow Architecture Components"/> <figcaption>Figure 1: Apache Airflow Architecture Components - <a href="https://airflow.apache.org/docs/apache-airflow/2.1.2/concepts/overview.html">Image Source</a></figcaption> </figure> <ul> <li><strong>Scheduler</strong>: The component responsible for scheduling tasks and determining when they should run. It checks the DAG directory for tasks that need to be executed.</li> <li><strong>Executor</strong>: This defines how and where tasks are executed. Various executors are available. In the default Airflow installation, this runs everything inside the scheduler, but most production-suitable executors actually push task execution out to workers. Most executors will generally also introduce other components to let them talk to their workers - like a <strong>task queue</strong> - but you can still think of the executor and its workers as a single logical component</li> <li><strong>Web Server</strong>: Provides a user interface for monitoring and managing workflows, allowing users to inspect DAGs and task statuses.</li> <li><strong>A folder of DAG files</strong>, read by the scheduler and executor (and any workers the executor has)</li> <li><strong>Metadata Database</strong>: Stores all metadata related to DAGs and tasks, typically using PostgreSQL or MySQL.</li> </ul> <h2 id="architecture-components-1">Architecture Components</h2> <ul> <li><strong>Task Management</strong>: Airflow manages task dependencies automatically, ensuring that tasks execute in the correct order.</li> <li><strong>Scheduling</strong>: Airflow provides advanced scheduling capabilities, allowing workflows to run on defined schedules or trigger based on external events.</li> <li><strong>Extensibility</strong>: Users can create custom operators and plugins to extend Airflow’s functionality, integrating with various data sources and services.</li> <li><strong>Error Handling and Retries</strong>: Built-in mechanisms allow tasks to be retried automatically upon failure, enhancing workflow reliability.</li> <li><strong>Scalability</strong>: Airflow can handle thousands of concurrent tasks across multiple workers, making it suitable for large-scale data operations.</li> <li><strong>Rich Command Line Interface (CLI)</strong>: The CLI provides utilities for managing DAGs and executing tasks directly from the command line.</li> <li><strong>Integration with Other Tools</strong>: Airflow supports integration with various cloud services and data tools, including AWS, Google Cloud Platform, and many others.</li> </ul> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Happy learning!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Apache Airflow simplifies workflow automation with DAGs, task scheduling, and data pipeline management for data engineering workflows.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/23_apache_airflow_architecture.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/23_apache_airflow_architecture.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Blogging Tools</title><link href="https://www.firasesbai.com/articles/2025/08/18/blogging-tools.html" rel="alternate" type="text/html" title="Blogging Tools"/><published>2025-08-18T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/08/18/blogging-tools</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/08/18/blogging-tools.html"><![CDATA[<p><em>A curated list of tools that I’m using for writing and building this site.</em></p> <p>In this article, I have curated a list of resources and tools that help me manage everything from hosting and analytics to writing and working on this site. Finding the right tool can make a huge difference in terms of efficiency. While there are countless options out there, this is what works best for me.</p> <p><em>So let’s get started!</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#writing-and-planning" id="markdown-toc-writing-and-planning">Writing and Planning</a> <ul> <li><a href="#trello" id="markdown-toc-trello">Trello</a></li> <li><a href="#obsidian" id="markdown-toc-obsidian">Obsidian</a></li> </ul> </li> <li><a href="#domain-and-hosting" id="markdown-toc-domain-and-hosting">Domain and Hosting</a> <ul> <li><a href="#cloudflare" id="markdown-toc-cloudflare">Cloudflare</a></li> <li><a href="#github-pages" id="markdown-toc-github-pages">Github Pages</a></li> </ul> </li> <li><a href="#analytics-and-tracking" id="markdown-toc-analytics-and-tracking">Analytics and Tracking</a> <ul> <li><a href="#google-analytics-4-ga4" id="markdown-toc-google-analytics-4-ga4">Google Analytics 4 (GA4)</a></li> <li><a href="#google-search-console" id="markdown-toc-google-search-console">Google Search Console</a></li> <li><a href="#looker" id="markdown-toc-looker">Looker</a></li> </ul> </li> <li><a href="#seo" id="markdown-toc-seo">SEO</a> <ul> <li><a href="#ahrefs" id="markdown-toc-ahrefs">Ahrefs</a></li> <li><a href="#screaming-frog" id="markdown-toc-screaming-frog">Screaming Frog</a></li> <li><a href="#google-chrome-lighthouse" id="markdown-toc-google-chrome-lighthouse">Google Chrome Lighthouse</a></li> </ul> </li> <li><a href="#add-ons" id="markdown-toc-add-ons">Add-ons</a> <ul> <li><a href="#email-marketing" id="markdown-toc-email-marketing">Email Marketing</a></li> <li><a href="#comments" id="markdown-toc-comments">Comments</a></li> <li><a href="#contact-form" id="markdown-toc-contact-form">Contact Form</a></li> </ul> </li> <li><a href="#speed-optimization" id="markdown-toc-speed-optimization">Speed Optimization</a> <ul> <li><a href="#cloudflare-1" id="markdown-toc-cloudflare-1">Cloudflare</a></li> <li><a href="#tinypng" id="markdown-toc-tinypng">TinyPNG</a></li> <li><a href="#google-pagespeed-insights" id="markdown-toc-google-pagespeed-insights">Google Pagespeed Insights</a></li> </ul> </li> <li><a href="#recap" id="markdown-toc-recap">Recap</a></li> </ul> <hr/> <h2 id="writing-and-planning">Writing and Planning</h2> <h3 id="trello">Trello</h3> <p>Trello is a visual project management tool. I use it to organise the content workflow and capture ideas for potential articles. I keep it simple with just 3 lists representing the stages of the writing process: To Do, Doing, and Done. Each card within represents a blog post or some fixes or improvements to the site itself. To distinguish between them I use specific labels. This visual approach allows me to easily see what needs to be done and ensure I’m on track.</p> <h3 id="obsidian">Obsidian</h3> <p>Obsidian is a markdown based note taking app. In this context, i use it for kicking off drafts, creating notes for each blog post and capturing relevant research. It allows for linking notes together and creating interconnected central knowledge base where you can easily jump between related notes to brainstorm and rediscover information.</p> <h2 id="domain-and-hosting">Domain and Hosting</h2> <h3 id="cloudflare">Cloudflare</h3> <p>While often thought of for performance and security (which I’ll touch on later), Cloudflare also handles my DNS. It provides a fast and robust way to manage my domain’s records. For more details, you can check <a href="https://www.firasesbai.com/articles/2025/01/19/google-domains-cloudflare-migration.html">the following article</a> where I outline how I migrated from Google Domains to Cloudflare.</p> <h3 id="github-pages">Github Pages</h3> <p>This is where the site physically lives. It’s a free solution for hosting static websites like this one directly from a GitHub repository, and it integrates seamlessly with custom domains via Cloudflare.</p> <h2 id="analytics-and-tracking">Analytics and Tracking</h2> <h3 id="google-analytics-4-ga4">Google Analytics 4 (GA4)</h3> <p>GA4 helps me understand traffic sources, user behavior, content performance, and conversions. It’s the primary source for overall site metrics.</p> <h3 id="google-search-console">Google Search Console</h3> <p>This is essential for understanding how my site performs in Google Search. It shows me search queries, indexing status, technical errors, and sitemaps.</p> <h3 id="looker">Looker</h3> <p>I use Looker to create custom dashboards pulling data from GA4, Cloudflare Analytics, Search Console and Google Forms. This allows me to visualize key metrics in a centralised view without switching between tools and overcoming data discrepancies as indicated in <a href="https://www.firasesbai.com/articles/2024/08/11/cloudflare-vs-google-analytics.html">this article</a> where I cover the difference between Cloudflare Analytics and Google Analytics.</p> <h2 id="seo">SEO</h2> <h3 id="ahrefs">Ahrefs</h3> <p>I mainly use Ahrefs Webmaster Tools which gives you free access to a bundle of 3 tools if you can verify the ownership of your website. You can achieve this by connecting Google Search Console as the recommended approach. - Web Analytics: You can setup web analytics for your site as an alternative to Google Analytics to get real time metrics about your visitors. - Site Audit: Scans your website for known technical and most common SEO issues such as broken links, duplicate content and missing metadata description. You can setup scheduled crawls of your site and get an email with an overview containing a health score of the site, number of issues grouped by severity and newly identified issues since the last crawl. This will allow you to proactively identify and fix issues to improve your site’s performance. - Site Explorer: Helps you gain insights into your site’s organic search performance</p> <h3 id="screaming-frog">Screaming Frog</h3> <p>Screaming Frog is a desktop based website crawler with a free version limited to 500 URLs which is more than enough for a small blog. It acts as a search engine spider by crawling your site’s URLs and extracting data to give you a comprehensive technical audit. By finding and fixing issues like broken links, you ensure a better user experience and help search engines properly crawl and index your site.</p> <h3 id="google-chrome-lighthouse">Google Chrome Lighthouse</h3> <p>As part of the Chrome browser’s developer tools, Lighthouse provides a detailed report on the performance, accessibility, best practices and SEO of any web page. It is very handy for a quick and easy way to check the health of individual pages on the site locally before publishing them.</p> <h2 id="add-ons">Add-ons</h2> <h3 id="email-marketing">Email Marketing</h3> <p>Building an email list is vital for direct communication with my audience. I’ve explored tools like <strong>ConvertKit</strong> and <strong>Mailerlite</strong>, both offering features to build and manage subscriber lists, create landing pages, and send broadcasts or sequences. I have settled for Mailerlite as the free plan was more interesting especially for starting out with small number of subscribers.</p> <h3 id="comments">Comments</h3> <p>I use <strong>Disqus</strong> for managing comments on the blog posts. Originally I started out using Github API where I create an issue for each blog post and the comments section will redirect the users to commenting on the created issue. The list of all the comments is then fetched from the API. This was easy to setup initially but had limits as it requires the users to have a Github account and to be logged in. In addition, the overhead of maintaining the different issues made it easy to switch to Disqus as it provides a robust commenting system with moderation tools.</p> <h3 id="contact-form">Contact Form</h3> <p>I use an embedded <strong>Google Forms</strong> in my contact page to enable visitors of my site to reach out and handle their requests and inquiries.</p> <h2 id="speed-optimization">Speed Optimization</h2> <h3 id="cloudflare-1">Cloudflare</h3> <p>As mentioned earlier, using Cloudflare comes with the additional bonus of faster and more responsive website. This is achieved through its role as a Content Delivery Network (CDN). A CDN is a network of servers located all over the world. When a visitor comes to my blog, Cloudflare automatically serves the static content (like images, CSS, and JavaScript files) from the server closest to them. This dramatically reduces the physical distance the data has to travel, which in turn cuts down on page load times.</p> <h3 id="tinypng">TinyPNG</h3> <p>Images are often the biggest culprit for slow page load times. TinyPNG is a free online tool that uses smart lossy compression techniques to reduce the file size of my images without a noticeable loss in quality.</p> <h3 id="google-pagespeed-insights">Google Pagespeed Insights</h3> <p>While using Google Chrome Lighthouse before publishing a post is useful for seeing how the introduced changes affect the performance, it is still run under controlled, simulated conditions such as throttled netweok speed on a specific device. This is where Pagespeed Insights comes in handy by complementing that with data on how real world visitors have experienced your site. It provides a holistic view of performance for both mobile and desktop with a list of actionable recommendations. This is a more accurate representation of how your site performs for your actual audience, across various devices and network conditions.</p> <p>If you have any remarks or suggestions for me, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p> <h2 id="recap">Recap</h2> <p>This is not an exhaustive list but rather a work in progress. Each tool serves a specific purpose in streamlining the process from writing to managing the site’s performance. While the exact setup may evolve, the goal remains the same: keep things simple and effective.</p> <p><em>Happy learning!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Blogging"/><summary type="html"><![CDATA[Discover the curated list of tools for writing and building this site, covering hosting, analytics, SEO, and speed optimization.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Introducing My Data Engineering Tech Radar</title><link href="https://www.firasesbai.com/articles/2025/08/10/data-engineering-tech-radar.html" rel="alternate" type="text/html" title="Introducing My Data Engineering Tech Radar"/><published>2025-08-10T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/08/10/data-engineering-tech-radar</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/08/10/data-engineering-tech-radar.html"><![CDATA[<p><em>Launching a Data Engineering Tech Radar.</em></p> <p>The world of data engineering is constantly changing. New tools emerge every week, architectural patterns fall in and out of favor, and the hype cycle churns endlessly. I’m sure we all at some point have come across variations of landscapes or state of data engineering diagrams packed with unreadable logos. Be assured, this is not one of those.</p> <p>Keeping up feels like a full-time job already and we are usually looking to find something to help us cut through the noise and answer the following: How do you decide which technologies are genuinely worth your time?</p> <p>To help, first myself, answer that question, I created this <strong><a href="https://www.firasesbai.com/data-tech-radar/" target="_blank">Data Engineering Tech Radar</a></strong>.</p> <p>Inspired by the pioneering <a href="https://www.thoughtworks.com/en-de/radar">ThoughtWorks Tech Radar</a>, this is my curated, opinionated, and practical guide to the data ecosystem. It’s a snapshot of my perspective on the tools, platforms, languages, and techniques that I have personally used or seen other teams adopt them in production. It is also a way to capture and keep track of what actually matters in our field right now and not feel overwhelmed.</p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#why-a-tech-radar" id="markdown-toc-why-a-tech-radar">Why a Tech Radar?</a></li> <li><a href="#how-it-works-quadrants-and-rings" id="markdown-toc-how-it-works-quadrants-and-rings">How It Works: Quadrants and Rings</a> <ul> <li><a href="#the-quadrants" id="markdown-toc-the-quadrants">The Quadrants</a></li> <li><a href="#the-rings" id="markdown-toc-the-rings">The Rings</a></li> </ul> </li> <li><a href="#this-is-just-the-beginning" id="markdown-toc-this-is-just-the-beginning">This Is Just the Beginning</a></li> </ul> <hr/> <h2 id="why-a-tech-radar">Why a Tech Radar?</h2> <p>My motivation for creating this radar is threefold:</p> <ol> <li><strong>To Navigate Complexity:</strong> The goal isn’t to list every tool, but to provide a filter. This radar helps separate the signal from the noise by offering a structured opinion on what’s production-ready, what’s promising, and what you might want to proceed with caution on.</li> <li><strong>To Share Real-World Experience:</strong> Tutorials can show you <em>how</em> a tool works, but they rarely tell you <em>if</em> you should use it. This radar is built on hands-on experience, reflecting what has worked well in practice and the lessons learned along the way.</li> <li><strong>To Track a Moving Target:</strong> The data landscape is not static, and neither is this radar. It’s a living document that I will update periodically to reflect new developments and evolving opinions, serving as a log of how our industry changes over time.</li> </ol> <h2 id="how-it-works-quadrants-and-rings">How It Works: Quadrants and Rings</h2> <p>To make sense of everything, the radar is broken down into four quadrants and four rings.</p> <figure> <img src="/assets/images/articles/22_data_tech_radar.png" alt="Data Tech Radar"/> <figcaption>Figure 1: Data Tech Radar</figcaption> </figure> <h3 id="the-quadrants">The Quadrants</h3> <p>The quadrants categorize items by their functional area in the data lifecycle.</p> <ul> <li><strong>Languages &amp; Frameworks:</strong> This quadrant covers the foundational skills, languages and frameworks.</li> <li><strong>Transformation &amp; Orchestration:</strong> Tools and practices for data transformation, ETL processes, and workflow orchestration.</li> <li><strong>Data Platforms &amp; Storage</strong>: Technologies for data storage, databases, and data warehousing solutions.</li> <li><strong>Data Analytics:</strong> Tools and platforms for data analysis, visualization, and business intelligence.</li> </ul> <h3 id="the-rings">The Rings</h3> <p>The rings represent my opinion on a technology’s maturity and my recommendation for its adoption.</p> <ul> <li><strong>Adopt:</strong> Technologies that are well-established and I have used or seen other teams adopt them in production and they’ve proven their value.</li> <li><strong>Trial:</strong> Emerging technologies that show promise and I have started exploring them or using on non-critical projects.</li> <li><strong>Assess:</strong> Technologies that caught my eye and are used or recommended by other teams that I think are worth exploring.</li> <li><strong>Hold:</strong> Technologies to proceed with caution on and I would not recommended for new projects.</li> </ul> <h2 id="this-is-just-the-beginning">This Is Just the Beginning</h2> <p>This is just the initial release and it might not have all the tools I have under the radar but this radar is a starting point, a snapshot in time, and it should be treated as such. My opinions will change as I learn, and new tools will emerge that demand a spot. I plan to revisit and update the radar periodically to keep it relevant.</p> <p>But most importantly, this is meant to be a conversation starter.</p> <p>What did I get right? What do you completely disagree with? What hidden gems are missing from the “Assess” ring? I’d love to hear your thoughts.</p> <p><strong>Check out the full <a href="https://www.firasesbai.com/data-tech-radar/" target="_blank">Data Engineering Tech Radar</a> and let me know what you think.</strong></p> <p><em>Stay tuned!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Launching a Data Engineering Tech Radar]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/articles/22_data_tech_radar.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/articles/22_data_tech_radar.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Data Engineering Capabilities and Personas</title><link href="https://www.firasesbai.com/articles/2025/05/25/data-engineering-capabilities-and-personas.html" rel="alternate" type="text/html" title="Data Engineering Capabilities and Personas"/><published>2025-05-25T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/05/25/data-engineering-capabilities-and-personas</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/05/25/data-engineering-capabilities-and-personas.html"><![CDATA[<p><em>In this article, we will look at key data engineering capabilities at the intersection of several archetypes and personas.</em></p> <p>In <a href="https://www.firasesbai.com/articles/2023/03/01/data-engineering-101.html">a previous article</a>, we looked into the role of a data engineer and the general responsibilities associated with it. However, the field of data engineering has changed dramatically over the last decade. This led to the emergence of different constellations of data teams. Depending on the company size and maturity level, more specialized personas appeared requiring certain set of capabilities. In today’s article, we will breakdown these types and the expected capabilities.</p> <p><em>So let’s get started!</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#software-engineering-at-heart" id="markdown-toc-software-engineering-at-heart">Software Engineering at Heart</a></li> <li><a href="#data-platform-engineering" id="markdown-toc-data-platform-engineering">Data Platform Engineering</a></li> <li><a href="#analytics-engineering" id="markdown-toc-analytics-engineering">Analytics Engineering</a></li> <li><a href="#recap" id="markdown-toc-recap">Recap</a></li> </ul> <hr/> <h2 id="software-engineering-at-heart">Software Engineering at Heart</h2> <p>There is a lot of debate and people comparing the role of data engineer with traditional software engineering. Some would argue that a data engineer is just a specialized software engineer while others would question the complexity and required technical skills in data engineering. One thing we can take out from this though is that the lines between the roles are blurring.</p> <p>A data engineer should be able to build, maintain and test the software architecture for managing different complexities of data. This includes understanding the principles, patterns and practices of writing clean code that is easy to evolve, test and get into production. A solid understanding of distributed systems and microservices architecture through the lenses of Application Programming Interfaces (APIs) in order to implement a secure, scalable and performant solution.</p> <h2 id="data-platform-engineering">Data Platform Engineering</h2> <p>Another area data teams focus on is the design and operation of the infrastructure required to run different types of data workloads. This includes knowing the tradeoffs between on-premises and cloud infrastructure as well as related tools and practices such as infrastructure as code, monitoring, performance testing and optimization.</p> <p>The purpose of building a data platform is to cover the end to end data lifecycle and related aspects including:</p> <ul> <li>Data pipelines: <ul> <li>Ability to build, deploy, and orchestrate data pipelines and the different technology options to implement them. Examples include Extract-Transform-Load (ETL), Extract-Load-Transform (ELT), Change Data Capture (CDC), and batch vs. streaming pipelines.</li> <li>Building data pipelines is not just about moving data from one system to another. It involves continuously evaluating, monitoring and improving the quality of your data over time. Common data quality aspects include completeness, timeliness, accuracy, integrity, and consistency.</li> </ul> </li> <li>Data Modeling: <ul> <li>Ability to model data in different types of databases according to the data architecture and business needs. This includes RDBMS, data warehouses, key-value stores, document stores, graph databases, distributed file systems and columnar data stores.</li> </ul> </li> <li>Data Storage: <ul> <li>Ability to understand and choose different platforms and technology options to store data. This includes different types of databases, data lake, data warehouse, and data serialization formats.</li> </ul> </li> <li>Data Governance: <ul> <li>Data governance includes a company-wide principles, practices and organizational structures. It involves the ability to understand, design, and apply security controls around the sharing and using of data across the enterprise. Encompassing aspects around authorization, encryption, information security, compliance and regulatory needs. In addition, familiarity with the elements of data privacy and ethics such as bias, are crucial in order to detect and mitigate the anticipated threats, vulnerabilities and unintended consequences that can arise when using data.</li> </ul> </li> </ul> <h2 id="analytics-engineering">Analytics Engineering</h2> <p>One more area to cover is analytics engineering where data engineering teams focus on extracting insights and knowledge from the processed data at the end of the data lifecycle. In addition to understanding multidimensional modeling and data warehousing technologies, it involves the ability to derive insights and actionable knowledge delivering clear reports, dashboards and KPIs containing compelling and effective visualizations to inform stakeholders and to support business decision-making.</p> <p>With this we have reached the end of this post, I hope you enjoyed it!</p> <p>Let me know what other teams should we include?</p> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p> <h2 id="recap">Recap</h2> <p>In this article, we discussed core capabilities required across different personas and team structures. Understanding these constellations helps organizations build effective teams and deliver value.</p> <p><em>Happy learning!</em></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Discover essential data engineering capabilities and personas in modern data teams for business success.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Data Encoding and Formats</title><link href="https://www.firasesbai.com/articles/2025/05/18/data-encoding-and-formats.html" rel="alternate" type="text/html" title="Data Encoding and Formats"/><published>2025-05-18T00:00:00+00:00</published><updated>2025-12-07T16:47:00+00:00</updated><id>https://www.firasesbai.com/articles/2025/05/18/data-encoding-and-formats</id><content type="html" xml:base="https://www.firasesbai.com/articles/2025/05/18/data-encoding-and-formats.html"><![CDATA[<p><em>In this article, we will explore the importance of data encoding and how to choose the right data format, weather it is simple CSVs or binary formats like Avro and Parquet, in order to achieve better performance, cost, and evolvability.</em></p> <p>Data Serialization and Data Formats go hand in hand; one converts data objects to a shareable or storable structure and the other describes how this new structure is stored or transmitted and retrieved. The choice of <em>how</em> we serialize and the <em>format</em> we use impacts performance, storage costs and interoperability. Getting it right is key to building efficient, scalable, and maintainable data pipelines.</p> <p><em>So let’s get started!</em></p> <hr/> <h1> Contents </h1> <ul id="markdown-toc"> <li><a href="#data-serialization" id="markdown-toc-data-serialization">Data Serialization</a></li> <li><a href="#data-formats-characteristics" id="markdown-toc-data-formats-characteristics">Data Formats Characteristics</a></li> <li><a href="#standardized-textual-formats" id="markdown-toc-standardized-textual-formats">Standardized Textual Formats</a> <ul> <li><a href="#xml" id="markdown-toc-xml">XML</a></li> <li><a href="#json" id="markdown-toc-json">JSON</a></li> <li><a href="#csv" id="markdown-toc-csv">CSV</a></li> </ul> </li> <li><a href="#binary-data-formats" id="markdown-toc-binary-data-formats">Binary Data Formats</a> <ul> <li><a href="#apache-avro" id="markdown-toc-apache-avro">Apache Avro</a></li> <li><a href="#apache-parquet" id="markdown-toc-apache-parquet">Apache Parquet</a></li> <li><a href="#apache-orc-optimized-row-columnar" id="markdown-toc-apache-orc-optimized-row-columnar">Apache ORC (Optimized Row Columnar)</a></li> </ul> </li> <li><a href="#choosing-the-right-format" id="markdown-toc-choosing-the-right-format">Choosing the Right Format</a></li> <li><a href="#recap" id="markdown-toc-recap">Recap</a></li> <li><a href="#resources" id="markdown-toc-resources">Resources</a></li> </ul> <hr/> <h2 id="data-serialization">Data Serialization</h2> <p>The seemingly simple act of storing data or sending it across a network involves a fundamental process: <strong>data serialization</strong>. This is the conversion of data objects, residing in potentially complex in-memory structures (like objects, lists, trees in our code), into a byte stream suitable for persistent storage or network transmission. Once data arrives at its destination, the reverse process, <strong>deserialization</strong> (or parsing/decoding), reconstructs the original object structures from the byte sequence.</p> <p>For example, in the context of databases, when a process is writing data to the database, the data is initially encoded into a sequence of bytes and then stored. Later when another process tries to read data from the database, it needs to decode it first. Similarly the communication between web services through REST APIs involves encoding the request by the client before sending it. Once the server receives it, it decodes the request, process it and then encode the response to be sent back to the client. The latter makes the last deserialization of this exchange.</p> <p>Data serialization is ideal for storing data efficiently as serialized data takes up less storage space. This results in faster data transfer and reduced latency as data can be transmitted quickly and efficiently over networks. In addition, it enhances flexibility and interoperability making data exchange seamless across different applications and networks.</p> <p>With this understanding and examples of the data serialization process and its application, in the coming sections we will look into the types and characteristics of common data formats.</p> <h2 id="data-formats-characteristics">Data Formats Characteristics</h2> <p>Before going through each data format separately, let’s start by understanding general characteristics we will be using when evaluating how they behave, what they are good for and where they might fall short.</p> <ul> <li><strong>Human Readability</strong>: Can humans easily open and understand the data with a text editor?</li> <li><strong>Compressibility:</strong> How well does the format compress and how much space does it take? This is important for saving storage and network bandwidth.</li> <li><strong>Speed and Performance</strong>: How quickly can the format be read/written?</li> <li><strong>Splittable:</strong> Can a single large file be processed in parallel chunks? This is essential for distributed data processing frameworks such as Apache Spark.</li> <li><strong>Schema Support</strong>: Does the format have a defined <strong>schema</strong> (structure definition)? Schema allows for validation and consistent data interpretation.</li> <li><strong>Self-describing:</strong> Does the file embed its own schema or metadata about its structure? This simplifies reading without needing external schema definitions.</li> <li><strong>Schema Evolution:</strong> How well does the format handle changes (adding/removing fields) over time while maintaining compatibility?</li> <li><strong>Interoperability</strong>: Can it be easily used across languages, tools, platforms?</li> </ul> <h2 id="standardized-textual-formats">Standardized Textual Formats</h2> <p>Many programming languages provide built-in mechanisms for encoding in-memory objects into byte sequences like Python’s <code class="language-plaintext highlighter-rouge">pickle</code> module. While these are easy to use, the encoding is tightly coupled to the specific language used making it very difficult or impossible to read the data in a different programming languages. In addition, the processes of encoding and decoding are usually CPU intensive and result in a sizeable encoded data. Not to mention that they are open to security vulnerabilities when malicious byte sequence is decoded that causes the application to instantiate unintended classes, potentially leading to arbitrary code execution.</p> <p>To overcome the limitations of language-specific formats, standardized encodings readable by multiple languages are widely used. Common examples include JSON, XML, and CSV due to their human-readability and simplicity, especially for data exchange.</p> <h3 id="xml">XML</h3> <p>XML (eXtensible Markup Language) is a nested markup format. Its hierarchical structure makes it splittable by elements, though it is often criticized for verbosity which reduces compression efficiency. It’s still widely used in enterprise environments despite declining popularity due to complexity.</p> <h3 id="json">JSON</h3> <p>JSON (JavaScript Object Notation) gained popularity due to its relative simplicity in comparison to XML and native support in web browsers. JSON offers a simpler, row-like nested structure with arrays and objects, making it both compressible and splittable. It’s self-describing (with field names), making it easier for data exchange, though schema evolution isn’t natively supported. JSON is common in web APIs, configs, and moderately complex datasets.</p> <h3 id="csv">CSV</h3> <p>CSV (Comma Separated Values) is a flat, row-based format that’s highly compressible and trivially splittable by lines. It lacks self-description and schema enforcement, making it lightweight but error-prone. It’s ideal for small, tabular data where structure is known or managed externally.</p> <p>Despite being readable, these textual formats share some key limitations. For instance, in both XML and CSV there is no clear distinction between a number and a string composed of digits. JSON on the other hand does separate numbers from strings but doesn’t specify numeric types or their precision. In addition, all three formats lack native support for binary data (sequences of raw bytes like images) and this usually involves encoding it into text using schemes like Base64, which increases the data size.</p> <h2 id="binary-data-formats">Binary Data Formats</h2> <p>There has been a couple of projects that converted JSON and XML into a binary representation and tried to solve some of the problems we mentioned before but the main drawback still the missing schema. For small data sets, the gains might be negligible but for large datasets or performance-critical applications, choosing the appropriate binary format offers significant advantages in size and speed. Therefore, in this section we will look into the characteristics of 3 common binary formats: Apache Avro, Apache Parquet and Apache ORC.</p> <h3 id="apache-avro">Apache Avro</h3> <p>Apache Avro is a data serialization system providing rich data structures and compact binary data format. It started in 2009 as a sub project of Hadoop as a result of <a href="https://thrift.apache.org/">Thrift</a> not being a good fit for Hadoop’s use case. A key feature of Apache Avro is its robust support for schema evolution. It uses JSON-based schema definition. The schema is typically <strong>embedded within the data file</strong> which permits full processing of the data without code generation and less type information encoded within the data resulting in smaller serialization size. Crucially, Avro distinguishes between the <strong>writer’s schema</strong> (used during encoding) and the <strong>reader’s schema</strong> (used during decoding). They don’t need to be identical, only <em>compatible</em>, enabling robust <strong>schema evolution</strong>. Apache Avro is often used in write-heavy batch ingestion, streaming data messages like Apache Kafka and scenarios needing strong schema validation and evolution guarantees.</p> <h3 id="apache-parquet">Apache Parquet</h3> <p><strong>Apache Parquet</strong> is a columnar storage format optimized for analytical workloads, developed jointly by Cloudera and Twitter in 2013 as part of the Hadoop ecosystem. Parquet stores data in a <strong>column-oriented</strong> layout, which means data from the same column are stored together. This design allows for <strong>highly efficient scans</strong> when querying subsets of columns and enables advanced <strong>compression and encoding techniques</strong> tailored per column. Parquet supports schema evolution through metadata that tracks schema changes over time. Parquet files embed the schema, allowing readers to reconcile differences between older and newer versions using a <strong>merge</strong> strategy. Apache Parquet is often used in data lakes, data warehouses and large-scale analytics engines like Spark, Presto, Trino, etc.</p> <h3 id="apache-orc-optimized-row-columnar">Apache ORC (Optimized Row Columnar)</h3> <p><strong>Apache ORC</strong> (Optimized Row Columnar) is another high-performance columnar format, created in 2013 by Hortonworks to optimize Hive workloads. Like Parquet, ORC organizes data column-wise but adds features tailored for Hive, such as ACID transaction support. It also stores extensive metadata, such as min/max values and row-level statistics, enabling <strong>aggressive query optimizations</strong>. ORC files store schema information and statistics in footers, enabling efficient reads. While similar to Parquet in structure, ORC shines in Hive-centric workflows and <strong>write-once, read-many</strong> batch processing jobs. Its design emphasizes <strong>fast scan performance</strong> and <strong>compression ratios</strong>, especially when working with large, structured datasets.</p> <h2 id="choosing-the-right-format">Choosing the Right Format</h2> <p>Choosing a format isn’t just about static storage. Systems evolve and so does the underlying data. There is no single best format and the choice will depend on some tradeoffs even though in practice pipelines end up using multiple formats for different purposes.</p> <p>The following table compares the key characteristics worth having in mind when choosing among the data formats we discussed:</p> <div style="overflow-x: auto;"> <table> <thead> <tr> <th><strong>Characteristic</strong></th> <th style="text-align: left"><strong>XML</strong></th> <th style="text-align: left"><strong>JSON</strong></th> <th style="text-align: left"><strong>CSV</strong></th> <th style="text-align: left"><strong>Avro</strong></th> <th style="text-align: left"><strong>Parquet</strong></th> <th style="text-align: left"><strong>ORC</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Human Readability</strong></td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes (simple)</td> <td style="text-align: left">No</td> <td style="text-align: left">No</td> <td style="text-align: left">No</td> </tr> <tr> <td><strong>Compressibility</strong></td> <td style="text-align: left">Moderate (verbose)</td> <td style="text-align: left">Moderate</td> <td style="text-align: left">Low</td> <td style="text-align: left">High (binary + schema)</td> <td style="text-align: left">Very High (columnar)</td> <td style="text-align: left">Very High (columnar)</td> </tr> <tr> <td><strong>Speed &amp; Performance(Read/Write)</strong></td> <td style="text-align: left">Slow (verbose parsing)</td> <td style="text-align: left">Decent (for small data)</td> <td style="text-align: left">Fast (simple format)</td> <td style="text-align: left">Fast</td> <td style="text-align: left">Very Fast (columnar)</td> <td style="text-align: left">Very Fast (columnar)</td> </tr> <tr> <td><strong>Splittable</strong></td> <td style="text-align: left">No</td> <td style="text-align: left">Partial (depends on parser)</td> <td style="text-align: left">Yes (if line delimited)</td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes</td> </tr> <tr> <td><strong>Schema Support</strong></td> <td style="text-align: left">Optional (XSD, external)</td> <td style="text-align: left">Optional</td> <td style="text-align: left">No</td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes</td> <td style="text-align: left">Yes</td> </tr> <tr> <td><strong>Schema Evolution</strong></td> <td style="text-align: left">Poor</td> <td style="text-align: left">Poor</td> <td style="text-align: left">None</td> <td style="text-align: left">Excellent</td> <td style="text-align: left">Good</td> <td style="text-align: left">Good</td> </tr> <tr> <td><strong>Best Use Case</strong></td> <td style="text-align: left">Config/debugging with structure</td> <td style="text-align: left">Debugging, API responses</td> <td style="text-align: left">Simple tabular data</td> <td style="text-align: left">OLTP-like ingestion, Kafka</td> <td style="text-align: left">OLAP, analytics, Spark/Hadoop</td> <td style="text-align: left">OLAP, analytics, Hive/Spark</td> </tr> <tr> <td><strong>Read vs. Write</strong></td> <td style="text-align: left">Read/debugging</td> <td style="text-align: left">Read/debugging</td> <td style="text-align: left">Read/debugging</td> <td style="text-align: left">Write-heavy (streaming, Kafka)</td> <td style="text-align: left">Read-heavy (query performance)</td> <td style="text-align: left">Read-heavy (query performance)</td> </tr> <tr> <td><strong>Data Complexity</strong></td> <td style="text-align: left">Supports nesting</td> <td style="text-align: left">Supports nesting</td> <td style="text-align: left">Flat only</td> <td style="text-align: left">Supports nesting</td> <td style="text-align: left">Supports nesting</td> <td style="text-align: left">Supports nesting</td> </tr> </tbody> </table> </div> <p style="text-align:center;">Table 1: Characteristics of data formats</p> <p>With this we have reached the end of this post, I hope you enjoyed it!</p> <p>If you have any remarks or questions, please don’t hesitate and do drop a comment below.</p> <p><em>Stay tuned!</em></p> <h2 id="recap">Recap</h2> <p>We’ve explored the critical role of data serialization and file formats in data engineering. We saw the limitations of language-specific and simple textual formats (CSV, JSON, XML) and contrasted them with powerful, schema-driven binary formats like Avro, Parquet, and ORC. Understanding the trade-offs—read/write performance, compression, splittability, schema evolution, and human readability—is vital for choosing the right tool for the job.</p> <p>These file formats often form the storage layer for modern <strong>Open Table Formats</strong> (Apache Iceberg, Apache Hudi, Delta Lake), which add transactional capabilities, time travel, and enhanced schema management on top – a powerful combination and perhaps a topic for a future deep dive!</p> <p><em>Happy learning!</em></p> <h2 id="resources">Resources</h2> <p><a href="https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/">https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/</a></p> <p><a href="https://luminousmen.com/post/big-data-file-formats">https://luminousmen.com/post/big-data-file-formats</a></p> <p><a href="https://www.confluent.io/learn/data-serialization/">https://www.confluent.io/learn/data-serialization/</a></p> <p><a href="https://d9nich.medium.com/json-xml-protobuf-thrift-avro-or-everything-you-need-to-know-about-encoding-data-6077a7e769e2">https://d9nich.medium.com/json-xml-protobuf-thrift-avro-or-everything-you-need-to-know-about-encoding-data-6077a7e769e2</a></p>]]></content><author><name>Firas Esbai</name></author><category term="articles"/><category term="Data Engineering"/><summary type="html"><![CDATA[Overview of data formats, their characteristics and how to choose the right format for better performance]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png"/><media:content medium="image" url="https://www.firasesbai.com/assets/images/default-seo-tag-image.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>